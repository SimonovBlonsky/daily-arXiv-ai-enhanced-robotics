<div id=toc></div>

# Table of Contents

- [cs.RO](#cs.RO) [Total: 19]


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [1] [Contact-aware Path Planning for Autonomous Neuroendovascular Navigation](https://arxiv.org/abs/2601.07945)
*Aabha Tamhankar,Ron Alterovitz,Ajit S. Puri,Giovanni Pittiglio*

Main category: cs.RO

TL;DR: 提出一种用于神经血管导航的确定性、时间高效且考虑接触的路径规划器，通过智能预测和利用与解剖结构的交互来导航预弯曲被动工具。


<details>
  <summary>Details</summary>
Motivation: 神经血管导航需要精确控制预弯曲被动工具在复杂血管结构中移动，传统方法难以高效处理工具与血管壁的接触交互，需要开发能够智能利用接触信息的路径规划算法。

Method: 基于术前和术中血管图像信息，推导运动学模型，采用基于采样的规划器进行树扩展，利用简化运动基元，智能预测和利用工具与解剖结构的交互。

Result: 在最坏情况下算法在22.8秒内达到100%收敛率，跟踪误差小于0.64毫米，在约94%患者的代表性解剖体模上验证有效。

Conclusion: 该方法能够快速计算可行路径，精度损失可忽略，在多样化和代表性的血管解剖结构中表现出色，为神经血管导航提供了高效可靠的路径规划解决方案。

Abstract: We propose a deterministic and time-efficient contact-aware path planner for neurovascular navigation. The algorithm leverages information from pre- and intra-operative images of the vessels to navigate pre-bent passive tools, by intelligently predicting and exploiting interactions with the anatomy. A kinematic model is derived and employed by the sampling-based planner for tree expansion that utilizes simplified motion primitives. This approach enables fast computation of the feasible path, with negligible loss in accuracy, as demonstrated in diverse and representative anatomies of the vessels. In these anatomical demonstrators, the algorithm shows a 100% convergence rate within 22.8s in the worst case, with sub-millimeter tracking errors (less than 0.64 mm), and is found effective on anatomical phantoms representative of around 94% of patients.

</details>


### [2] [Fiducial Exoskeletons: Image-Centric Robot State Estimation](https://arxiv.org/abs/2601.08034)
*Cameron Smith,Basile Van Hoorick,Vitor Guizilini,Yue Wang*

Main category: cs.RO

TL;DR: Fiducial Exoskeletons：一种基于图像的3D机器人状态估计方法，通过单张RGB图像实现机器人-相机外参估计、连杆6D姿态和关节角度状态恢复，简化了传统繁琐的标定流程。


<details>
  <summary>Details</summary>
Motivation: 传统机器人状态估计方法（特别是机器人-相机外参估计）依赖高精度执行器和耗时的手眼标定流程，而现代基于学习的机器人控制越来越多地使用低成本硬件上的RGB观测。需要一种更简单、更鲁棒的图像基状态估计方法。

Method: 1. 将机器人状态估计重构为从单张RGB图像估计每个连杆的6D姿态；2. 引入"基准外骨骼"：在每个连杆上安装已知几何关系的基准标记的3D打印支架；3. 通过轻量级全局优化强制运动学一致性，从观测到的连杆姿态恢复关节状态（可选择用编码器读数预热）。

Result: 在低成本机械臂上验证，基准外骨骼显著简化了设置过程，同时提高了标定精度、状态准确性和下游3D控制性能。即使在断电机器人上也能实现鲁棒的状态估计。

Conclusion: 基准外骨骼提供了一种简单而鲁棒的图像基机器人状态估计方法，实现了算法-硬件协同设计，为机器人状态估计提供了新的范式，并开源了代码和可打印硬件设计。

Abstract: We introduce Fiducial Exoskeletons, an image-based reformulation of 3D robot state estimation that replaces cumbersome procedures and motor-centric pipelines with single-image inference. Traditional approaches - especially robot-camera extrinsic estimation - often rely on high-precision actuators and require time-consuming routines such as hand-eye calibration. In contrast, modern learning-based robot control is increasingly trained and deployed from RGB observations on lower-cost hardware.
  Our key insight is twofold. First, we cast robot state estimation as 6D pose estimation of each link from a single RGB image: the robot-camera base transform is obtained directly as the estimated base-link pose, and the joint state is recovered via a lightweight global optimization that enforces kinematic consistency with the observed link poses (optionally warm-started with encoder readings). Second, we make per-link 6D pose estimation robust and simple - even without learning - by introducing the fiducial exoskeleton: a lightweight 3D-printed mount with a fiducial marker on each link and known marker-link geometry.
  This design yields robust camera-robot extrinsics, per-link SE(3) poses, and joint-angle state from a single image, enabling robust state estimation even on unplugged robots. Demonstrated on a low-cost robot arm, fiducial exoskeletons substantially simplify setup while improving calibration, state accuracy, and downstream 3D control performance. We release code and printable hardware designs to enable further algorithm-hardware co-design.

</details>


### [3] [Efficient Incremental SLAM via Information-Guided and Selective Optimization](https://arxiv.org/abs/2601.08110)
*Reza Arablouei*

Main category: cs.RO

TL;DR: 提出了一种高效的增量SLAM后端方法，结合信息引导门控和选择性部分优化，在保持批量优化精度的同时显著降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 传统增量SLAM方法在实时动态数据丰富环境中面临计算效率与精度平衡的挑战，需要一种既能保持全局一致性又能高效利用计算资源的方法。

Method: 结合信息引导门控（基于信息矩阵对数行列式的信息理论准则）和选择性部分优化（限制每次迭代到受新测量影响最大的变量子集），动态调整优化范围。

Result: 在基准SLAM数据集上的实验表明，该方法在保持批量求解器估计精度的同时，相比传统增量方法实现了显著的计算节省。

Conclusion: 该方法提供了精度与效率之间的原则性平衡，为动态数据丰富环境中的实时操作提供了稳健且可扩展的解决方案。

Abstract: We present an efficient incremental SLAM back-end that achieves the accuracy of full batch optimization while substantially reducing computational cost. The proposed approach combines two complementary ideas: information-guided gating (IGG) and selective partial optimization (SPO). IGG employs an information-theoretic criterion based on the log-determinant of the information matrix to quantify the contribution of new measurements, triggering global optimization only when a significant information gain is observed. This avoids unnecessary relinearization and factorization when incoming data provide little additional information. SPO executes multi-iteration Gauss-Newton (GN) updates but restricts each iteration to the subset of variables most affected by the new measurements, dynamically refining this active set until convergence. Together, these mechanisms retain all measurements to preserve global consistency while focusing computation on parts of the graph where it yields the greatest benefit. We provide theoretical analysis showing that the proposed approach maintains the convergence guarantees of full GN. Extensive experiments on benchmark SLAM datasets show that our approach consistently matches the estimation accuracy of batch solvers, while achieving significant computational savings compared to conventional incremental approaches. The results indicate that the proposed approach offers a principled balance between accuracy and efficiency, making it a robust and scalable solution for real-time operation in dynamic data-rich environments.

</details>


### [4] [A Pin-Array Structure for Gripping and Shape Recognition of Convex and Concave Terrain Profiles](https://arxiv.org/abs/2601.08143)
*Takuya Kato,Kentaro Uno,Kazuya Yoshida*

Main category: cs.RO

TL;DR: 开发了一种用于极端环境移动机器人的夹持器，能够抓握和识别地形形状，采用针阵列结构同时实现自适应抓握和地形测量


<details>
  <summary>Details</summary>
Motivation: 多肢攀爬机器人在崎岖地形（如悬崖和洞穴墙壁）上有效，但在未知自然环境中可能因误抓表面或失去可抓握点而摔倒或卡住。需要一种既能自适应抓握不规则地形，又能准确测量地形形状的夹持器。

Method: 开发了一种采用针阵列结构的夹持器，能够抓握凸面和凹面地形，并同时测量地形形状。通过原型机评估了其抓握和地形识别性能。

Result: 该夹持器能够有效抓握不规则地形并测量地形形状，针阵列设计既适用于3D地形映射，也适用于不规则地形的自适应抓握。

Conclusion: 提出的针阵列夹持器解决了极端环境中移动机器人面临的抓握和地形识别问题，为在未知自然环境中安全导航提供了有效的解决方案。

Abstract: This paper presents a gripper capable of grasping and recognizing terrain shapes for mobile robots in extreme environments. Multi-limbed climbing robots with grippers are effective on rough terrains, such as cliffs and cave walls. However, such robots may fall over by misgrasping the surface or getting stuck owing to the loss of graspable points in unknown natural environments. To overcome these issues, we need a gripper capable of adaptive grasping to irregular terrains, not only for grasping but also for measuring the shape of the terrain surface accurately. We developed a gripper that can grasp both convex and concave terrains and simultaneously measure the terrain shape by introducing a pin-array structure. We demonstrated the mechanism of the gripper and evaluated its grasping and terrain recognition performance using a prototype. Moreover, the proposed pin-array design works well for 3D terrain mapping as well as adaptive grasping for irregular terrains.

</details>


### [5] [Robust Subpixel Localization of Diagonal Markers in Large-Scale Navigation via Multi-Layer Screening and Adaptive Matching](https://arxiv.org/abs/2601.08161)
*Jing Tao,Banglei Guan,Yang Shang,Shunkun Liang,Qifeng Yu*

Main category: cs.RO

TL;DR: 提出了一种用于大规模飞行导航的鲁棒高精度定位方法，通过多层角点筛选和自适应模板匹配解决复杂背景干扰和传统滑动窗口匹配计算效率低的问题


<details>
  <summary>Details</summary>
Motivation: 解决大规模飞行导航中复杂背景干扰导致的定位失败问题，以及传统滑动窗口匹配技术计算效率低下的局限性

Method: 采用三层框架：1）通过光照均衡和结构信息提取降低维度；2）粗到细候选点选择策略减少滑动窗口计算成本；3）为候选点生成自适应模板，通过改进的模板匹配和相关系数极值拟合实现亚像素精度

Result: 实验结果表明该方法在复杂大规模环境中有效提取和定位对角标记，适用于导航任务中的视场测量

Conclusion: 该方法能够在大规模复杂环境中实现鲁棒、高精度的定位，特别适用于飞行导航应用

Abstract: This paper proposes a robust, high-precision positioning methodology to address localization failures arising from complex background interference in large-scale flight navigation and the computational inefficiency inherent in conventional sliding window matching techniques. The proposed methodology employs a three-tiered framework incorporating multi-layer corner screening and adaptive template matching. Firstly, dimensionality is reduced through illumination equalization and structural information extraction. A coarse-to-fine candidate selection strategy minimizes sliding window computational costs, enabling rapid estimation of the marker's position. Finally, adaptive templates are generated for candidate points, achieving subpixel precision through improved template matching with correlation coefficient extremum fitting. Experimental results demonstrate the method's effectiveness in extracting and localizing diagonal markers in complex, large-scale environments, making it ideal for field-of-view measurement in navigation tasks.

</details>


### [6] [FSAG: Enhancing Human-to-Dexterous-Hand Finger-Specific Affordance Grounding via Diffusion Models](https://arxiv.org/abs/2601.08246)
*Yifan Han,Pengfei Yi,Junyan Li,Hanqing Wang,Gaojing Zhang,Qi Peng Liu,Wenzhao Lian*

Main category: cs.RO

TL;DR: 提出基于预训练扩散模型的数据高效框架，从人类视频演示中提取语义先验，实现无需硬件特定数据集的多指灵巧抓取合成


<details>
  <summary>Details</summary>
Motivation: 灵巧抓取合成面临高维度和运动多样性挑战，现有方法依赖硬件特定的大规模数据集，限制了新灵巧手设计的可扩展性

Method: 从人类视频演示中提取时间对齐的精细抓取功能，与深度图像3D场景几何融合，通过运动学感知重定向模块映射到不同灵巧手

Result: 系统产生稳定、功能适当的多接触抓取，在常见物体和工具上可靠成功，对未见物体实例、姿态变化和不同手部实现具有强泛化能力

Conclusion: 通过人类演示和预训练生成模型，为可扩展、硬件无关的灵巧操作提供了一条路径，单一深度模态结合基础模型语义即可实现高性能抓取合成

Abstract: Dexterous grasp synthesis remains a central challenge: the high dimensionality and kinematic diversity of multi-fingered hands prevent direct transfer of algorithms developed for parallel-jaw grippers. Existing approaches typically depend on large, hardware-specific grasp datasets collected in simulation or through costly real-world trials, hindering scalability as new dexterous hand designs emerge. To this end, we propose a data-efficient framework, which is designed to bypass robot grasp data collection by exploiting the rich, object-centric semantic priors latent in pretrained generative diffusion models. Temporally aligned and fine-grained grasp affordances are extracted from raw human video demonstrations and fused with 3D scene geometry from depth images to infer semantically grounded contact targets. A kinematics-aware retargeting module then maps these affordance representations to diverse dexterous hands without per-hand retraining. The resulting system produces stable, functionally appropriate multi-contact grasps that remain reliably successful across common objects and tools, while exhibiting strong generalization across previously unseen object instances within a category, pose variations, and multiple hand embodiments. This work (i) introduces a semantic affordance extraction pipeline leveraging vision-language generative priors for dexterous grasping, (ii) demonstrates cross-hand generalization without constructing hardware-specific grasp datasets, and (iii) establishes that a single depth modality suffices for high-performance grasp synthesis when coupled with foundation-model semantics. Our results highlight a path toward scalable, hardware-agnostic dexterous manipulation driven by human demonstrations and pretrained generative models.

</details>


### [7] [ActiveVLA: Injecting Active Perception into Vision-Language-Action Models for Precise 3D Robotic Manipulation](https://arxiv.org/abs/2601.08325)
*Zhenyang Liu,Yongchong Gu,Yikai Wang,Xiangyang Xue,Yanwei Fu*

Main category: cs.RO

TL;DR: ActiveVLA：一种具有主动感知能力的视觉-语言-动作框架，通过粗到细的两阶段方法（关键区域定位和主动感知优化）实现高精度细粒度机器人操作


<details>
  <summary>Details</summary>
Motivation: 现有机器人操作研究大多依赖静态腕部摄像头，缺乏主动感知能力，无法在任务执行过程中自适应选择最优视角和分辨率，限制了在长时程任务和细粒度操作场景中的性能

Method: 采用粗到细的两阶段方法：1）关键区域定位：将3D输入投影到多视角2D投影，识别关键3D区域并支持动态空间感知；2）主动感知优化：基于定位的关键区域，使用主动视角选择策略选择最优视角（最大化模态相关性和多样性，最小化遮挡），并对关键区域进行3D放大以提高分辨率

Result: 在三个仿真基准测试中优于现有最先进基线，实现了精确的3D操作，并能无缝迁移到现实世界场景，使机器人能够在复杂环境中学习高精度任务

Conclusion: ActiveVLA框架通过赋予机器人主动感知能力，有效解决了现有视觉-语言-动作模型在细粒度操作中的局限性，为高精度机器人操作提供了新的解决方案

Abstract: Recent advances in robot manipulation have leveraged pre-trained vision-language models (VLMs) and explored integrating 3D spatial signals into these models for effective action prediction, giving rise to the promising vision-language-action (VLA) paradigm. However, most existing approaches overlook the importance of active perception: they typically rely on static, wrist-mounted cameras that provide an end-effector-centric viewpoint. As a result, these models are unable to adaptively select optimal viewpoints or resolutions during task execution, which significantly limits their performance in long-horizon tasks and fine-grained manipulation scenarios. To address these limitations, we propose ActiveVLA, a novel vision-language-action framework that empowers robots with active perception capabilities for high-precision, fine-grained manipulation. ActiveVLA adopts a coarse-to-fine paradigm, dividing the process into two stages: (1) Critical region localization. ActiveVLA projects 3D inputs onto multi-view 2D projections, identifies critical 3D regions, and supports dynamic spatial awareness. (2) Active perception optimization. Drawing on the localized critical regions, ActiveVLA uses an active view selection strategy to choose optimal viewpoints. These viewpoints aim to maximize amodal relevance and diversity while minimizing occlusions. Additionally, ActiveVLA applies a 3D zoom-in to improve resolution in key areas. Together, these steps enable finer-grained active perception for precise manipulation. Extensive experiments demonstrate that ActiveVLA achieves precise 3D manipulation and outperforms state-of-the-art baselines on three simulation benchmarks. Moreover, ActiveVLA transfers seamlessly to real-world scenarios, enabling robots to learn high-precision tasks in complex environments.

</details>


### [8] [Large Language Models to Enhance Multi-task Drone Operations in Simulated Environments](https://arxiv.org/abs/2601.08405)
*Yizhan Feng,Hichem Snoussi,Jing Teng,Abel Cherouat,Tian Wang*

Main category: cs.RO

TL;DR: 本文提出了一种将微调CodeT5模型与基于Unreal Engine的AirSim无人机模拟器相结合的方法，通过自然语言命令高效执行多任务操作，降低无人机操作门槛。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的快速发展，人机交互迎来了前所未有的机遇。本文旨在利用自然语言命令简化无人机操作，降低用户使用门槛，通过模拟环境实现复杂场景下的无人机应用。

Method: 方法包括：1) 将微调的CodeT5模型与AirSim无人机模拟器集成；2) 使用ChatGPT生成的自然语言-程序代码命令执行对数据集和开发者编写的无人机代码作为训练数据；3) 在AirSim中构建视觉逼真的动态环境模拟复杂场景；4) 实现自然语言到可执行无人机任务代码的自动翻译。

Result: 实验结果表明，所提方法在模拟环境中表现出优越的任务执行效率和命令理解能力。模型能够准确地将自然语言命令转换为可执行的无人机控制代码。

Conclusion: 该方法成功实现了通过自然语言控制模拟无人机，降低了操作门槛。未来计划以模块化方式扩展模型功能，增强其对复杂场景的适应性，推动无人机技术在现实环境中的应用。

Abstract: Benefiting from the rapid advancements in large language models (LLMs), human-drone interaction has reached unprecedented opportunities. In this paper, we propose a method that integrates a fine-tuned CodeT5 model with the Unreal Engine-based AirSim drone simulator to efficiently execute multi-task operations using natural language commands. This approach enables users to interact with simulated drones through prompts or command descriptions, allowing them to easily access and control the drone's status, significantly lowering the operational threshold. In the AirSim simulator, we can flexibly construct visually realistic dynamic environments to simulate drone applications in complex scenarios. By combining a large dataset of (natural language, program code) command-execution pairs generated by ChatGPT with developer-written drone code as training data, we fine-tune the CodeT5 to achieve automated translation from natural language to executable code for drone tasks. Experimental results demonstrate that the proposed method exhibits superior task execution efficiency and command understanding capabilities in simulated environments. In the future, we plan to extend the model functionality in a modular manner, enhancing its adaptability to complex scenarios and driving the application of drone technologies in real-world environments.

</details>


### [9] [Teaching Robots Like Dogs: Learning Agile Navigation from Luring, Gesture, and Speech](https://arxiv.org/abs/2601.08422)
*Taerim Yoon,Dongho Kang,Jin Cheng,Fatemeh Zargarbashi,Yijiang Huang,Minsung Ahn,Stelian Coros,Sungjoon Choi*

Main category: cs.RO

TL;DR: 提出人机交互框架，让四足机器人通过少量人类演示数据学习理解社交线索和导航行为，使用物理仿真重建交互场景，通过渐进式目标提示策略实现高效学习。


<details>
  <summary>Details</summary>
Motivation: 让四足机器人能够理解人类社交线索并产生适当行为，但传统物理交互学习需要大量人类数据，给用户带来沉重负担，需要更高效的学习方法。

Method: 提出人机交互框架，使用多模态自然输入（手势和语音命令），通过物理仿真重建交互场景并聚合数据以缓解分布偏移，采用渐进式目标提示策略在训练中自适应提供命令和导航目标。

Result: 在六个真实世界敏捷导航场景（包括跳跃或避开障碍物）中评估，方法在几乎所有试验中都成功，总演示数据少于1小时的情况下达到97.15%的任务成功率。

Conclusion: 该框架能够以数据高效的方式让机器人学习导航行为，实现人类输入与机器人行为之间的强对齐，在真实世界场景中表现出色。

Abstract: In this work, we aim to enable legged robots to learn how to interpret human social cues and produce appropriate behaviors through physical human guidance. However, learning through physical engagement can place a heavy burden on users when the process requires large amounts of human-provided data. To address this, we propose a human-in-the-loop framework that enables robots to acquire navigational behaviors in a data-efficient manner and to be controlled via multimodal natural human inputs, specifically gestural and verbal commands. We reconstruct interaction scenes using a physics-based simulation and aggregate data to mitigate distributional shifts arising from limited demonstration data. Our progressive goal cueing strategy adaptively feeds appropriate commands and navigation goals during training, leading to more accurate navigation and stronger alignment between human input and robot behavior. We evaluate our framework across six real-world agile navigation scenarios, including jumping over or avoiding obstacles. Our experimental results show that our proposed method succeeds in almost all trials across these scenarios, achieving a 97.15% task success rate with less than 1 hour of demonstration data in total.

</details>


### [10] [Large Multimodal Models for Embodied Intelligent Driving: The Next Frontier in Self-Driving?](https://arxiv.org/abs/2601.08434)
*Long Zhang,Yuchen Xia*

Main category: cs.RO

TL;DR: 提出语义与策略双驱动混合决策框架，结合大语言模型和深度强化学习，解决自动驾驶在开放世界场景中的持续学习和联合决策问题


<details>
  <summary>Details</summary>
Motivation: 现有模块化自动驾驶设计在需要持续环境理解和逻辑推理的开放世界场景中存在局限性，而仅依赖大语言模型增强具身智能驾驶无法实现联合决策，需要新的框架来确保持续学习和联合决策能力

Method: 提出语义与策略双驱动混合决策框架，融合大语言模型进行语义理解和认知表征，结合深度强化学习进行实时策略优化，通过实验案例验证在车道变换规划任务中的性能优势

Result: 通过实验案例验证了该框架在完成车道变换规划任务中的性能优越性，展示了其在具身智能驾驶中的有效性

Conclusion: 该语义与策略双驱动混合决策框架能够有效解决自动驾驶在开放世界场景中的持续学习和联合决策挑战，为具身智能驾驶提供了新的研究方向，并指出了多个未来研究方向

Abstract: The advent of Large Multimodal Models (LMMs) offers a promising technology to tackle the limitations of modular design in autonomous driving, which often falters in open-world scenarios requiring sustained environmental understanding and logical reasoning. Besides, embodied artificial intelligence facilitates policy optimization through closed-loop interactions to achieve the continuous learning capability, thereby advancing autonomous driving toward embodied intelligent (El) driving. However, such capability will be constrained by relying solely on LMMs to enhance EI driving without joint decision-making. This article introduces a novel semantics and policy dual-driven hybrid decision framework to tackle this challenge, ensuring continuous learning and joint decision. The framework merges LMMs for semantic understanding and cognitive representation, and deep reinforcement learning (DRL) for real-time policy optimization. We starts by introducing the foundational principles of EI driving and LMMs. Moreover, we examine the emerging opportunities this framework enables, encompassing potential benefits and representative use cases. A case study is conducted experimentally to validate the performance superiority of our framework in completing lane-change planning task. Finally, several future research directions to empower EI driving are identified to guide subsequent work.

</details>


### [11] [Real2Sim based on Active Perception with automatically VLM-generated Behavior Trees](https://arxiv.org/abs/2601.08454)
*Alessandro Adami,Sebastian Zudaire,Ruggero Carli,Pietro Falco*

Main category: cs.RO

TL;DR: 提出了一种基于行为树的Real2Sim框架，通过视觉语言模型进行多模态推理，自主生成任务特定的物理交互行为来估计所需物理参数，无需预定义任务模板或专家设计的探索程序。


<details>
  <summary>Details</summary>
Motivation: 传统Real2Sim流水线依赖手动测量或固定的预编程探索程序，限制了其对不同任务和用户意图的适应性。需要一种能够自主获取特定仿真目标所需参数的方法。

Method: 给定高级用户请求、不完整的仿真描述和场景RGB观测，使用视觉语言模型进行多模态推理识别相关对象、推断所需物理参数，并生成由基本机器人动作组成的结构化行为树。在扭矩控制的Franka Emika Panda上执行这些行为，通过顺应性、接触丰富的交互进行参数估计。

Result: 在真实机械臂上的实验结果表明，该方法能够在多种场景下估计物体质量、表面高度和摩擦相关参数，包括遮挡物体和不完整先验模型的情况。

Conclusion: 所提出的方法实现了可解释、意图驱动和自主的Real2Sim流水线，将高级推理与物理基础的机器人交互相结合。

Abstract: Constructing an accurate simulation model of real-world environments requires reliable estimation of physical parameters such as mass, geometry, friction, and contact surfaces. Traditional real-to-simulation (Real2Sim) pipelines rely on manual measurements or fixed, pre-programmed exploration routines, which limit their adaptability to varying tasks and user intents. This paper presents a Real2Sim framework that autonomously generates and executes Behavior Trees for task-specific physical interactions to acquire only the parameters required for a given simulation objective, without relying on pre-defined task templates or expert-designed exploration routines. Given a high-level user request, an incomplete simulation description, and an RGB observation of the scene, a vision-language model performs multi-modal reasoning to identify relevant objects, infer required physical parameters, and generate a structured Behavior Tree composed of elementary robotic actions. The resulting behavior is executed on a torque-controlled Franka Emika Panda, enabling compliant, contact-rich interactions for parameter estimation. The acquired measurements are used to automatically construct a physics-aware simulation. Experimental results on the real manipulator demonstrate estimation of object mass, surface height, and friction-related quantities across multiple scenarios, including occluded objects and incomplete prior models. The proposed approach enables interpretable, intent-driven, and autonomously Real2Sim pipelines, bridging high-level reasoning with physically-grounded robotic interaction.

</details>


### [12] [AME-2: Agile and Generalized Legged Locomotion via Attention-Based Neural Map Encoding](https://arxiv.org/abs/2601.08485)
*Chong Zhang,Victor Klemm,Fan Yang,Marco Hutter*

Main category: cs.RO

TL;DR: AME-2是一个统一的强化学习框架，通过注意力机制的地图编码器和基于学习的映射管道，实现了四足和双足机器人在复杂地形上的敏捷和泛化运动。


<details>
  <summary>Details</summary>
Motivation: 现有方法存在局限性：端到端感觉运动模型泛化性和可解释性有限；针对泛化运动的方法敏捷性不足且难以处理视觉遮挡。需要一种既能实现敏捷运动又能良好泛化到未知地形的统一框架。

Method: 1. 提出注意力机制的地图编码器，提取局部和全局地图特征，聚焦关键区域，为RL控制提供可解释的泛化嵌入。2. 开发基于学习的映射管道，将深度观测快速转换为带不确定性的局部高程图，融合里程计信息，对噪声和遮挡具有鲁棒性。3. 映射管道与并行仿真集成，支持在线映射训练控制器，促进仿真到现实的迁移。

Result: 在四足和双足机器人上验证了AME-2框架，控制器在仿真和真实世界实验中表现出强大的敏捷性和对未知地形的泛化能力。

Conclusion: AME-2框架通过注意力机制的地图编码器和鲁棒的映射管道，成功实现了敏捷且泛化的腿部运动，解决了现有方法在可解释性、泛化能力和处理遮挡方面的局限性。

Abstract: Achieving agile and generalized legged locomotion across terrains requires tight integration of perception and control, especially under occlusions and sparse footholds. Existing methods have demonstrated agility on parkour courses but often rely on end-to-end sensorimotor models with limited generalization and interpretability. By contrast, methods targeting generalized locomotion typically exhibit limited agility and struggle with visual occlusions. We introduce AME-2, a unified reinforcement learning (RL) framework for agile and generalized locomotion that incorporates a novel attention-based map encoder in the control policy. This encoder extracts local and global mapping features and uses attention mechanisms to focus on salient regions, producing an interpretable and generalized embedding for RL-based control. We further propose a learning-based mapping pipeline that provides fast, uncertainty-aware terrain representations robust to noise and occlusions, serving as policy inputs. It uses neural networks to convert depth observations into local elevations with uncertainties, and fuses them with odometry. The pipeline also integrates with parallel simulation so that we can train controllers with online mapping, aiding sim-to-real transfer. We validate AME-2 with the proposed mapping pipeline on a quadruped and a biped robot, and the resulting controllers demonstrate strong agility and generalization to unseen terrains in simulation and in real-world experiments.

</details>


### [13] [AUV Trajectory Learning for Underwater Acoustic Energy Transfer and Age Minimization](https://arxiv.org/abs/2601.08491)
*Mohamed Afouene Melki,Mohammad Shehab,Mohamed-Slim Alouini*

Main category: cs.RO

TL;DR: 本文提出了一种可持续的水下物联网(IoUT)解决方案，通过自主水下航行器(AUV)同时实现信息上行和声能传输，采用深度强化学习算法优化年龄信息和公平性指标。


<details>
  <summary>Details</summary>
Motivation: 传统水下物联网设备依赖电池供电，存在寿命有限和废弃后环境危害的问题，需要可持续的解决方案来支持长期水下监测和运维。

Method: 提出通过自主水下航行器(AUV)同时进行信息上行和声能传输的可持续方案，采用年龄信息(AoI)和Jain公平性指数作为评估指标，开发了两种深度强化学习算法：高性能的频分双工(FDD)方案和中等性能的时分双工(TDD)方案。

Result: 提出的FDD和TDD解决方案相比基线方法显著降低了平均年龄信息(AoI)，提高了能量收集效率和数据收集的公平性。

Conclusion: 该研究为水下物联网提供了一种可持续的解决方案，通过AUV同时实现信息传输和能量补给，有望使水下设备实现无限期运行，深度强化学习算法在优化系统性能方面表现出色。

Abstract: Internet of underwater things (IoUT) is increasingly gathering attention with the aim of monitoring sea life and deep ocean environment, underwater surveillance as well as maintenance of underwater installments. However, conventional IoUT devices, reliant on battery power, face limitations in lifespan and pose environmental hazards upon disposal. This paper introduces a sustainable approach for simultaneous information uplink from the IoUT devices and acoustic energy transfer (AET) to the devices via an autonomous underwater vehicle (AUV), potentially enabling them to operate indefinitely. To tackle the time-sensitivity, we adopt age of information (AoI), and Jain's fairness index. We develop two deep-reinforcement learning (DRL) algorithms, offering a high-complexity, high-performance frequency division duplex (FDD) solution and a low-complexity, medium-performance time division duplex (TDD) approach. The results elucidate that the proposed FDD and TDD solutions significantly reduce the average AoI and boost the harvested energy as well as data collection fairness compared to baseline approaches.

</details>


### [14] [Simplifying ROS2 controllers with a modular architecture for robot-agnostic reference generation](https://arxiv.org/abs/2601.08514)
*Davide Risi,Vincenzo Petrone,Antonio Langella,Lorenzo Pagliara,Enrico Ferrentino,Pasquale Chiacchio*

Main category: cs.RO

TL;DR: 本文提出了一种新颖的ROS2模块化架构，通过解耦参考获取、验证和插值逻辑与控制律跟踪，提高了机器人控制器的可重用性和代码复用性。


<details>
  <summary>Details</summary>
Motivation: 现有ROS2控制器中，参考处理代码（获取、验证、插值）与控制律代码耦合在一起，导致代码重复和跨平台重用困难。需要一种架构来分离这些关注点，提高控制器的模块化和可重用性。

Method: 设计了一个名为"参考生成器"的专用组件，负责从外部节点（如规划器）接收参考（单点或轨迹），并通过ros2_control链式机制以控制器采样周期向下游控制器写入单点参考。实现了两种参考生成器：关节空间参考生成器和笛卡尔空间参考生成器，并开发了新的控制器（PD重力补偿、笛卡尔位姿、导纳控制器）。

Result: 在模拟和真实机器人（Universal Robots和Franka Emika机械臂）上验证表明：(1) 所有测试场景中参考都能可靠跟踪；(2) 参考生成器减少了链式控制器中的重复参考处理代码，有利于构建和重用复杂控制器管道；(3) 控制器实现仅专注于控制律。

Conclusion: 提出的模块化架构成功分离了参考处理和控制律关注点，提高了ROS2控制器的代码复用性和跨平台重用性，为构建复杂控制器管道提供了更好的支持。

Abstract: This paper introduces a novel modular architecture for ROS2 that decouples the logic required to acquire, validate, and interpolate references from the control laws that track them. The design includes a dedicated component, named Reference Generator, that receives references, in the form of either single points or trajectories, from external nodes (e.g., planners), and writes single-point references at the controller's sampling period via the existing ros2_control chaining mechanism to downstream controllers. This separation removes duplicated reference-handling code from controllers and improves reusability across robot platforms. We implement two reference generators: one for handling joint-space references and one for Cartesian references, along with a set of new controllers (PD with gravity compensation, Cartesian pose, and admittance controllers) and validate the approach on simulated and real Universal Robots and Franka Emika manipulators. Results show that (i) references are tracked reliably in all tested scenarios, (ii) reference generators reduce duplicated reference-handling code across chained controllers to favor the construction and reuse of complex controller pipelines, and (iii) controller implementations remain focused only on control laws.

</details>


### [15] [Keyframe-based Dense Mapping with the Graph of View-Dependent Local Maps](https://arxiv.org/abs/2601.08520)
*Krzysztof Zielinski,Dominik Belter*

Main category: cs.RO

TL;DR: 提出了一种基于关键帧的NDT地图构建系统，利用RGB-D传感器数据更新局部NDT地图，通过2D视角相关结构存储NDT单元，并整合到姿态图中实现回环检测后的全局地图校正。


<details>
  <summary>Details</summary>
Motivation: 现有地图构建方法未能充分利用RGB-D相机的特性和不确定性模型，需要一种能够更好地处理RGB-D数据、自然表示物体距离与精度关系的地图构建系统。

Method: 使用RGB-D传感器数据更新局部NDT地图，将NDT单元存储在2D视角相关结构中，局部地图存储在姿态图中，提出局部地图合并和过滤程序以获得全局地图。

Result: 与Octomap和NDT-OM方法进行了比较，展示了所提地图构建方法的应用示例，证明了方法的有效性。

Conclusion: 提出的基于关键帧的NDT地图构建系统能够更好地利用RGB-D相机特性，自然实现距离相机原点越近的物体表示精度越高，并通过姿态图实现全局地图校正。

Abstract: In this article, we propose a new keyframe-based mapping system. The proposed method updates local Normal Distribution Transform maps (NDT) using data from an RGB-D sensor. The cells of the NDT are stored in 2D view-dependent structures to better utilize the properties and uncertainty model of RGB-D cameras. This method naturally represents an object closer to the camera origin with higher precision. The local maps are stored in the pose graph which allows correcting global map after loop closure detection. We also propose a procedure that allows merging and filtering local maps to obtain a global map of the environment. Finally, we compare our method with Octomap and NDT-OM and provide example applications of the proposed mapping method.

</details>


### [16] [QP-Based Control of an Underactuated Aerial Manipulator under Constraints](https://arxiv.org/abs/2601.08523)
*Nesserine Laribi,Mohammed Rida Mokhtari,Abdelaziz Benallegue,Abdelhafid El-Hadri,Mehdi Benallegue*

Main category: cs.RO

TL;DR: 本文提出了一种针对欠驱动空中机械臂的约束感知控制框架，能够在明确考虑安全和可行性约束的同时实现精确的末端执行器轨迹跟踪。


<details>
  <summary>Details</summary>
Motivation: 针对欠驱动空中机械臂在复杂环境中执行任务时，需要同时满足轨迹跟踪精度、安全约束和系统可行性的挑战。现有方法往往难以在保证控制性能的同时明确处理各种物理约束和不确定性。

Method: 将控制问题表述为二次规划问题，计算满足欠驱动特性、执行器边界和系统约束的动态一致广义加速度。为增强鲁棒性，在力矩层面引入了无源积分作用，以应对扰动、建模不确定性和稳态误差，同时不损害可行性。

Result: 通过高保真物理仿真验证了方法的有效性，仿真包含了参数扰动、粘性关节摩擦以及真实的传感和状态估计效应。结果表明该方法能够实现精确跟踪、平滑控制输入，并在实际工况下可靠满足约束条件。

Conclusion: 提出的约束感知控制框架为欠驱动空中机械臂提供了一种有效的解决方案，能够在复杂约束条件下实现高性能控制，具有良好的实际应用前景。

Abstract: This paper presents a constraint-aware control framework for underactuated aerial manipulators, enabling accurate end-effector trajectory tracking while explicitly accounting for safety and feasibility constraints. The control problem is formulated as a quadratic program that computes dynamically consistent generalized accelerations subject to underactuation, actuator bounds, and system constraints. To enhance robustness against disturbances, modeling uncertainties, and steady-state errors, a passivity-based integral action is incorporated at the torque level without compromising feasibility. The effectiveness of the proposed approach is demonstrated through high-fidelity physics-based simulations, which include parameter perturbations, viscous joint friction, and realistic sensing and state-estimation effects. This demonstrates accurate tracking, smooth control inputs, and reliable constraint satisfaction under realistic operating conditions.

</details>


### [17] [VLingNav: Embodied Navigation with Adaptive Reasoning and Visual-Assisted Linguistic Memory](https://arxiv.org/abs/2601.08665)
*Shaoan Wang,Yuanfei Luo,Xingyu Chen,Aocheng Luo,Dongyue Li,Chang Liu,Sheng Chen,Yangang Zhang,Junzhi Yu*

Main category: cs.RO

TL;DR: VLingNav：一种基于语言驱动认知的VLA模型，通过自适应思维链和视觉辅助语言记忆模块，在具身导航中实现状态最先进的性能，并能零样本迁移到真实机器人平台。


<details>
  <summary>Details</summary>
Motivation: 现有VLA模型在具身导航中主要依赖从观察到动作的被动映射，缺乏复杂、长时程导航任务所需的显式推理能力和持久记忆。

Method: 1. 引入自适应思维链机制，根据人类认知双过程理论动态触发显式推理；2. 开发视觉辅助语言记忆模块，构建跨模态语义记忆；3. 构建Nav-AdaCoT-2.9M数据集，包含自适应CoT标注；4. 采用在线专家引导强化学习训练方法。

Result: VLingNav在广泛的具身导航基准测试中实现了最先进的性能，能够零样本迁移到真实机器人平台，执行各种导航任务，并展现出强大的跨域和跨任务泛化能力。

Conclusion: VLingNav通过语言驱动认知方法，结合自适应推理和持久记忆，显著提升了VLA模型在复杂具身导航任务中的性能，为具身智能的发展提供了新思路。

Abstract: VLA models have shown promising potential in embodied navigation by unifying perception and planning while inheriting the strong generalization abilities of large VLMs. However, most existing VLA models rely on reactive mappings directly from observations to actions, lacking the explicit reasoning capabilities and persistent memory required for complex, long-horizon navigation tasks. To address these challenges, we propose VLingNav, a VLA model for embodied navigation grounded in linguistic-driven cognition. First, inspired by the dual-process theory of human cognition, we introduce an adaptive chain-of-thought mechanism, which dynamically triggers explicit reasoning only when necessary, enabling the agent to fluidly switch between fast, intuitive execution and slow, deliberate planning. Second, to handle long-horizon spatial dependencies, we develop a visual-assisted linguistic memory module that constructs a persistent, cross-modal semantic memory, enabling the agent to recall past observations to prevent repetitive exploration and infer movement trends for dynamic environments. For the training recipe, we construct Nav-AdaCoT-2.9M, the largest embodied navigation dataset with reasoning annotations to date, enriched with adaptive CoT annotations that induce a reasoning paradigm capable of adjusting both when to think and what to think about. Moreover, we incorporate an online expert-guided reinforcement learning stage, enabling the model to surpass pure imitation learning and to acquire more robust, self-explored navigation behaviors. Extensive experiments demonstrate that VLingNav achieves state-of-the-art performance across a wide range of embodied navigation benchmarks. Notably, VLingNav transfers to real-world robotic platforms in a zero-shot manner, executing various navigation tasks and demonstrating strong cross-domain and cross-task generalization.

</details>


### [18] [Real-Time Localization Framework for Autonomous Basketball Robots](https://arxiv.org/abs/2601.08713)
*Naren Medarametla,Sreejon Mondal*

Main category: cs.RO

TL;DR: 本文提出了一种混合定位算法，结合经典技术和基于学习的方法，仅利用球场地面视觉数据实现篮球场上的自主定位。


<details>
  <summary>Details</summary>
Motivation: 定位是自主机器人的基本能力，在Robocon 2025竞赛中，准确可靠的定位对于提高射击精度、避免与其他机器人碰撞以及高效导航比赛场地至关重要。

Method: 提出混合定位算法，集成经典定位技术与基于学习的视觉方法，仅依赖球场地面视觉数据进行定位。

Result: 论文未在摘要中提供具体实验结果，但暗示该方法旨在实现篮球场上的自主定位。

Conclusion: 该方法旨在为Robocon 2025竞赛中的机器人提供准确可靠的定位能力，从而提高整体性能。

Abstract: Localization is a fundamental capability for autonomous robots, enabling them to operate effectively in dynamic environments. In Robocon 2025, accurate and reliable localization is crucial for improving shooting precision, avoiding collisions with other robots, and navigating the competition field efficiently. In this paper, we propose a hybrid localization algorithm that integrates classical techniques with learning based methods that rely solely on visual data from the court's floor to achieve self-localization on the basketball field.

</details>


### [19] [Older Adults' Preferences for Feedback Cadence from an Exercise Coach Robot](https://arxiv.org/abs/2601.08819)
*Roshni Kaushik,Reid Simmons*

Main category: cs.RO

TL;DR: 研究探索老年人对机器人运动教练不同节奏的言语和非言语反馈的反应，发现改变一种模态的节奏会影响两种模态的感知。


<details>
  <summary>Details</summary>
Motivation: 人们以不同方式回应反馈和指导，机器人需要个性化交互并利用言语和非言语沟通线索。特别关注老年人对机器人运动教练反馈节奏的反应，以优化人机交互设计。

Method: 通过在线研究，让老年人参与者评估机器人以不同节奏提供言语和非言语反馈的视频，分析不同节奏对感知的影响。

Result: 结果表明，改变一种模态（言语或非言语）的反馈节奏会影响对该模态和另一种模态的感知，两种模态之间存在相互影响。

Conclusion: 研究结果可用于优化机器人教练在老年人运动训练中的反馈频率设计，提高人机交互效果和用户体验。

Abstract: People can respond to feedback and guidance in different ways, and it is important for robots to personalize their interactions and utilize verbal and nonverbal communication cues. We aim to understand how older adults respond to different cadences of verbal and nonverbal feedback of a robot exercise coach. We conducted an online study of older adults, where participants evaluated videos of the robot giving feedback at different cadences for each modality. The results indicate that changing the cadence of one modality affects the perception of both it and the other modality. We can use the results from this study to better design the frequency of the robot coach's feedback during an exercise session with this population.

</details>
