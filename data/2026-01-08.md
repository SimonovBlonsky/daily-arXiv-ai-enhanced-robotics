<div id=toc></div>

# Table of Contents

- [cs.RO](#cs.RO) [Total: 15]


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [1] [Revisiting Continuous-Time Trajectory Estimation via Gaussian Processes and the Magnus Expansion](https://arxiv.org/abs/2601.03360)
*Timothy Barfoot,Cedric Le Gentil,Sven Lilge*

Main category: cs.RO

TL;DR: 本文提出了一种基于Magnus展开的全局高斯过程先验方法，用于李群上的连续时间状态估计，相比现有的局部线性时不变SDE核方法更加优雅和通用。


<details>
  <summary>Details</summary>
Motivation: 连续时间状态估计在处理异步高频率测量、引入估计平滑性、事后查询任意时间点估计值以及解决扫描移动传感器可观测性问题方面具有优势。现有李群状态估计方法采用局部线性时不变SDE核的高斯过程先验，虽然实用但缺乏理论优雅性。

Method: 采用Magnus展开推导李群上的全局高斯过程先验，基于线性时变随机微分方程生成先验的均值和协方差函数，为连续时间轨迹估计提供更优雅和通用的解决方案。

Result: 提出了基于Magnus展开的全局高斯过程先验方法，与现有的局部线性时不变SDE核方法进行了数值比较，并讨论了两种方法的相对优势。

Conclusion: 通过Magnus展开推导的李群全局高斯过程先验为连续时间轨迹估计提供了比现有局部方法更优雅和通用的理论框架，同时保持了实用效果。

Abstract: Continuous-time state estimation has been shown to be an effective means of (i) handling asynchronous and high-rate measurements, (ii) introducing smoothness to the estimate, (iii) post hoc querying the estimate at times other than those of the measurements, and (iv) addressing certain observability issues related to scanning-while-moving sensors. A popular means of representing the trajectory in continuous time is via a Gaussian process (GP) prior, with the prior's mean and covariance functions generated by a linear time-varying (LTV) stochastic differential equation (SDE) driven by white noise. When the state comprises elements of Lie groups, previous works have resorted to a patchwork of local GPs each with a linear time-invariant SDE kernel, which while effective in practice, lacks theoretical elegance. Here we revisit the full LTV GP approach to continuous-time trajectory estimation, deriving a global GP prior on Lie groups via the Magnus expansion, which offers a more elegant and general solution. We provide a numerical comparison between the two approaches and discuss their relative merits.

</details>


### [2] [Cost-Effective Radar Sensors for Field-Based Water Level Monitoring with Sub-Centimeter Accuracy](https://arxiv.org/abs/2601.03447)
*Anna Zavei-Boroda,J. Toby Minear,Kyle Harlow,Dusty Woods,Christoffer Heckman*

Main category: cs.RO

TL;DR: 该论文探索使用商用雷达传感器进行低成本水位监测，通过统计滤波技术实现厘米级精度，适用于无人机和机器人平台的自主水监测系统。


<details>
  <summary>Details</summary>
Motivation: 传统水位监测方法成本高、覆盖范围有限，而雷达传感具有非接触、环境适应性强等优势，可作为低成本替代方案。

Method: 评估商用雷达传感器在实际野外测试中的性能，应用统计滤波技术提高测量精度。

Result: 单个雷达传感器在最小化校准的情况下可实现厘米级精度，证明其作为自主水监测解决方案的实用性。

Conclusion: 雷达传感技术为低成本、高精度的水位监测提供了可行方案，特别适合与无人机和机器人平台集成，实现自主水监测。

Abstract: Water level monitoring is critical for flood management, water resource allocation, and ecological assessment, yet traditional methods remain costly and limited in coverage. This work explores radar-based sensing as a low-cost alternative for water level estimation, leveraging its non-contact nature and robustness to environmental conditions. Commercial radar sensors are evaluated in real-world field tests, applying statistical filtering techniques to improve accuracy. Results show that a single radar sensor can achieve centimeter-scale precision with minimal calibration, making it a practical solution for autonomous water monitoring using drones and robotic platforms.

</details>


### [3] [A Vision-Language-Action Model with Visual Prompt for OFF-Road Autonomous Driving](https://arxiv.org/abs/2601.03519)
*Liangdong Zhang,Yiming Nie,Haoyang Li,Fanjie Kong,Baobao Zhang,Shunxin Huang,Kai Fu,Chen Min,Liang Xiao*

Main category: cs.RO

TL;DR: OFF-EMMA是一个用于越野自动驾驶的端到端多模态框架，通过视觉提示块和链式思维自洽推理策略，显著提升了轨迹规划的准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统越野轨迹规划方法存在适应性有限的问题，而现有的视觉-语言-动作模型在越野场景中存在空间感知不足和推理不稳定的缺陷，需要新的解决方案。

Method: 设计了视觉提示块，利用语义分割掩码作为视觉提示来增强预训练视觉-语言模型的空间理解能力；引入链式思维自洽推理策略，通过多路径推理机制减少异常值对规划性能的影响。

Result: 在RELLIS-3D越野数据集上的实验表明，OFF-EMMA显著优于现有方法，将Qwen骨干模型的平均L2误差降低了13.3%，并将失败率从16.52%降至6.56%。

Conclusion: OFF-EMMA通过创新的视觉提示和推理策略，有效解决了越野自动驾驶中轨迹规划的空间感知不足和推理不稳定问题，为端到端多模态规划提供了有效解决方案。

Abstract: Efficient trajectory planning in off-road terrains presents a formidable challenge for autonomous vehicles, often necessitating complex multi-step pipelines. However, traditional approaches exhibit limited adaptability in dynamic environments. To address these limitations, this paper proposes OFF-EMMA, a novel end-to-end multimodal framework designed to overcome the deficiencies of insufficient spatial perception and unstable reasoning in visual-language-action (VLA) models for off-road autonomous driving scenarios. The framework explicitly annotates input images through the design of a visual prompt block and introduces a chain-of-thought with self-consistency (COT-SC) reasoning strategy to enhance the accuracy and robustness of trajectory planning. The visual prompt block utilizes semantic segmentation masks as visual prompts, enhancing the spatial understanding ability of pre-trained visual-language models for complex terrains. The COT- SC strategy effectively mitigates the error impact of outliers on planning performance through a multi-path reasoning mechanism. Experimental results on the RELLIS-3D off-road dataset demonstrate that OFF-EMMA significantly outperforms existing methods, reducing the average L2 error of the Qwen backbone model by 13.3% and decreasing the failure rate from 16.52% to 6.56%.

</details>


### [4] [From Score to Sound: An End-to-End MIDI-to-Motion Pipeline for Robotic Cello Performance](https://arxiv.org/abs/2601.03562)
*Samantha Sudhoff,Pranesh Velmurugan,Jiashu Liu,Vincent Zhao,Yung-Hsiang Lu,Kristen Yeon-Ji Yun*

Main category: cs.RO

TL;DR: 提出了一种端到端的MIDI乐谱到机器人运动管道，用于机器人演奏大提琴，无需运动捕捉，通过实时数据交换记录演奏数据，并引入音乐图灵测试进行性能评估。


<details>
  <summary>Details</summary>
Motivation: 机器人演奏弦乐器（如大提琴）面临挑战，需要精确控制弓的角度和压力来产生理想声音。现有机器人演奏者依赖昂贵的运动捕捉技术，且无法像人类一样视奏乐谱。

Method: 提出端到端的MIDI乐谱到机器人运动管道，使用Universal Robot Freedrive功能实现人类化声音，无需运动捕捉。通过实时数据交换记录关节数据，收集五首标准曲目的标记数据。引入音乐图灵测试评估性能，并提出残差强化学习方法改进控制。

Result: 132名人类参与者通过音乐图灵测试评估机器人演奏与人类基准的对比，建立了机器人演奏大提琴的首个基准。发布了人类参考录音供未来研究比较。

Conclusion: 该方法实现了无需运动捕捉的机器人演奏，建立了首个机器人演奏大提琴的评估基准，并展示了通过残差强化学习改进弦交叉效率和音质的未来机会。

Abstract: Robot musicians require precise control to obtain proper note accuracy, sound quality, and musical expression. Performance of string instruments, such as violin and cello, presents a significant challenge due to the precise control required over bow angle and pressure to produce the desired sound. While prior robotic cellists focus on accurate bowing trajectories, these works often rely on expensive motion capture techniques, and fail to sightread music in a human-like way.
  We propose a novel end-to-end MIDI score to robotic motion pipeline which converts musical input directly into collision-aware bowing motions for a UR5e robot cellist. Through use of Universal Robot Freedrive feature, our robotic musician can achieve human-like sound without the need for motion capture. Additionally, this work records live joint data via Real-Time Data Exchange (RTDE) as the robot plays, providing labeled robotic playing data from a collection of five standard pieces to the research community. To demonstrate the effectiveness of our method in comparison to human performers, we introduce the Musical Turing Test, in which a collection of 132 human participants evaluate our robot's performance against a human baseline. Human reference recordings are also released, enabling direct comparison for future studies. This evaluation technique establishes the first benchmark for robotic cello performance. Finally, we outline a residual reinforcement learning methodology to improve upon baseline robotic controls, highlighting future opportunities for improved string-crossing efficiency and sound quality.

</details>


### [5] [Locomotion Beyond Feet](https://arxiv.org/abs/2601.03607)
*Tae Hoon Yang,Haochen Shi,Jiacheng Hu,Zhicong Zhang,Daniel Jiang,Weizhuo Wang,Yao He,Zhen Wu,Yuming Chen,Yifan Hou,Monroe Kennedy,Shuran Song,C. Karen Liu*

Main category: cs.RO

TL;DR: 该论文提出了"Locomotion Beyond Feet"系统，让人形机器人能够使用全身（包括手、膝盖、肘部）在极端复杂地形中实现稳健运动，结合了关键帧动画和强化学习的方法。


<details>
  <summary>Details</summary>
Motivation: 现有的人形机器人运动方法主要关注基于腿部的步态，但自然双足生物在复杂环境中经常依赖手、膝盖和肘部建立额外接触点以获得稳定性和支撑。需要开发能够在极端挑战性地形中实现全身运动的方法。

Method: 结合基于物理的关键帧动画和强化学习。关键帧编码人类运动技能知识，针对特定机器人形态设计，可在仿真或硬件中验证；强化学习将这些参考转化为稳健的物理精确运动。采用分层框架，包括地形特定运动跟踪策略、故障恢复机制和基于视觉的技能规划器。

Result: 真实世界实验表明，Locomotion Beyond Feet系统能够实现稳健的全身运动，并能泛化到不同障碍物尺寸、障碍物实例和地形序列中，成功应对低间隙空间、膝盖高墙、膝盖高平台和陡峭上下楼梯等极端地形。

Conclusion: 通过结合关键帧动画和强化学习的方法，成功开发了能够在极端复杂地形中实现稳健全身运动的人形机器人系统，超越了传统基于脚部的运动方法，展示了在多样化地形中的泛化能力。

Abstract: Most locomotion methods for humanoid robots focus on leg-based gaits, yet natural bipeds frequently rely on hands, knees, and elbows to establish additional contacts for stability and support in complex environments. This paper introduces Locomotion Beyond Feet, a comprehensive system for whole-body humanoid locomotion across extremely challenging terrains, including low-clearance spaces under chairs, knee-high walls, knee-high platforms, and steep ascending and descending stairs. Our approach addresses two key challenges: contact-rich motion planning and generalization across diverse terrains. To this end, we combine physics-grounded keyframe animation with reinforcement learning. Keyframes encode human knowledge of motor skills, are embodiment-specific, and can be readily validated in simulation or on hardware, while reinforcement learning transforms these references into robust, physically accurate motions. We further employ a hierarchical framework consisting of terrain-specific motion-tracking policies, failure recovery mechanisms, and a vision-based skill planner. Real-world experiments demonstrate that Locomotion Beyond Feet achieves robust whole-body locomotion and generalizes across obstacle sizes, obstacle instances, and terrain sequences.

</details>


### [6] [PointWorld: Scaling 3D World Models for In-The-Wild Robotic Manipulation](https://arxiv.org/abs/2601.03782)
*Wenlong Huang,Yu-Wei Chao,Arsalan Mousavian,Ming-Yu Liu,Dieter Fox,Kaichun Mo,Li Fei-Fei*

Main category: cs.RO

TL;DR: PointWorld是一个大规模预训练的3D世界模型，通过将状态和动作统一表示为3D点流来预测机器人动作对3D世界的响应，支持多种机器人平台和任务。


<details>
  <summary>Details</summary>
Motivation: 人类能够从一瞥和身体动作中预测3D世界的响应，这种能力对机器人操作同样重要。现有方法通常使用特定于具体机器人的动作空间，难以跨平台学习和集成。

Method: 将状态和动作统一表示为共享3D空间中的3D点流：给定RGB-D图像序列和低级机器人动作命令，预测每个像素在3D空间中的位移响应。使用包含约200万轨迹、500小时数据的大规模数据集训练，涵盖单臂Franka和双足人形机器人。

Result: PointWorld实现了实时（0.1秒）推理速度，可集成到模型预测控制框架中。单个预训练检查点使真实Franka机器人能够执行刚体推动、可变形和关节物体操作以及工具使用，无需演示或后训练。

Conclusion: 通过将动作表示为3D点流而非特定于具体机器人的动作空间，PointWorld实现了跨机器人平台的学习集成，为大规模3D世界建模提供了设计原则，展示了在真实世界机器人操作中的广泛应用潜力。

Abstract: Humans anticipate, from a glance and a contemplated action of their bodies, how the 3D world will respond, a capability that is equally vital for robotic manipulation. We introduce PointWorld, a large pre-trained 3D world model that unifies state and action in a shared 3D space as 3D point flows: given one or few RGB-D images and a sequence of low-level robot action commands, PointWorld forecasts per-pixel displacements in 3D that respond to the given actions. By representing actions as 3D point flows instead of embodiment-specific action spaces (e.g., joint positions), this formulation directly conditions on physical geometries of robots while seamlessly integrating learning across embodiments. To train our 3D world model, we curate a large-scale dataset spanning real and simulated robotic manipulation in open-world environments, enabled by recent advances in 3D vision and simulated environments, totaling about 2M trajectories and 500 hours across a single-arm Franka and a bimanual humanoid. Through rigorous, large-scale empirical studies of backbones, action representations, learning objectives, partial observability, data mixtures, domain transfers, and scaling, we distill design principles for large-scale 3D world modeling. With a real-time (0.1s) inference speed, PointWorld can be efficiently integrated in the model-predictive control (MPC) framework for manipulation. We demonstrate that a single pre-trained checkpoint enables a real-world Franka robot to perform rigid-body pushing, deformable and articulated object manipulation, and tool use, without requiring any demonstrations or post-training and all from a single image captured in-the-wild. Project website at https://point-world.github.io/.

</details>


### [7] [Generational Replacement and Learning for High-Performing and Diverse Populations in Evolvable Robots](https://arxiv.org/abs/2601.03807)
*K. Ege de Bruin,Kyrre Glette,Kai Olav Ellefsen*

Main category: cs.RO

TL;DR: 结合代际替换与个体生命周期内学习，可以在保持性能的同时增加进化机器人形态与控制协同优化中的种群多样性


<details>
  <summary>Details</summary>
Motivation: 进化机器人学中形态与控制的协同优化面临两个挑战：1）控制器需要时间适应不断演化的形态，导致有潜力的新设计难以进入种群；2）进化过程往往过早收敛，缺乏多样性。传统解决方案各有缺陷：个体生命周期内学习解决适应性问题，但代际替换增加多样性却会降低性能。

Method: 将代际替换（完全替换种群）与个体生命周期内学习（每个个体增加控制器优化循环）相结合。同时强调在研究形态演化机器人的学习时，需要关注不同的性能评估指标（基于函数评估次数 vs 基于进化代数）。

Result: 结合代际替换与个体生命周期内学习的方法能够增加种群多样性，同时保持性能水平。研究还发现，根据函数评估次数与根据进化代数评估性能可能会得出不同的结论。

Conclusion: 代际替换与个体生命周期内学习的结合是解决进化机器人学中形态-控制协同优化问题的有效方法，既能增加多样性又不牺牲性能。同时，选择合适的性能评估指标对于研究形态演化机器人的学习至关重要。

Abstract: Evolutionary Robotics offers the possibility to design robots to solve a specific task automatically by optimizing their morphology and control together. However, this co-optimization of body and control is challenging, because controllers need some time to adapt to the evolving morphology - which may make it difficult for new and promising designs to enter the evolving population. A solution to this is to add intra-life learning, defined as an additional controller optimization loop, to each individual in the evolving population. A related problem is the lack of diversity often seen in evolving populations as evolution narrows the search down to a few promising designs too quickly. This problem can be mitigated by implementing full generational replacement, where offspring robots replace the whole population. This solution for increasing diversity usually comes at the cost of lower performance compared to using elitism. In this work, we show that combining such generational replacement with intra-life learning can increase diversity while retaining performance. We also highlight the importance of performance metrics when studying learning in morphologically evolving robots, showing that evaluating according to function evaluations versus according to generations of evolution can give different conclusions.

</details>


### [8] [Integrating Sample Inheritance into Bayesian Optimization for Evolutionary Robotics](https://arxiv.org/abs/2601.03813)
*K. Ege de Bruin,Kyrre Glette,Kai Olav Ellefsen*

Main category: cs.RO

TL;DR: 该研究探讨了在进化机器人学中，通过贝叶斯优化和样本继承机制来解决形态-控制器协同优化问题，在有限学习预算下提升性能。


<details>
  <summary>Details</summary>
Motivation: 进化机器人学中自动设计机器人形态时，需要同时优化形态和控制器，形成身体-大脑协同优化问题。传统方法为每个新形态从头开始优化控制器，需要很高的学习预算。本研究旨在在有限控制器学习预算下，通过继承机制提升进化效率。

Method: 使用贝叶斯优化进行控制器优化，利用其样本效率和强探索能力。研究两种样本继承方式：(1) 将父代所有样本作为先验知识转移给子代而不重新评估；(2) 在子代上重新评估父代的最佳样本。这两种方法与无继承的基线进行比较，并在故意设置的低控制器学习预算下进行实验。

Result: 重新评估父代最佳样本的方法表现最好，基于先验的继承也优于无继承。分析表明，虽然单个形态的学习预算不足，但代际继承通过积累学习适应来弥补这一点。继承主要使与父代相似的子代形态受益。研究还发现环境的关键作用，更具挑战性的环境能产生更稳定的行走步态。

Conclusion: 继承机制可以在不需要大量学习预算的情况下提升进化机器人学的性能，为更高效的机器人设计提供了有效途径。拉马克式继承通过跨代积累学习适应，在有限预算下实现了更好的控制器优化。

Abstract: In evolutionary robotics, robot morphologies are designed automatically using evolutionary algorithms. This creates a body-brain optimization problem, where both morphology and control must be optimized together. A common approach is to include controller optimization for each morphology, but starting from scratch for every new body may require a high controller learning budget. We address this by using Bayesian optimization for controller optimization, exploiting its sample efficiency and strong exploration capabilities, and using sample inheritance as a form of Lamarckian inheritance. Under a deliberately low controller learning budget for each morphology, we investigate two types of sample inheritance: (1) transferring all the parent's samples to the offspring to be used as prior without evaluating them, and (2) reevaluating the parent's best samples on the offspring. Both are compared to a baseline without inheritance. Our results show that reevaluation performs best, with prior-based inheritance also outperforming no inheritance. Analysis reveals that while the learning budget is too low for a single morphology, generational inheritance compensates for this by accumulating learned adaptations across generations. Furthermore, inheritance mainly benefits offspring morphologies that are similar to their parents. Finally, we demonstrate the critical role of the environment, with more challenging environments resulting in more stable walking gaits. Our findings highlight that inheritance mechanisms can boost performance in evolutionary robotics without needing large learning budgets, offering an efficient path toward more capable robot design.

</details>


### [9] [Towards Safe Autonomous Driving: A Real-Time Motion Planning Algorithm on Embedded Hardware](https://arxiv.org/abs/2601.03904)
*Korbinian Moller,Glenn Johannes Tungka,Lucas Jürgens,Johannes Betz*

Main category: cs.RO

TL;DR: 该论文提出了一种用于自动驾驶的主动安全扩展方案，在嵌入式实时操作系统平台上部署轻量级采样轨迹规划器，为故障操作提供确定性时序保障。


<details>
  <summary>Details</summary>
Motivation: 现有自动驾驶安全验证方法（如在线验证）只能检测不可行的规划输出，缺乏在主规划器故障时确保安全运行的主动机制。需要为故障操作自动驾驶系统开发主动安全扩展。

Method: 在汽车级嵌入式平台上部署轻量级采样轨迹规划器，该平台运行实时操作系统。规划器在受限计算资源下持续计算轨迹，为未来紧急规划架构奠定基础。

Result: 实验结果表明系统具有确定性时序行为，具有有界延迟和最小抖动，验证了在安全可认证硬件上进行轨迹规划的可行性。

Conclusion: 该研究展示了将主动回退机制作为下一代安全保障框架组成部分的潜力，同时也指出了剩余挑战。代码已开源供进一步研究。

Abstract: Ensuring the functional safety of Autonomous Vehicles (AVs) requires motion planning modules that not only operate within strict real-time constraints but also maintain controllability in case of system faults. Existing safeguarding concepts, such as Online Verification (OV), provide safety layers that detect infeasible planning outputs. However, they lack an active mechanism to ensure safe operation in the event that the main planner fails. This paper presents a first step toward an active safety extension for fail-operational Autonomous Driving (AD). We deploy a lightweight sampling-based trajectory planner on an automotive-grade, embedded platform running a Real-Time Operating System (RTOS). The planner continuously computes trajectories under constrained computational resources, forming the foundation for future emergency planning architectures. Experimental results demonstrate deterministic timing behavior with bounded latency and minimal jitter, validating the feasibility of trajectory planning on safety-certifiable hardware. The study highlights both the potential and the remaining challenges of integrating active fallback mechanisms as an integral part of next-generation safeguarding frameworks. The code is available at: https://github.com/TUM-AVS/real-time-motion-planning

</details>


### [10] [An Event-Based Opto-Tactile Skin](https://arxiv.org/abs/2601.03907)
*Mohammadreza Koolani,Simeon Bamford,Petr Trunin,Simon F. Müller-Cleve,Matteo Lo Preti,Fulvio Mastrogiovanni,Lucia Beccai,Chiara Bartolozzi*

Main category: cs.RO

TL;DR: 基于DVS事件相机的柔性光学波导触觉传感系统，通过双目视觉和DBSCAN聚类实现大面积软皮肤上的按压定位，在极端数据稀疏化下仍保持功能


<details>
  <summary>Details</summary>
Motivation: 开发适用于软机器人和交互环境的大面积、柔性、响应式触觉传感系统，解决传统嵌入式光接收器重复扫描的问题

Method: 采用柔性硅胶光学波导皮肤与两个DVS事件相机组成的双目视觉系统，通过检测亮度变化产生事件，利用DBSCAN聚类和三角测量估计2D皮肤表面的按压位置

Result: 在4620mm²探测区域内，95%可见按压的定位RMSE为4.66mm；极端数据稀疏化(1/1024)下，平均定位误差仅增至9.33mm，85%试验仍能产生有效定位；检测延迟分布特征宽度为31ms

Conclusion: 该神经形态事件驱动触觉传感系统在大面积柔性皮肤上表现出良好的定位性能，即使在极端数据稀疏化下仍保持功能，为降低功耗和计算负载的未来实现提供了前景

Abstract: This paper presents a neuromorphic, event-driven tactile sensing system for soft, large-area skin, based on the Dynamic Vision Sensors (DVS) integrated with a flexible silicone optical waveguide skin. Instead of repetitively scanning embedded photoreceivers, this design uses a stereo vision setup comprising two DVS cameras looking sideways through the skin. Such a design produces events as changes in brightness are detected, and estimates press positions on the 2D skin surface through triangulation, utilizing Density-Based Spatial Clustering of Applications with Noise (DBSCAN) to find the center of mass of contact events resulting from pressing actions. The system is evaluated over a 4620 mm2 probed area of the skin using a meander raster scan. Across 95 % of the presses visible to both cameras, the press localization achieved a Root-Mean-Squared Error (RMSE) of 4.66 mm. The results highlight the potential of this approach for wide-area flexible and responsive tactile sensors in soft robotics and interactive environments. Moreover, we examined how the system performs when the amount of event data is strongly reduced. Using stochastic down-sampling, the event stream was reduced to 1/1024 of its original size. Under this extreme reduction, the average localization error increased only slightly (from 4.66 mm to 9.33 mm), and the system still produced valid press localizations for 85 % of the trials. This reduction in pass rate is expected, as some presses no longer produce enough events to form a reliable cluster for triangulation. These results show that the sensing approach remains functional even with very sparse event data, which is promising for reducing power consumption and computational load in future implementations. The system exhibits a detection latency distribution with a characteristic width of 31 ms.

</details>


### [11] [CoINS: Counterfactual Interactive Navigation via Skill-Aware VLM](https://arxiv.org/abs/2601.03956)
*Kangjie Zhou,Zhejia Wen,Zhiyong Zhuo,Zike Yan,Pengying Wu,Ieng Hou U,Shuaiyang Li,Han Gao,Kang Ding,Wenhan Cao,Wei Pan,Chang Liu*

Main category: cs.RO

TL;DR: CoINS是一个用于机器人交互导航的分层框架，通过技能感知的视觉语言模型进行反事实推理，结合强化学习技能库执行，显著提升了在复杂环境中的导航成功率。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉语言模型在机器人规划中主要作为语义推理器，缺乏对机器人具体物理能力的理解。在交互导航中，机器人需要主动修改杂乱环境以创建可通行路径，而现有VLM导航器主要局限于被动避障，无法推理何时以及如何与物体交互来清理阻塞路径。

Method: 提出了CoINS分层框架：1）微调InterNav-VLM模型，将技能可及性和具体约束参数融入输入上下文，并将其映射到度量尺度环境表示；2）通过InterNav数据集微调，模型学习隐式评估物体移除对导航连通性的因果效应；3）开发全面的技能库，通过强化学习训练，引入面向可通行性的策略来操纵多样物体以清理路径。

Result: 在Isaac Sim中建立了系统性基准测试，评估交互导航的推理和执行两方面。广泛的仿真和真实世界实验表明，CoINS显著优于代表性基线方法，总体成功率提高了17%，在复杂长视野场景中相比最佳基线有超过80%的改进。

Conclusion: CoINS通过集成技能感知推理和鲁棒低级执行，成功解决了现有VLM导航器在交互导航中的局限性，实现了更智能的环境交互和路径清理能力。

Abstract: Recent Vision-Language Models (VLMs) have demonstrated significant potential in robotic planning. However, they typically function as semantic reasoners, lacking an intrinsic understanding of the specific robot's physical capabilities. This limitation is particularly critical in interactive navigation, where robots must actively modify cluttered environments to create traversable paths. Existing VLM-based navigators are predominantly confined to passive obstacle avoidance, failing to reason about when and how to interact with objects to clear blocked paths. To bridge this gap, we propose Counterfactual Interactive Navigation via Skill-aware VLM (CoINS), a hierarchical framework that integrates skill-aware reasoning and robust low-level execution. Specifically, we fine-tune a VLM, named InterNav-VLM, which incorporates skill affordance and concrete constraint parameters into the input context and grounds them into a metric-scale environmental representation. By internalizing the logic of counterfactual reasoning through fine-tuning on the proposed InterNav dataset, the model learns to implicitly evaluate the causal effects of object removal on navigation connectivity, thereby determining interaction necessity and target selection. To execute the generated high-level plans, we develop a comprehensive skill library through reinforcement learning, specifically introducing traversability-oriented strategies to manipulate diverse objects for path clearance. A systematic benchmark in Isaac Sim is proposed to evaluate both the reasoning and execution aspects of interactive navigation. Extensive simulations and real-world experiments demonstrate that CoINS significantly outperforms representative baselines, achieving a 17\% higher overall success rate and over 80\% improvement in complex long-horizon scenarios compared to the best-performing baseline

</details>


### [12] [Stable Language Guidance for Vision-Language-Action Models](https://arxiv.org/abs/2601.04052)
*Zhihao Zhan,Yuhao Chen,Jiaying Zhou,Qinhan Lv,Hao Liu,Keze Wang,Liang Lin,Guangrun Wang*

Main category: cs.RO

TL;DR: 提出Residual Semantic Steering (RSS)框架解决VLA模型对语言扰动脆弱的问题，通过解耦物理可操作性和语义执行，提升模型鲁棒性。


<details>
  <summary>Details</summary>
Motivation: VLA模型在广义机器人控制中表现出色，但对语言扰动非常脆弱。研究发现存在"模态坍塌"现象，即强烈的视觉先验会压倒稀疏的语言信号，导致智能体过度拟合特定指令表达而忽略底层语义意图。

Method: 提出Residual Semantic Steering (RSS)概率框架，包含两个理论创新：1) Monte Carlo Syntactic Integration - 通过密集的LLM驱动的分布扩展来近似真实语义后验；2) Residual Affordance Steering - 双流解码机制，通过减去视觉可操作性先验来显式隔离语言对动作的因果影响。

Result: 理论分析表明RSS能有效最大化动作与意图之间的互信息，同时抑制视觉干扰。在多样化操作基准测试中，RSS实现了最先进的鲁棒性，即使在对抗性语言扰动下也能保持性能。

Conclusion: RSS框架成功解决了VLA模型中的模态坍塌问题，通过解耦物理可操作性和语义执行，显著提升了模型对语言扰动的鲁棒性，为更可靠的机器人控制提供了理论和方法基础。

Abstract: Vision-Language-Action (VLA) models have demonstrated impressive capabilities in generalized robotic control; however, they remain notoriously brittle to linguistic perturbations. We identify a critical ``modality collapse'' phenomenon where strong visual priors overwhelm sparse linguistic signals, causing agents to overfit to specific instruction phrasings while ignoring the underlying semantic intent. To address this, we propose \textbf{Residual Semantic Steering (RSS)}, a probabilistic framework that disentangles physical affordance from semantic execution. RSS introduces two theoretical innovations: (1) \textbf{Monte Carlo Syntactic Integration}, which approximates the true semantic posterior via dense, LLM-driven distributional expansion, and (2) \textbf{Residual Affordance Steering}, a dual-stream decoding mechanism that explicitly isolates the causal influence of language by subtracting the visual affordance prior. Theoretical analysis suggests that RSS effectively maximizes the mutual information between action and intent while suppressing visual distractors. Empirical results across diverse manipulation benchmarks demonstrate that RSS achieves state-of-the-art robustness, maintaining performance even under adversarial linguistic perturbations.

</details>


### [13] [CLAP: Contrastive Latent Action Pretraining for Learning Vision-Language-Action Models from Human Videos](https://arxiv.org/abs/2601.04061)
*Chubin Zhang,Jianan Wang,Zifeng Gao,Yue Su,Tianru Dai,Cai Zhou,Jiwen Lu,Yansong Tang*

Main category: cs.RO

TL;DR: CLAP框架通过对比学习将人类视频的视觉潜在空间与机器人本体感知潜在空间对齐，利用量化可执行代码库实现技能从视频到机器人的迁移，并提出两种VLA模型和防遗忘正则化策略。


<details>
  <summary>Details</summary>
Motivation: 当前通用视觉-语言-动作模型面临机器人数据稀缺而人类视频数据丰富的困境，现有潜在动作模型容易陷入视觉纠缠问题，无法有效提取操作技能。

Method: 提出对比潜在动作预训练框架，通过对比学习对齐视频视觉潜在空间和机器人轨迹本体感知潜在空间，将视频转换映射到量化可执行代码库；构建双形式VLA框架：CLAP-NTP自回归模型和CLAP-RF整流流策略；提出知识匹配正则化防止微调时的灾难性遗忘。

Result: 大量实验表明CLAP显著优于强基线方法，能够有效将人类视频中的技能迁移到机器人执行中。

Conclusion: CLAP框架通过对比学习解决了视觉纠缠问题，实现了从丰富人类视频数据到机器人技能的有效迁移，为通用视觉-语言-动作模型提供了新的解决方案。

Abstract: Generalist Vision-Language-Action models are currently hindered by the scarcity of robotic data compared to the abundance of human video demonstrations. Existing Latent Action Models attempt to leverage video data but often suffer from visual entanglement, capturing noise rather than manipulation skills. To address this, we propose Contrastive Latent Action Pretraining (CLAP), a framework that aligns the visual latent space from videos with a proprioceptive latent space from robot trajectories. By employing contrastive learning, CLAP maps video transitions onto a quantized, physically executable codebook. Building on this representation, we introduce a dual-formulation VLA framework offering both CLAP-NTP, an autoregressive model excelling at instruction following and object generalization, and CLAP-RF, a Rectified Flow-based policy designed for high-frequency, precise manipulation. Furthermore, we propose a Knowledge Matching (KM) regularization strategy to mitigate catastrophic forgetting during fine-tuning. Extensive experiments demonstrate that CLAP significantly outperforms strong baselines, enabling the effective transfer of skills from human videos to robotic execution. Project page: https://lin-shan.com/CLAP/.

</details>


### [14] [Wow, wo, val! A Comprehensive Embodied World Model Evaluation Turing Test](https://arxiv.org/abs/2601.04137)
*Chun-Kai Fan,Xiaowei Chi,Xiaozhu Ju,Hao Li,Yong Bao,Yu-Kai Wang,Lizhang Chen,Zhiyuan Jiang,Kuangzhi Ge,Ying Li,Weishi Mi,Qingpo Wuwu,Peidong Jia,Yulin Luo,Kevin Zhang,Zhiyuan Qin,Yong Dai,Sirui Han,Yike Guo,Shanghang Zhang,Jian Tang*

Main category: cs.RO

TL;DR: 该研究提出了Embodied Turing Test基准WoW-World-Eval，用于评估视频基础模型作为世界模型在具身AI中的能力，发现现有模型在长时程规划和物理一致性方面表现有限，与现实世界存在明显差距。


<details>
  <summary>Details</summary>
Motivation: 随着世界模型在具身AI中的重要性日益增长，需要评估视频基础模型是否具备足够的生成泛化能力和鲁棒性，以作为真实世界具身智能体的通用先验知识。目前缺乏标准化框架来回答这些关键问题。

Method: 基于609个机器人操作数据构建WoW-World-Eval基准，评估五个核心能力：感知、规划、预测、泛化和执行。提出包含22个指标的综合评估协议，建立可靠的人类图灵测试基础，并引入逆动态模型图灵测试评估真实世界执行准确性。

Result: 模型在长时程规划上仅得17.27分，物理一致性最高68.02分，显示时空一致性和物理推理能力有限。在逆动态模型图灵测试中，大多数模型成功率接近0%，而WoW模型保持40.74%的成功率。评估协议与人类偏好的皮尔逊相关系数>0.93。

Conclusion: 生成的视频与现实世界存在显著差距，突显了在具身AI中基准测试世界模型的紧迫性和必要性。现有视频基础模型作为世界模型的能力仍有很大提升空间。

Abstract: As world models gain momentum in Embodied AI, an increasing number of works explore using video foundation models as predictive world models for downstream embodied tasks like 3D prediction or interactive generation. However, before exploring these downstream tasks, video foundation models still have two critical questions unanswered: (1) whether their generative generalization is sufficient to maintain perceptual fidelity in the eyes of human observers, and (2) whether they are robust enough to serve as a universal prior for real-world embodied agents. To provide a standardized framework for answering these questions, we introduce the Embodied Turing Test benchmark: WoW-World-Eval (Wow,wo,val). Building upon 609 robot manipulation data, Wow-wo-val examines five core abilities, including perception, planning, prediction, generalization, and execution. We propose a comprehensive evaluation protocol with 22 metrics to assess the models' generation ability, which achieves a high Pearson Correlation between the overall score and human preference (>0.93) and establishes a reliable foundation for the Human Turing Test. On Wow-wo-val, models achieve only 17.27 on long-horizon planning and at best 68.02 on physical consistency, indicating limited spatiotemporal consistency and physical reasoning. For the Inverse Dynamic Model Turing Test, we first use an IDM to evaluate the video foundation models' execution accuracy in the real world. However, most models collapse to $\approx$ 0% success, while WoW maintains a 40.74% success rate. These findings point to a noticeable gap between the generated videos and the real world, highlighting the urgency and necessity of benchmarking World Model in Embodied AI.

</details>


### [15] [Embedding Autonomous Agents in Resource-Constrained Robotic Platforms](https://arxiv.org/abs/2601.04191)
*Negar Halakou,Juan F. Gutierrez,Ye Sun,Han Jiang,Xueming Wu,Yilun Song,Andres Gomez*

Main category: cs.RO

TL;DR: 将AgentSpeak智能体集成到小型两轮机器人中，使其能够在迷宫中自主探索决策，实验显示59秒内完成迷宫求解，推理过程高效适合资源受限硬件


<details>
  <summary>Details</summary>
Motivation: 嵌入式设备通常在资源受限和动态环境中运行，需要本地决策能力。使设备能够在这样的环境中独立决策可以提高系统响应性，减少对外部控制的依赖

Method: 将使用AgentSpeak编程的自主智能体集成到小型两轮机器人中，机器人利用自身决策和传感器数据探索迷宫

Result: 智能体在59秒内成功解决了迷宫问题，使用了287个推理周期，决策阶段耗时不到1毫秒。结果表明推理过程足够高效，可以在资源受限的硬件上实时执行

Conclusion: 这种集成展示了高级基于智能体的控制如何应用于资源受限的嵌入式系统，实现自主操作，证明了智能体推理在实时嵌入式环境中的可行性

Abstract: Many embedded devices operate under resource constraints and in dynamic environments, requiring local decision-making capabilities. Enabling devices to make independent decisions in such environments can improve the responsiveness of the system and reduce the dependence on constant external control. In this work, we integrate an autonomous agent, programmed using AgentSpeak, with a small two-wheeled robot that explores a maze using its own decision-making and sensor data. Experimental results show that the agent successfully solved the maze in 59 seconds using 287 reasoning cycles, with decision phases taking less than one millisecond. These results indicate that the reasoning process is efficient enough for real-time execution on resource-constrained hardware. This integration demonstrates how high-level agent-based control can be applied to resource-constrained embedded systems for autonomous operation.

</details>
