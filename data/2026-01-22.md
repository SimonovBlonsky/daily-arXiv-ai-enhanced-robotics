<div id=toc></div>

# Table of Contents

- [cs.RO](#cs.RO) [Total: 24]


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [1] [RoboBrain 2.5: Depth in Sight, Time in Mind](https://arxiv.org/abs/2601.14352)
*Huajie Tan,Enshen Zhou,Zhiyu Li,Yijie Xu,Yuheng Ji,Xiansheng Chen,Cheng Chi,Pengwei Wang,Huizhu Jia,Yulong Ao,Mingyu Cao,Sixiang Chen,Zhe Li,Mengzhen Liu,Zixiao Wang,Shanyu Rong,Yaoxu Lyu,Zhongxia Zhao,Peterson Co,Yibo Li,Yi Han,Shaoxuan Xie,Guocai Yao,Songjing Wang,Leiduo Zhang,Xi Yang,Yance Jiao,Donghai Shi,Kunchang Xie,Shaokai Nie,Chunlei Men,Yonghua Lin,Zhongyuan Wang,Tiejun Huang,Shanghang Zhang*

Main category: cs.RO

TL;DR: RoboBrain 2.5是新一代具身AI基础模型，通过高质量时空监督训练，提升了感知、空间推理和时间建模能力，支持复杂精细操作


<details>
  <summary>Details</summary>
Motivation: 构建更物理基础和执行感知的具身智能系统，以处理复杂、精细的操作任务，超越前代模型的限制

Method: 引入两大能力升级：1) 精确3D空间推理：从2D像素相对定位转向深度感知坐标预测和绝对度量约束理解，生成受物理约束的有序关键点序列；2) 密集时间价值估计：提供密集、步骤感知的进度预测和执行状态理解，产生稳定的反馈信号

Result: 模型实现了更物理基础和执行感知的具身智能，能够处理复杂精细的操作任务，代码和检查点已公开

Conclusion: RoboBrain 2.5通过精确3D空间推理和密集时间价值估计两大升级，显著提升了具身AI在复杂精细操作任务中的能力，为更物理基础和执行感知的智能系统奠定了基础

Abstract: We introduce RoboBrain 2.5, a next-generation embodied AI foundation model that advances general perception, spatial reasoning, and temporal modeling through extensive training on high-quality spatiotemporal supervision. Building upon its predecessor, RoboBrain 2.5 introduces two major capability upgrades. Specifically, it unlocks Precise 3D Spatial Reasoning by shifting from 2D pixel-relative grounding to depth-aware coordinate prediction and absolute metric constraint comprehension, generating complete 3D manipulation traces as ordered keypoint sequences under physical constraints. Complementing this spatial precision, the model establishes Dense Temporal Value Estimation that provides dense, step-aware progress prediction and execution state understanding across varying viewpoints, producing stable feedback signals for downstream learning. Together, these upgrades extend the framework toward more physically grounded and execution-aware embodied intelligence for complex, fine-grained manipulation. The code and checkpoints are available at project website: https://superrobobrain.github.io

</details>


### [2] [Agentic AI Meets Edge Computing in Autonomous UAV Swarms](https://arxiv.org/abs/2601.14437)
*Thuan Minh Nguyen,Vu Tuan Truong,Long Bao Le*

Main category: cs.RO

TL;DR: 该论文研究将基于大语言模型的智能体AI与边缘计算集成到无人机集群中，实现可扩展和弹性的自主性，特别针对野火搜救等高风险场景。


<details>
  <summary>Details</summary>
Motivation: 将具备自主推理、规划和执行能力的智能体AI集成到无人机集群中，为无人机互联网愿景带来新可能性，但基础设施限制、动态环境和多智能体协调的计算需求限制了其在高风险场景（如野火和灾难响应）中的实际部署。

Method: 首先讨论了支持无人机集群的三种架构：独立部署、边缘使能和边缘-云混合部署，每种架构针对不同的自主性和连接级别进行优化。然后设计了野火搜救用例来展示边缘使能架构的效率。

Result: 边缘使能架构相比传统方法，能够实现更高的搜救覆盖率、减少任务完成时间，并提供更高水平的自主性。

Conclusion: 论文强调了将大语言模型和边缘计算集成到任务关键型无人机集群应用中面临的开放挑战。

Abstract: The integration of agentic AI, powered by large language models (LLMs) with autonomous reasoning, planning, and execution, into unmanned aerial vehicle (UAV) swarms opens new operational possibilities and brings the vision of the Internet of Drones closer to reality. However, infrastructure constraints, dynamic environments, and the computational demands of multi-agent coordination limit real-world deployment in high-risk scenarios such as wildfires and disaster response. This paper investigates the integration of LLM-based agentic AI and edge computing to realize scalable and resilient autonomy in UAV swarms. We first discuss three architectures for supporting UAV swarms - standalone, edge-enabled, and edge-cloud hybrid deployment - each optimized for varying autonomy and connectivity levels. Then, a use case for wildfire search and rescue (SAR) is designed to demonstrate the efficiency of the edge-enabled architecture, enabling high SAR coverage, reduced mission completion times, and a higher level of autonomy compared to traditional approaches. Finally, we highlight open challenges in integrating LLMs and edge computing for mission-critical UAV-swarm applications.

</details>


### [3] [Robust Haptic Rendering Using a Nonlinear Impedance Matching Approach (NIMA) for Robotic Laparoscopic Surgery](https://arxiv.org/abs/2601.14445)
*Aiden Mazidi,Majid Roshanfar,Amir Sayadi,Javad Dargahi,Jake Barralet,Liane S. Feldman,Amir Hooshiar*

Main category: cs.RO

TL;DR: NIMA方法通过非线性阻抗匹配显著提升机器人辅助微创手术中的力反馈精度，减少95%的平均绝对误差并消除触觉"回弹"现象


<details>
  <summary>Details</summary>
Motivation: 机器人辅助微创手术(RAMIS)中的触觉反馈长期受限于力渲染精度和系统安全性问题，需要稳健、高保真的触觉系统来提升远程操作手术工具的精确性和可靠性

Method: 提出非线性阻抗匹配方法(NIMA)，在先前验证的阻抗匹配方法(IMA)基础上引入非线性动力学，准确建模和渲染工具-组织相互作用力

Result: NIMA将力反馈精度提升至平均绝对误差0.01N(SD 0.02)，相比IMA减少95%误差；有效消除触觉"回弹"现象，确保用户松开手柄时触觉设备不施加力，提升患者安全性和用户舒适度

Conclusion: NIMA能够考虑工具-组织相互作用的非线性特性，在各种手术条件下提升力保真度、响应性和精确性，推动机器人手术触觉反馈系统的发展，为机器人辅助手术提供真实可靠的界面

Abstract: Background: The integration of haptic feedback into robot-assisted minimally invasive surgery (RAMIS) has long been limited by challenges in accurately rendering forces and ensuring system safety. The need for robust, high-fidelity haptic systems is critical for enhancing the precision and reliability of teleoperated surgical tools. Methods: In this study, we present a Nonlinear Impedance Matching Approach (NIMA) designed to improve force rendering by accurately modelling complex tool-tissue interactions. Based on our previously validated Impedance Matching Approach (IMA), our novel NIMA method includes nonlinear dynamics to capture and render tool-tissue forces effectively. Results: NIMA improves force feedback accuracy with a mean absolute error (MAE) of 0.01 (SD 0.02) N, achieving a 95% reduction in MAE compared to IMA. Furthermore, NIMA effectively eliminates haptic "kickback" by ensuring no force is applied by the haptic device to the user's hand when they release the handle, enhancing both patient safety and user comfort. Conclusion: NIMA's ability to account for nonlinearities in tool-tissue interactions provides an improvement in force fidelity, responsiveness, and precision across various surgical conditions. Our findings promote the advancement of haptic feedback systems for robotic surgery, offering a realistic and reliable interface for robot-assisted surgical procedures.

</details>


### [4] [UNCLE-Grasp: Uncertainty-Aware Grasping of Leaf-Occluded Strawberries](https://arxiv.org/abs/2601.14492)
*Malak Mansour,Ali Abouzeid,Zezhou Sun,Qinbo Sun,Dezhen Song,Abdalla Swikir*

Main category: cs.RO

TL;DR: 该论文提出了一种用于部分遮挡草莓的感知不确定性抓取方法，通过多假设形状补全和保守决策机制，在遮挡严重时安全放弃抓取尝试，在几何置信度足够时保持稳健抓取。


<details>
  <summary>Details</summary>
Motivation: 机器人草莓采摘在部分遮挡条件下具有挑战性，叶片遮挡导致显著的几何不确定性，基于单一确定性形状估计的抓取决策不可靠。从单一局部观测中，可能存在多个不兼容的3D补全假设，导致在一个补全上可行的抓取在另一个补全上失败。

Method: 提出不确定性感知抓取流水线，显式建模遮挡和学习形状重建带来的补全不确定性。使用蒙特卡洛dropout点云补全采样多个形状假设，为每个补全生成候选抓取，使用基于力闭合的物理基础指标评估抓取可行性。不基于单一估计选择抓取，而是跨补全聚合可行性，应用保守的下置信界准则决定是否尝试抓取或安全放弃。

Result: 在仿真和物理机器人上评估了该方法，涵盖合成和真实叶片遮挡的递增水平。结果表明，不确定性感知决策能够在严重遮挡下可靠地放弃高风险抓取尝试，同时在几何置信度足够时保持稳健的抓取执行，在仿真和物理机器人实验中均优于确定性基线方法。

Conclusion: 该方法通过显式建模几何不确定性并采用保守决策策略，显著提高了机器人草莓采摘在部分遮挡条件下的可靠性和安全性，实现了在不确定条件下的智能抓取决策。

Abstract: Robotic strawberry harvesting is challenging under partial occlusion, where leaves induce significant geometric uncertainty and make grasp decisions based on a single deterministic shape estimate unreliable. From a single partial observation, multiple incompatible 3D completions may be plausible, causing grasps that appear feasible on one completion to fail on another. We propose an uncertainty-aware grasping pipeline for partially occluded strawberries that explicitly models completion uncertainty arising from both occlusion and learned shape reconstruction. Our approach uses point cloud completion with Monte Carlo dropout to sample multiple shape hypotheses, generates candidate grasps for each completion, and evaluates grasp feasibility using physically grounded force-closure-based metrics. Rather than selecting a grasp based on a single estimate, we aggregate feasibility across completions and apply a conservative lower confidence bound (LCB) criterion to decide whether a grasp should be attempted or safely abstained. We evaluate the proposed method in simulation and on a physical robot across increasing levels of synthetic and real leaf occlusion. Results show that uncertainty-aware decision making enables reliable abstention from high-risk grasp attempts under severe occlusion while maintaining robust grasp execution when geometric confidence is sufficient, outperforming deterministic baselines in both simulated and physical robot experiments.

</details>


### [5] [TacUMI: A Multi-Modal Universal Manipulation Interface for Contact-Rich Tasks](https://arxiv.org/abs/2601.14550)
*Tailai Cheng,Kejia Chen,Lingyun Chen,Liding Zhang,Yue Zhang,Yao Ling,Mahdi Hamad,Zhenshan Bing,Fan Wu,Karan Sharma,Alois Knoll*

Main category: cs.RO

TL;DR: TacUMI系统通过集成多模态传感器（ViTac、力扭矩传感器、姿态跟踪器）来收集接触丰富任务的高质量演示数据，并提出多模态分割框架来分解复杂操作任务


<details>
  <summary>Details</summary>
Motivation: 对于涉及丰富物理交互的复杂长时程操作任务，仅依靠视觉观察和机器人本体感知信息往往无法揭示底层事件转换，因此需要高效收集高质量多模态数据以及鲁棒的分割方法来将演示分解为有意义的模块

Method: 基于手持演示设备UMI，开发了TacUMI多模态数据收集系统，集成ViTac传感器、力扭矩传感器和姿态跟踪器到紧凑的机器人兼容夹爪设计中，实现演示过程中所有模态的同步采集；提出多模态分割框架，利用时序模型检测顺序操作中的语义有意义事件边界

Result: 在具有挑战性的电缆安装任务评估中，实现了超过90%的分割准确率，并显示出更多模态带来的显著改进

Conclusion: TacUMI为接触丰富任务中多模态演示的可扩展收集和分割建立了实用基础

Abstract: Task decomposition is critical for understanding and learning complex long-horizon manipulation tasks. Especially for tasks involving rich physical interactions, relying solely on visual observations and robot proprioceptive information often fails to reveal the underlying event transitions. This raises the requirement for efficient collection of high-quality multi-modal data as well as robust segmentation method to decompose demonstrations into meaningful modules. Building on the idea of the handheld demonstration device Universal Manipulation Interface (UMI), we introduce TacUMI, a multi-modal data collection system that integrates additionally ViTac sensors, force-torque sensor, and pose tracker into a compact, robot-compatible gripper design, which enables synchronized acquisition of all these modalities during human demonstrations. We then propose a multi-modal segmentation framework that leverages temporal models to detect semantically meaningful event boundaries in sequential manipulations. Evaluation on a challenging cable mounting task shows more than 90 percent segmentation accuracy and highlights a remarkable improvement with more modalities, which validates that TacUMI establishes a practical foundation for both scalable collection and segmentation of multi-modal demonstrations in contact-rich tasks.

</details>


### [6] [UniCon: A Unified System for Efficient Robot Learning Transfers](https://arxiv.org/abs/2601.14617)
*Yunfeng Lin,Li Xu,Yong Yu,Jiangmiao Pang,Weinan Zhang*

Main category: cs.RO

TL;DR: UniCon是一个轻量级框架，通过标准化状态、控制流和仪器化，实现异构机器人间的学习控制器无缝部署，减少代码冗余并提高推理效率。


<details>
  <summary>Details</summary>
Motivation: 异构机器人部署学习控制器面临平台差异、接口不一致和中间件效率低下的挑战，需要统一的解决方案来简化跨平台部署。

Method: 将工作流分解为可重用组件的执行图，分离系统状态与控制逻辑，采用批处理和向量化数据流最小化通信开销，实现模块化、数据导向的方法。

Result: UniCon在转移工作流时减少代码冗余，相比ROS系统获得更高的推理效率，已在7个制造商的12个机器人模型上成功部署并集成到实际研究项目中。

Conclusion: UniCon框架通过标准化和高效的数据流设计，实现了异构机器人间的无缝学习控制器部署，显著降低了跨平台部署的工程成本。

Abstract: Deploying learning-based controllers across heterogeneous robots is challenging due to platform differences, inconsistent interfaces, and inefficient middleware. To address these issues, we present UniCon, a lightweight framework that standardizes states, control flow, and instrumentation across platforms. It decomposes workflows into execution graphs with reusable components, separating system states from control logic to enable plug-and-play deployment across various robot morphologies. Unlike traditional middleware, it prioritizes efficiency through batched, vectorized data flow, minimizing communication overhead and improving inference latency. This modular, data-oriented approach enables seamless sim-to-real transfer with minimal re-engineering. We demonstrate that UniCon reduces code redundancy when transferring workflows and achieves higher inference efficiency compared to ROS-based systems. Deployed on over 12 robot models from 7 manufacturers, it has been successfully integrated into ongoing research projects, proving its effectiveness in real-world scenarios.

</details>


### [7] [Probing Prompt Design for Socially Compliant Robot Navigation with Vision Language Models](https://arxiv.org/abs/2601.14622)
*Ling Xiao,Toshihiko Yamasaki*

Main category: cs.RO

TL;DR: 该研究探索了社交机器人导航中基于认知理论的提示设计，发现竞争性动机框架和系统指导提示能显著提升小视觉语言模型的决策准确性，特别是对抗过去自我的竞争效果最佳。


<details>
  <summary>Details</summary>
Motivation: 当前社交机器人导航基准大多忽视基于原则的提示设计，而实际部署中许多系统依赖效率高的小视觉语言模型。相比大语言模型，小VLM决策能力较弱，因此有效的提示设计对准确导航至关重要。

Method: 受人类学习和动机认知理论启发，研究从两个维度设计提示：系统指导（行动导向、推理导向、感知-推理提示）和动机框架（对抗人类、对抗其他AI系统、对抗过去自我）。在两个社交合规导航数据集上进行实验。

Result: 1. 对于未微调的GPT-4o，对抗人类的竞争表现最佳，对抗其他AI系统最差。对于微调模型，对抗过去自我的竞争效果最强，其次是对抗人类。2. 不恰当的系统提示设计会显著降低性能，甚至不如直接微调。3. 直接微调主要提升语义级指标（感知、预测、推理），但对行动准确性提升有限；而系统提示对行动准确性提升更大，表明提示设计主要作为决策级约束而非表征增强。

Conclusion: 基于认知理论的提示设计，特别是竞争性动机框架，能有效提升小视觉语言模型在社交机器人导航中的决策准确性。提示设计主要作为决策级约束机制，对行动准确性的提升效果优于直接微调。

Abstract: Language models are increasingly used for social robot navigation, yet existing benchmarks largely overlook principled prompt design for socially compliant behavior. This limitation is particularly relevant in practice, as many systems rely on small vision language models (VLMs) for efficiency. Compared to large language models, small VLMs exhibit weaker decision-making capabilities, making effective prompt design critical for accurate navigation. Inspired by cognitive theories of human learning and motivation, we study prompt design along two dimensions: system guidance (action-focused, reasoning-oriented, and perception-reasoning prompts) and motivational framing, where models compete against humans, other AI systems, or their past selves. Experiments on two socially compliant navigation datasets reveal three key findings. First, for non-finetuned GPT-4o, competition against humans achieves the best performance, while competition against other AI systems performs worst. For finetuned models, competition against the model's past self yields the strongest results, followed by competition against humans, with performance further influenced by coupling effects among prompt design, model choice, and dataset characteristics. Second, inappropriate system prompt design can significantly degrade performance, even compared to direct finetuning. Third, while direct finetuning substantially improves semantic-level metrics such as perception, prediction, and reasoning, it yields limited gains in action accuracy. In contrast, our system prompts produce a disproportionately larger improvement in action accuracy, indicating that the proposed prompt design primarily acts as a decision-level constraint rather than a representational enhancement.

</details>


### [8] [A Brain-inspired Embodied Intelligence for Fluid and Fast Reflexive Robotics Control](https://arxiv.org/abs/2601.14628)
*Weiyu Guo,He Zhang,Pengteng Li,Tiefu Cai,Ziyang Chen,Yandong Guo,Xiao He,Yongkui Yang,Ying Sun,Hui Xiong*

Main category: cs.RO

TL;DR: NeuroVLA是一个受生物神经系统启发的机器人控制框架，通过模拟大脑皮层、小脑和脊髓的结构组织，实现了动态稳定性、反射响应和时间记忆能力，在物理机器人上取得了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 当前机器人策略难以复制生物运动的动态稳定性、反射响应性和时间记忆能力，而生物系统却能从稀疏经验中快速学习技能。需要开发能模仿生物神经系统结构的机器人控制框架。

Method: 采用系统级生物启发设计：高层模型规划目标，自适应小脑模块利用高频传感器反馈稳定运动，生物启发脊髓层执行快速动作生成。这是首个在物理机器人上部署的神经形态VLA框架。

Result: 实现了最先进的性能，观察到生物运动特性的涌现：消除了机械臂抖动，显著节能（神经形态处理器仅需0.4w），展示了时间记忆能力，并在20毫秒内触发安全反射。

Conclusion: NeuroVLA框架成功模仿了生物神经系统的结构组织，在物理机器人上实现了生物运动特性，为机器人控制提供了新的生物启发范式。

Abstract: Recent advances in embodied intelligence have leveraged massive scaling of data and model parameters to master natural-language command following and multi-task control. In contrast, biological systems demonstrate an innate ability to acquire skills rapidly from sparse experience. Crucially, current robotic policies struggle to replicate the dynamic stability, reflexive responsiveness, and temporal memory inherent in biological motion. Here we present Neuromorphic Vision-Language-Action (NeuroVLA), a framework that mimics the structural organization of the bio-nervous system between the cortex, cerebellum, and spinal cord. We adopt a system-level bio-inspired design: a high-level model plans goals, an adaptive cerebellum module stabilizes motion using high-frequency sensors feedback, and a bio-inspired spinal layer executes lightning-fast actions generation. NeuroVLA represents the first deployment of a neuromorphic VLA on physical robotics, achieving state-of-the-art performance. We observe the emergence of biological motor characteristics without additional data or special guidance: it stops the shaking in robotic arms, saves significant energy(only 0.4w on Neuromorphic Processor), shows temporal memory ability and triggers safety reflexes in less than 20 milliseconds.

</details>


### [9] [Landing-Induced Viscoelastic Changes in an Anthropomimetic Foot Joint Structure are Modulated by Foot Structure and Posture](https://arxiv.org/abs/2601.14634)
*Satoru Hashimoto,Yinlai Jiang,Hiroshi Yokoi,Shunta Togo*

Main category: cs.RO

TL;DR: 开发仿人足部关节结构研究骨骼架构对落地冲击的影响，发现多关节拱形结构比简化平足有更高阻尼比，踝背屈和趾伸展会降低阻尼比，表明骨骼架构能增强冲击衰减并通过姿态调节衰减与回弹的平衡。


<details>
  <summary>Details</summary>
Motivation: 尸体研究难以重复测试落地冲击后的姿态依赖粘弹性响应，导致骨骼架构对落地动力学的贡献不完全清楚。需要开发仿人足部结构来研究骨骼结构和姿态如何调节冲击后的粘弹性响应。

Method: 开发仿人足部关节结构复制人类足部骨骼几何，使用垂直跌落装置模拟落地，结合粘弹性系统辨识模型，研究骨骼结构和姿态如何调节冲击后的表观粘弹性响应。

Result: 多关节仿人结构比简化的平足和刚性足表现出更高的阻尼比。踝背屈和趾伸展会系统性改变辨识参数，在测试条件下降低阻尼比。拱形多关节骨骼架构能增强仿人机械足的冲击衰减，仅通过形态和被动姿态就能调节衰减与回弹的平衡。

Conclusion: 骨骼架构可能部分解释人类落地策略的差异，观察到的姿态依赖趋势与报道的人类落地策略差异定性一致。这些结果突显了基于解剖学的骨骼复制在通过落地时姿态调整实现类人表观粘弹性行为方面的工程优势。

Abstract: Cadaveric studies have provided important insights into the mechanics of the human foot arch and plantar fascia. However, repeatedly probing posture-dependent viscoelastic responses immediately after landing impact is difficult in biological specimens, leaving the contribution of skeletal architecture to landing dynamics incompletely understood. In this study, we developed an anthropomimetic foot joint structure aimed at replicating the skeletal geometry of the human foot. Using a vertical drop apparatus that simulates landing and a viscoelastic system-identification model, we investigated how skeletal structure and posture modulate the apparent post-impact viscoelastic response. The results show that the multi-jointed anthropomimetic structure exhibited a higher damping ratio than simplified flat and rigid feet. Moreover, ankle dorsiflexion and toe extension systematically shifted the identified parameters, reducing the damping ratio under the tested conditions. Taken together, these findings indicate that an arch-like, multi-jointed skeletal architecture can enhance impact attenuation in an anthropomimetic mechanical foot, and that morphology and passive posture alone can tune the trade-off between attenuation and rebound. The observed posture-dependent trends are qualitatively consistent with reported differences in human landing strategies, suggesting that skeletal architecture may partly account for the modulation. Furthermore, these results highlight the engineering advantage of anatomically informed skeletal replication for achieving human-like apparent viscoelastic behavior through postural adjustment during landing.

</details>


### [10] [FARE: Fast-Slow Agentic Robotic Exploration](https://arxiv.org/abs/2601.14681)
*Shuhao Liao,Xuxin Lv,Jeric Lew,Shizhe Zhang,Jingsong Liang,Peizhuo Li,Yuhong Cao,Wenjun Wu,Guillaume Sartoretti*

Main category: cs.RO

TL;DR: FARE框架通过结合大语言模型的全局语义推理和强化学习的局部控制，实现高效自主机器人探索


<details>
  <summary>Details</summary>
Motivation: 现有自主探索方法在全局语义推理和局部快速决策之间存在脱节，需要一种能够同时处理高层次语义理解和低层次几何决策的框架

Method: 提出FARE分层框架：慢思考LLM模块解析环境文本描述并生成探索策略，通过拓扑图生成全局路径点；快思考RL模块基于局部观察执行探索，同时遵循LLM生成的全局路径点指导

Result: 在仿真环境中，FARE相比现有方法显著提升了探索效率；在200m×130m的大型建筑环境中硬件部署验证了其有效性

Conclusion: FARE成功将语义推理与几何决策解耦，使各模块在合适的时空尺度上运行，实现了高效、鲁棒的自主探索

Abstract: This work advances autonomous robot exploration by integrating agent-level semantic reasoning with fast local control. We introduce FARE, a hierarchical autonomous exploration framework that integrates a large language model (LLM) for global reasoning with a reinforcement learning (RL) policy for local decision making. FARE follows a fast-slow thinking paradigm. The slow-thinking LLM module interprets a concise textual description of the unknown environment and synthesizes an agent-level exploration strategy, which is then grounded into a sequence of global waypoints through a topological graph. To further improve reasoning efficiency, this module employs a modularity-based pruning mechanism that reduces redundant graph structures. The fast-thinking RL module executes exploration by reacting to local observations while being guided by the LLM-generated global waypoints. The RL policy is additionally shaped by a reward term that encourages adherence to the global waypoints, enabling coherent and robust closed-loop behavior. This architecture decouples semantic reasoning from geometric decision, allowing each module to operate in its appropriate temporal and spatial scale. In challenging simulated environments, our results show that FARE achieves substantial improvements in exploration efficiency over state-of-the-art baselines. We further deploy FARE on hardware and validate it in complex, large scale $200m\times130m$ building environment.

</details>


### [11] [Stochastic Decision-Making Framework for Human-Robot Collaboration in Industrial Applications](https://arxiv.org/abs/2601.14809)
*Muhammad Adel Yusuf,Ali Nasir,Zeeshan Hameed Khan*

Main category: cs.RO

TL;DR: 本文提出了一种基于随机建模的人机协作决策方法，通过概率模型和控制策略预测人类行为和情绪，使协作机器人能够相应调整行为。


<details>
  <summary>Details</summary>
Motivation: 随着协作机器人在工业和服务领域的广泛应用，要实现有效的人机协作，机器人需要基于人类因素（如动机水平和攻击性水平）进行推理。目前大多数研究集中在检测人类同事的意图上，需要更全面的决策框架。

Method: 采用随机建模方法，利用概率模型和控制策略来预测人类行为和情绪，使协作机器人能够适应性地调整自身行为。

Result: 论文讨论了理论框架、实施策略、仿真结果以及双边协作方法在协作机器人安全性和效率方面的潜在应用。

Conclusion: 提出的基于随机建模的决策方法能够使协作机器人更好地理解人类因素，实现更安全、更高效的人机协作环境。

Abstract: Collaborative robots, or cobots, are increasingly integrated into various industrial and service settings to work efficiently and safely alongside humans. However, for effective human-robot collaboration, robots must reason based on human factors such as motivation level and aggression level. This paper proposes an approach for decision-making in human-robot collaborative (HRC) environments utilizing stochastic modeling. By leveraging probabilistic models and control strategies, the proposed method aims to anticipate human actions and emotions, enabling cobots to adapt their behavior accordingly. So far, most of the research has been done to detect the intentions of human co-workers. This paper discusses the theoretical framework, implementation strategies, simulation results, and potential applications of the bilateral collaboration approach for safety and efficiency in collaborative robotics.

</details>


### [12] [Moving Beyond Compliance in Soft-Robotic Catheters Through Modularity for Precision Therapies](https://arxiv.org/abs/2601.14837)
*B. Calmé,N. J. Greenidge,A. Metcalf,A. Bacchetti,G. Loza,D. Kpeglo,P. Lloyd,V. Pensabene,J. H. Chandler,P. Valdastri*

Main category: cs.RO

TL;DR: 研究人员开发了一种直径1.47毫米的模块化软体机器人导管，集成了传感、驱动和治疗功能，能够在腔内导航中实现半自主部署和精准操作，特别适用于胰腺导管等难以进入的区域。


<details>
  <summary>Details</summary>
Motivation: 软体机器人器械在脆弱、曲折的解剖结构中导航比刚性工具更安全，但临床应用中存在尖端功能不足和实时反馈缺失的问题。目前缺乏紧凑、鲁棒且适应性强的传感和治疗模块来测量和响应腔内手术中的细微生理信号。

Method: 开发了直径1.47毫米的模块化软体机器人导管架构，支持最多四个独立控制的功能单元（锚定、操作、传感和靶向药物输送）。采用闭环自主/共享控制系统，结合学习模型、磁驱动、板载形状传感和视觉标记跟踪技术。

Result: 在活体猪模型中成功演示了半自主部署到胰腺导管内，并在其中进行了7.5厘米的内窥镜导航，这是标准导管目前无法进入的区域。闭环控制系统进一步提高了插管准确性。

Conclusion: 该研究建立了一个可扩展的多功能软体机器人导管平台，为复杂的腔内介入提供了新范式，有望减少辐射暴露、缩短培训时间并加速软体机器人技术的临床转化。

Abstract: Soft robotic instruments could navigate delicate, tortuous anatomy more safely than rigid tools, but clinical adoption is limited by insufficient tip functionalization and real-time feedback at the tissue interface. Few sensing and therapeutic modules are compact, robust, and adaptable enough to measure, and respond to, subtle physiological cues during intraluminal procedures. We present a 1.47 mm diameter modular soft robotic catheter that integrates sensing, actuation, and therapy while retaining the compliance needed for safe endoluminal navigation. Validated across multiple in vivo settings, we emphasize its utility in endoscopic retrograde cholangiopancreatography (ERCP), a highly technical procedure and a key access route to the pancreas, an organ that is fragile, difficult to instrument, and central to diseases such as pancreatic cancer. Our architecture supports up to four independently controlled functional units, allowing customizable combinations of anchoring, manipulation, sensing, and targeted drug delivery. In a live porcine model, we demonstrate semi-autonomous deployment into the pancreatic duct and 7.5 cm of endoscopic navigation within it, a region currently inaccessible with standard catheters. A closed-loop autonomous/shared-control system that combines a learned model, magnetic actuation, onboard shape sensing, and visual marker tracking further improves cannulation accuracy. Together, these results establish a scalable platform for multifunctional soft robotic catheters and a new paradigm for complex endoluminal interventions, with potential to reduce radiation exposure, shorten training, and accelerate clinical translation of soft robotic technologies.

</details>


### [13] [On-the-fly hand-eye calibration for the da Vinci surgical robot](https://arxiv.org/abs/2601.14871)
*Zejian Cui,Ferdinando Rodriguez y Baena*

Main category: cs.RO

TL;DR: 提出一种用于机器人辅助微创手术的在线手眼标定框架，通过特征关联和标定算法提高电缆驱动机器人工具定位精度，在多种手术场景下显著降低定位误差且时间效率更高。


<details>
  <summary>Details</summary>
Motivation: 在机器人辅助微创手术中，电缆驱动机器人（如达芬奇机器人）由于编码器读数错误导致工具定位不准确，这对患者安全和手术成功至关重要，因此需要一种有效的标定方法来解决这一问题。

Method: 提出一个包含两个相互关联算法的标定框架：特征关联模块和手眼标定模块。特征关联模块无需预训练即可为单目图像检测到的关键点提供鲁棒对应关系；手眼标定模块采用多种滤波方法以适应各种手术场景，实现在线手眼变换矩阵计算。

Result: 在公开视频数据集上进行了广泛测试，涵盖体外和离体场景、不同光照条件和关键点测量精度。结果显示，在提出的标定框架下，工具定位误差显著降低，精度与最先进方法相当，但时间效率更高。

Conclusion: 该在线手眼标定框架能够有效提高电缆驱动机器人在机器人辅助微创手术中的工具定位精度，适应多种手术场景，在保持高精度的同时具有更好的时间效率。

Abstract: In Robot-Assisted Minimally Invasive Surgery (RMIS), accurate tool localization is crucial to ensure patient safety and successful task execution. However, this remains challenging for cable-driven robots, such as the da Vinci robot, because erroneous encoder readings lead to pose estimation errors. In this study, we propose a calibration framework to produce accurate tool localization results through computing the hand-eye transformation matrix on-the-fly. The framework consists of two interrelated algorithms: the feature association block and the hand-eye calibration block, which provide robust correspondences for key points detected on monocular images without pre-training, and offer the versatility to accommodate various surgical scenarios by adopting an array of filter approaches, respectively. To validate its efficacy, we test the framework extensively on publicly available video datasets that feature multiple surgical instruments conducting tasks in both in vitro and ex vivo scenarios, under varying illumination conditions and with different levels of key point measurement accuracy. The results show a significant reduction in tool localization errors under the proposed calibration framework, with accuracies comparable to other state-of-the-art methods while being more time-efficient.

</details>


### [14] [HumanoidVLM: Vision-Language-Guided Impedance Control for Contact-Rich Humanoid Manipulation](https://arxiv.org/abs/2601.14874)
*Yara Mahmoud,Yasheerah Yaqoot,Miguel Altamirano Cabrera,Dzmitry Tsetserukou*

Main category: cs.RO

TL;DR: HumanoidVLM：基于视觉语言模型的检索框架，让Unitree G1人形机器人通过第一人称RGB图像选择任务相关的笛卡尔阻抗参数和抓取器配置，实现自适应操作。


<details>
  <summary>Details</summary>
Motivation: 人形机器人需要适应不同物体和任务的接触行为，但现有控制器大多依赖固定、手动调整的阻抗增益和抓取器设置，缺乏自适应能力。

Method: 结合视觉语言模型进行语义任务推理，使用FAISS基础的检索增强生成模块从两个自定义数据库中检索实验验证的刚度-阻尼对和物体特定抓取角度，通过任务空间阻抗控制器执行。

Result: 在14个视觉场景中达到93%的检索准确率；实际实验中z轴跟踪误差通常在1-3.5厘米内，虚拟力与任务相关阻抗设置一致，展示稳定的交互动力学。

Conclusion: 将语义感知与基于检索的控制相结合，为人形机器人自适应操作提供了一条可解释的路径，证明了该方法的可行性。

Abstract: Humanoid robots must adapt their contact behavior to diverse objects and tasks, yet most controllers rely on fixed, hand-tuned impedance gains and gripper settings. This paper introduces HumanoidVLM, a vision-language driven retrieval framework that enables the Unitree G1 humanoid to select task-appropriate Cartesian impedance parameters and gripper configurations directly from an egocentric RGB image. The system couples a vision-language model for semantic task inference with a FAISS-based Retrieval-Augmented Generation (RAG) module that retrieves experimentally validated stiffness-damping pairs and object-specific grasp angles from two custom databases, and executes them through a task-space impedance controller for compliant manipulation. We evaluate HumanoidVLM on 14 visual scenarios and achieve a retrieval accuracy of 93%. Real-world experiments show stable interaction dynamics, with z-axis tracking errors typically within 1-3.5 cm and virtual forces consistent with task-dependent impedance settings. These results demonstrate the feasibility of linking semantic perception with retrieval-based control as an interpretable path toward adaptive humanoid manipulation.

</details>


### [15] [Vision-Language Models on the Edge for Real-Time Robotic Perception](https://arxiv.org/abs/2601.14921)
*Sarat Ahmad,Maryam Hafeez,Syed Ali Raza Zaidi*

Main category: cs.RO

TL;DR: 该研究探索在6G边缘计算（ORAN/MEC）上部署视觉语言模型，使用Unitree G1人形机器人作为测试平台，比较边缘与云端部署的性能差异。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型在机器人感知和交互中具有重要作用，但在实际部署中面临延迟、资源限制和云卸载隐私风险等挑战。6G边缘智能（特别是Open RAN和MEC）通过将计算靠近数据源，为解决这些问题提供了途径。

Method: 使用Unitree G1人形机器人作为实体测试平台，设计基于WebRTC的管道将多模态数据流传输到边缘节点。评估LLaMA-3.2-11B-Vision-Instruct在边缘与云端的实时部署性能，并进一步评估为资源受限环境优化的紧凑模型Qwen2-VL-2B-Instruct。

Result: 边缘部署在保持接近云端准确性的同时，将端到端延迟降低了5%。紧凑模型Qwen2-VL-2B-Instruct实现了亚秒级响应，将延迟减少了一半以上，但以准确性为代价。

Conclusion: 在ORAN/MEC基础设施上部署视觉语言模型是可行的，边缘计算能够在保持准确性的同时显著降低延迟，而紧凑模型则提供了延迟与准确性之间的权衡选择，为实时机器人应用提供了实用解决方案。

Abstract: Vision-Language Models (VLMs) enable multimodal reasoning for robotic perception and interaction, but their deployment in real-world systems remains constrained by latency, limited onboard resources, and privacy risks of cloud offloading. Edge intelligence within 6G, particularly Open RAN and Multi-access Edge Computing (MEC), offers a pathway to address these challenges by bringing computation closer to the data source. This work investigates the deployment of VLMs on ORAN/MEC infrastructure using the Unitree G1 humanoid robot as an embodied testbed. We design a WebRTC-based pipeline that streams multimodal data to an edge node and evaluate LLaMA-3.2-11B-Vision-Instruct deployed at the edge versus in the cloud under real-time conditions. Our results show that edge deployment preserves near-cloud accuracy while reducing end-to-end latency by 5\%. We further evaluate Qwen2-VL-2B-Instruct, a compact model optimized for resource-constrained environments, which achieves sub-second responsiveness, cutting latency by more than half but at the cost of accuracy.

</details>


### [16] [TIDAL: Temporally Interleaved Diffusion and Action Loop for High-Frequency VLA Control](https://arxiv.org/abs/2601.14945)
*Yuteng Sun,Haoran Wang,Ruofei Bai,Zhengguo Li,Jun Li,Meng Yee,Chuah,Wei Yun Yau*

Main category: cs.RO

TL;DR: TIDAL提出了一种分层框架，通过解耦语义推理和高频执行来解决VLA模型推理延迟问题，实现约9Hz控制更新，在动态拦截任务中性能提升2倍。


<details>
  <summary>Details</summary>
Motivation: 大规模视觉-语言-动作模型存在高推理延迟问题，只能采用低频批处理执行范式，导致在动态环境中目标移动时出现执行盲区。

Method: TIDAL采用双频架构：低频宏意图循环缓存语义嵌入，高频微控制循环交错单步流集成与执行。引入时间错位训练策略和差分运动预测器。

Result: 在边缘硬件上实现约9Hz控制更新（基线约2.4Hz），动态拦截任务性能提升2倍，反馈频率提高4倍，语义嵌入有效范围超越原生动作块大小。

Conclusion: TIDAL通过架构级解耦解决了VLA模型的延迟问题，在动态环境中保持鲁棒性，为语义推理与高频执行的有效结合提供了解决方案。

Abstract: Large-scale Vision-Language-Action (VLA) models offer semantic generalization but suffer from high inference latency, limiting them to low-frequency batch-and-execute paradigm. This frequency mismatch creates an execution blind spot, causing failures in dynamic environments where targets move during the open-loop execution window. We propose TIDAL (Temporally Interleaved Diffusion and Action Loop), a hierarchical framework that decouples semantic reasoning from high-frequency actuation. TIDAL operates as a backbone-agnostic module for diffusion-based VLAs, using a dual-frequency architecture to redistribute the computational budget. Specifically, a low-frequency macro-intent loop caches semantic embeddings, while a high-frequency micro-control loop interleaves single-step flow integration with execution. This design enables approximately 9 Hz control updates on edge hardware (vs. approximately 2.4 Hz baselines) without increasing marginal overhead. To handle the resulting latency shift, we introduce a temporally misaligned training strategy where the policy learns predictive compensation using stale semantic intent alongside real-time proprioception. Additionally, we address the insensitivity of static vision encoders to velocity by incorporating a differential motion predictor. TIDAL is architectural, making it orthogonal to system-level optimizations. Experiments show a 2x performance gain over open-loop baselines in dynamic interception tasks. Despite a marginal regression in static success rates, our approach yields a 4x increase in feedback frequency and extends the effective horizon of semantic embeddings beyond the native action chunk size. Under non-paused inference protocols, TIDAL remains robust where standard baselines fail due to latency.

</details>


### [17] [HumanDiffusion: A Vision-Based Diffusion Trajectory Planner with Human-Conditioned Goals for Search and Rescue UAV](https://arxiv.org/abs/2601.14973)
*Faryal Batool,Iana Zhura,Valerii Serpiva,Roohan Ahmed Khan,Ivan Valuev,Issatay Tokmurziyev,Dzmitry Tsetserukou*

Main category: cs.RO

TL;DR: HumanDiffusion：基于RGB图像的轻量级扩散规划器，用于无人机在紧急场景中实现人类感知的导航，无需先验地图或复杂规划管道


<details>
  <summary>Details</summary>
Motivation: 紧急场景中可靠的人机协作需要自主系统能够检测人类、推断导航目标，并在动态环境中安全操作。现有方法通常依赖先验地图或计算密集的规划管道，限制了在时间关键场景中的应用。

Method: 结合YOLO-11人类检测与扩散驱动的轨迹生成，直接从RGB图像生成人类感知的导航轨迹。轨迹在像素空间中预测，确保平滑运动并保持与人类的安全距离。

Result: 在300个样本测试集上，模型在像素空间轨迹重建中达到0.02的均方误差。真实世界实验中，在部分遮挡的事故响应和搜索定位任务中，整体任务成功率达到80%。

Conclusion: 人类条件扩散规划为时间关键援助场景中的人类感知无人机导航提供了实用且鲁棒的解决方案，能够在不依赖先验地图的情况下实现可靠的人机协作。

Abstract: Reliable human--robot collaboration in emergency scenarios requires autonomous systems that can detect humans, infer navigation goals, and operate safely in dynamic environments. This paper presents HumanDiffusion, a lightweight image-conditioned diffusion planner that generates human-aware navigation trajectories directly from RGB imagery. The system combines YOLO-11--based human detection with diffusion-driven trajectory generation, enabling a quadrotor to approach a target person and deliver medical assistance without relying on prior maps or computationally intensive planning pipelines. Trajectories are predicted in pixel space, ensuring smooth motion and a consistent safety margin around humans. We evaluate HumanDiffusion in simulation and real-world indoor mock-disaster scenarios. On a 300-sample test set, the model achieves a mean squared error of 0.02 in pixel-space trajectory reconstruction. Real-world experiments demonstrate an overall mission success rate of 80% across accident-response and search-and-locate tasks with partial occlusions. These results indicate that human-conditioned diffusion planning offers a practical and robust solution for human-aware UAV navigation in time-critical assistance settings.

</details>


### [18] [Graph-Based Adaptive Planning for Coordinated Dual-Arm Robotic Disassembly of Electronic Devices (eGRAP)](https://arxiv.org/abs/2601.14998)
*Adip Ranjan Das,Maria Koskinopoulou*

Main category: cs.RO

TL;DR: 提出eGRAP系统，通过视觉、动态规划和双机械臂执行实现电子设备的自主拆解，在硬盘驱动器上验证了高效拆解能力


<details>
  <summary>Details</summary>
Motivation: 电子废弃物快速增长而回收率低，需要自动化拆解解决方案来应对这一挑战

Method: 使用基于图的适应性规划(eGRAP)，集成视觉识别、动态规划和双机械臂协同执行。通过摄像头识别部件和姿态估计，有向图编码拆解顺序，调度器使用拓扑排序选择下一步操作并分配给两个机械臂并行执行

Result: 在3.5英寸硬盘驱动器上成功演示，系统能够在线更新图和计划，实现一致的完整拆解，具有高成功率和高效的循环时间

Conclusion: eGRAP方法能够实时自适应协调双机械臂任务，为电子废弃物自动化拆解提供了有效解决方案

Abstract: E-waste is growing rapidly while recycling rates remain low. We propose an electronic-device Graph-based Adaptive Planning (eGRAP) that integrates vision, dynamic planning, and dual-arm execution for autonomous disassembly. A camera-equipped arm identifies parts and estimates their poses, and a directed graph encodes which parts must be removed first. A scheduler uses topological ordering of this graph to select valid next steps and assign them to two robot arms, allowing independent tasks to run in parallel. One arm carries a screwdriver (with an eye-in-hand depth camera) and the other holds or handles components. We demonstrate eGRAP on 3.5in hard drives: as parts are unscrewed and removed, the system updates its graph and plan online. Experiments show consistent full disassembly of each HDD, with high success rates and efficient cycle times, illustrating the method's ability to adaptively coordinate dual-arm tasks in real time.

</details>


### [19] [DWPP: Dynamic Window Pure Pursuit Considering Velocity and Acceleration Constraints](https://arxiv.org/abs/2601.15006)
*Fumiya Ohnishi,Masaki Takahashi*

Main category: cs.RO

TL;DR: DWPP是一种改进的纯追踪算法，通过将速度指令计算重新表述在速度空间中，并考虑机器人的速度和加速度约束，从而避免违反约束的指令，提高路径跟踪精度。


<details>
  <summary>Details</summary>
Motivation: 传统纯追踪算法及其变体虽然简单且计算效率高，但通常没有明确考虑速度和加速度约束，导致指令速度与实际速度之间存在差异，造成超调和跟踪性能下降。

Method: DWPP从根本上重新表述了速度指令计算过程，在速度空间（v-ω平面）中制定速度指令计算，并在动态窗口中选择最接近ω=κv线的点作为指令速度。

Result: 实验结果表明，DWPP能够避免违反约束的指令，相比传统纯追踪方法实现了更优的路径跟踪精度。该方法已集成到官方Nav2仓库并公开可用。

Conclusion: DWPP通过显式纳入速度和加速度约束，解决了传统纯追踪算法中指令速度与实际速度不匹配的问题，显著提升了路径跟踪性能。

Abstract: Pure pursuit and its variants are widely used for mobile robot path tracking owing to their simplicity and computational efficiency. However, many conventional approaches do not explicitly account for velocity and acceleration constraints, resulting in discrepancies between commanded and actual velocities that result in overshoot and degraded tracking performance. To address this problem, this paper proposes dynamic window pure pursuit (DWPP), which fundamentally reformulates the command velocity computation process to explicitly incorporate velocity and acceleration constraints. Specifically, DWPP formulates command velocity computation in the velocity space (the $v$-$ω$ plane) and selects the command velocity as the point within the dynamic window that is closest to the line $ω= κv$. Experimental results demonstrate that DWPP avoids constraint-violating commands and achieves superior path-tracking accuracy compared with conventional pure pursuit methods. The proposed method has been integrated into the official Nav2 repository and is publicly available (https://github.com/ros-navigation/navigation2).

</details>


### [20] [Risk Estimation for Automated Driving](https://arxiv.org/abs/2601.15018)
*Leon Tolksdorf,Arturo Tejada,Jonas Bauernfeind,Christian Birkner,Nathan van de Wouw*

Main category: cs.RO

TL;DR: 该论文提出了一种结合碰撞概率估计和碰撞严重性概念的通用风险估计方法，用于自动驾驶车辆的运动规划和安全评估。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶的安全评估需要准确的风险估计。现有方法要么依赖经验模型，要么采用严重近似，缺乏通用性和准确性。自动驾驶中的风险包含两个关键方面：对其他道路参与者状态估计的不确定性，以及碰撞事件的严重性。

Method: 结合碰撞概率估计的最新进展与碰撞严重性概念，开发了一种通用的风险估计方法。该方法允许为不同的碰撞场景（如正面碰撞或侧面碰撞）分配单独的严重性函数，并支持高斯不确定性建模。

Result: 提出的方法具有计算效率高的特点，适用于实时运动规划应用。论文还提供了高斯不确定性下的示例实现代码。

Conclusion: 该方法为自动驾驶车辆提供了准确且通用的风险估计工具，能够支持运动规划技术并改善安全评估，通过约束或最小化风险来引导车辆在保持安全距离的同时规避其他交通参与者。

Abstract: Safety is a central requirement for automated vehicles. As such, the assessment of risk in automated driving is key in supporting both motion planning technologies and safety evaluation. In automated driving, risk is characterized by two aspects. The first aspect is the uncertainty on the state estimates of other road participants by an automated vehicle. The second aspect is the severity of a collision event with said traffic participants. Here, the uncertainty aspect typically causes the risk to be non-zero for near-collision events. This makes risk particularly useful for automated vehicle motion planning. Namely, constraining or minimizing risk naturally navigates the automated vehicle around traffic participants while keeping a safety distance based on the level of uncertainty and the potential severity of the impending collision. Existing approaches to calculate the risk either resort to empirical modeling or severe approximations, and, hence, lack generalizability and accuracy. In this paper, we combine recent advances in collision probability estimation with the concept of collision severity to develop a general method for accurate risk estimation. The proposed method allows us to assign individual severity functions for different collision constellations, such as, e.g., frontal or side collisions. Furthermore, we show that the proposed approach is computationally efficient, which is beneficial, e.g., in real-time motion planning applications. The programming code for an exemplary implementation of Gaussian uncertainties is also provided.

</details>


### [21] [CADGrasp: Learning Contact and Collision Aware General Dexterous Grasping in Cluttered Scenes](https://arxiv.org/abs/2601.15039)
*Jiyao Zhang,Zhiyuan Ma,Tianhao Wu,Zeyuan Chen,Hao Dong*

Main category: cs.RO

TL;DR: CADGrasp是一个两阶段算法，使用单视角点云输入实现灵巧抓取，通过预测稀疏IBS表示和优化生成高质量、无碰撞的抓取姿态。


<details>
  <summary>Details</summary>
Motivation: 解决在杂乱环境中灵巧抓取的挑战，包括灵巧手的高自由度、遮挡问题、以及不同物体几何形状和复杂布局导致的潜在碰撞问题。

Method: 两阶段算法：第一阶段预测稀疏IBS（场景解耦、接触感知和碰撞感知表示）作为优化目标，使用具有体素级条件引导和力闭合分数过滤的占用扩散模型；第二阶段基于稀疏IBS开发能量函数和排序策略进行优化，生成高质量灵巧抓取姿态。

Result: 在模拟和真实世界环境中进行了广泛实验，验证了方法的有效性，能够减少碰撞同时在不同物体和复杂场景中保持高抓取成功率。

Conclusion: CADGrasp算法能够有效处理杂乱环境中的灵巧抓取问题，通过稀疏IBS表示和优化策略实现了稳定、无碰撞的高质量抓取。

Abstract: Dexterous grasping in cluttered environments presents substantial challenges due to the high degrees of freedom of dexterous hands, occlusion, and potential collisions arising from diverse object geometries and complex layouts. To address these challenges, we propose CADGrasp, a two-stage algorithm for general dexterous grasping using single-view point cloud inputs. In the first stage, we predict sparse IBS, a scene-decoupled, contact- and collision-aware representation, as the optimization target. Sparse IBS compactly encodes the geometric and contact relationships between the dexterous hand and the scene, enabling stable and collision-free dexterous grasp pose optimization. To enhance the prediction of this high-dimensional representation, we introduce an occupancy-diffusion model with voxel-level conditional guidance and force closure score filtering. In the second stage, we develop several energy functions and ranking strategies for optimization based on sparse IBS to generate high-quality dexterous grasp poses. Extensive experiments in both simulated and real-world settings validate the effectiveness of our approach, demonstrating its capability to mitigate collisions while maintaining a high grasp success rate across diverse objects and complex scenes.

</details>


### [22] [Systematic Evaluation of Hip Exoskeleton Assistance Parameters for Enhancing Gait Stability During Ground Slip Perturbations](https://arxiv.org/abs/2601.15056)
*Maria T. Tagliaferri,Inseung Kang*

Main category: cs.RO

TL;DR: 研究通过系统调节双侧髋关节外骨骼的扭矩大小和持续时间，发现外骨骼辅助对稳定性的影响取决于持续时间参数，稳定性优化参数相比能量优化控制器能减少25.7%的全身角动量范围。


<details>
  <summary>Details</summary>
Motivation: 跌倒是老年人受伤相关住院和死亡的主要原因，现有外骨骼控制器主要优化行走能量消耗而非稳定性，特定参数（如辅助大小和持续时间）对稳定性的影响尚未探索。

Method: 在8名健康成年人中进行滑倒扰动实验，系统调节双侧髋关节外骨骼的扭矩大小和持续时间，使用全身角动量（WBAM）量化稳定性，并与现有能量优化控制器进行比较。

Result: 辅助大小和持续时间之间存在显著交互作用，持续时间决定外骨骼辅助是稳定还是不稳定；稳定性优化参数相比能量优化控制器减少25.7%的WBAM范围；观察到显著的受试者间变异性。

Conclusion: 仅优化外骨骼辅助的能量消耗不足以改善步态扰动期间的响应稳定性；稳定性导向的外骨骼控制应优先考虑时间辅助参数并包含用户特异性个性化，这对改善老年人稳定性和降低跌倒风险有直接意义。

Abstract: Falls are the leading cause of injury related hospitalization and mortality among older adults. Consequently, mitigating age-related declines in gait stability and reducing fall risk during walking is a critical goal for assistive devices. Lower-limb exoskeletons have the potential to support users in maintaining stability during walking. However, most exoskeleton controllers are optimized to reduce the energetic cost of walking rather than to improve stability. While some studies report stability benefits with assistance, the effects of specific parameters, such as assistance magnitude and duration, remain unexplored. To address this gap, we systematically modulated the magnitude and duration of torque provided by a bilateral hip exoskeleton during slip perturbations in eight healthy adults, quantifying stability using whole-body angular momentum (WBAM). WBAM responses were governed by a significant interaction between assistance magnitude and duration, with duration determining whether exoskeleton assistance was stabilizing or destabilizing relative to not wearing the exoskeleton device. Compared to an existing energy-optimized controller, experimentally identified stability-optimal parameters reduced WBAM range by 25.7% on average. Notably, substantial inter-subject variability was observed in the parameter combinations that minimized WBAM during perturbations. We found that optimizing exoskeleton assistance for energetic outcomes alone is insufficient for improving reactive stability during gait perturbations. Stability-focused exoskeleton control should prioritize temporal assistance parameters and include user-specific personalization. This study represents an important step toward personalized, stability-focused exoskeleton control, with direct implications for improving stability and reducing fall risk in older adults.

</details>


### [23] [Influence of Operator Expertise on Robot Supervision and Intervention](https://arxiv.org/abs/2601.15069)
*Yanran Jiang,Pavan Sikka,Leimin Tian,Dana Kuliic,Cecile Paris*

Main category: cs.RO

TL;DR: 本研究探索不同专业水平的用户如何监督远程机器人，发现新手、中级和专家用户在干预时机和决策策略上存在差异。


<details>
  <summary>Details</summary>
Motivation: 随着机器人自主性提高，监督机器人的用户群体越来越多样化，包括不同专业水平的用户。了解不同专业水平的用户如何执行监督任务以及这对人机团队绩效的影响变得很重要。

Method: 进行用户研究（N=27），参与者在模拟器中监督机器人自主探索四个未知隧道环境，当他们认为机器人遇到困难时提供航点进行干预。通过分析交互数据和问卷回答来研究不同专业水平用户的干预模式。

Result: 通过分析交互数据和问卷回答，识别了新手、中级和专家用户在干预时机和决策策略上的不同模式。

Conclusion: 不同专业水平的用户在监督机器人时表现出不同的干预模式和决策策略，这对设计适应不同用户水平的机器人监督系统具有重要意义。

Abstract: With increasing levels of robot autonomy, robots are increasingly being supervised by users with varying levels of robotics expertise. As the diversity of the user population increases, it is important to understand how users with different expertise levels approach the supervision task and how this impacts performance of the human-robot team. This exploratory study investigates how operators with varying expertise levels perceive information and make intervention decisions when supervising a remote robot. We conducted a user study (N=27) where participants supervised a robot autonomously exploring four unknown tunnel environments in a simulator, and provided waypoints to intervene when they believed the robot had encountered difficulties. By analyzing the interaction data and questionnaire responses, we identify differing patterns in intervention timing and decision-making strategies across novice, intermediate, and expert users.

</details>


### [24] [V-CAGE: Context-Aware Generation and Verification for Scalable Long-Horizon Embodied Tasks](https://arxiv.org/abs/2601.15164)
*Yaru Liu,Ao-bo Wang,Nanyang Ye*

Main category: cs.RO

TL;DR: V-CAGE是一个闭环框架，用于生成物理合理、语义对齐的机器人操作数据集，通过上下文感知的场景实例化、分层指令分解和VLM验证循环来解决合成数据中的物理不可行和语义不对齐问题。


<details>
  <summary>Details</summary>
Motivation: 现有合成数据方法存在三个主要问题：1）生成的场景物理上不可行；2）语言驱动程序经常"成功"但未满足任务语义；3）高级指令需要接地到可执行动作序列。这些限制了从合成数据学习长时程具身行为的效果。

Method: V-CAGE框架包含三个核心组件：1）上下文感知实例化机制，通过动态维护禁止空间区域地图确保几何一致性；2）分层指令分解模块，将高级目标分解为组合动作基元；3）VLM验证循环，作为视觉批评器进行严格拒绝采样，过滤"静默失败"。

Result: 实验表明V-CAGE生成的数据集具有优越的物理和语义保真度，相比未验证的基线方法，显著提高了下游策略的成功率和泛化能力。

Conclusion: V-CAGE通过闭环验证框架解决了合成数据中的物理和语义对齐问题，为大规模生成鲁棒的机器人操作数据集提供了有效方案，推动了从合成数据学习长时程具身行为的发展。

Abstract: Learning long-horizon embodied behaviors from synthetic data remains challenging because generated scenes are often physically implausible, language-driven programs frequently "succeed" without satisfying task semantics, and high-level instructions require grounding into executable action sequences. To address these limitations, we introduce V-CAGE, a closed-loop framework for generating robust, semantically aligned manipulation datasets at scale. First, we propose a context-aware instantiation mechanism that enforces geometric consistency during scene synthesis. By dynamically maintaining a map of prohibited spatial areas as objects are placed, our system prevents interpenetration and ensures reachable, conflict-free configurations in cluttered environments. Second, to bridge the gap between abstract intent and low-level control, we employ a hierarchical instruction decomposition module. This decomposes high-level goals (e.g., "get ready for work") into compositional action primitives, facilitating coherent long-horizon planning. Crucially, we enforce semantic correctness through a VLM-based verification loop. Acting as a visual critic, the VLM performs rigorous rejection sampling after each subtask, filtering out "silent failures" where code executes but fails to achieve the visual goal. Experiments demonstrate that V-CAGE yields datasets with superior physical and semantic fidelity, significantly boosting the success rate and generalization of downstream policies compared to non-verified baselines.

</details>
