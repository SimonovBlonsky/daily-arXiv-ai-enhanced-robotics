{"id": "2602.15060", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.15060", "abs": "https://arxiv.org/abs/2602.15060", "authors": ["Tengjie Zhu", "Guanyu Cai", "Yang Zhaohui", "Guanzhu Ren", "Haohui Xie", "ZiRui Wang", "Junsong Wu", "Jingbo Wang", "Xiaokang Yang", "Yao Mu", "Yichao Yan", "Yichao Yan"], "title": "CLOT: Closed-Loop Global Motion Tracking for Whole-Body Humanoid Teleoperation", "comment": null, "summary": "Long-horizon whole-body humanoid teleoperation remains challenging due to accumulated global pose drift, particularly on full-sized humanoids. Although recent learning-based tracking methods enable agile and coordinated motions, they typically operate in the robot's local frame and neglect global pose feedback, leading to drift and instability during extended execution. In this work, we present CLOT, a real-time whole-body humanoid teleoperation system that achieves closed-loop global motion tracking via high-frequency localization feedback. CLOT synchronizes operator and robot poses in a closed loop, enabling drift-free human-to-humanoid mimicry over long timehorizons. However, directly imposing global tracking rewards in reinforcement learning, often results in aggressive and brittle corrections. To address this, we propose a data-driven randomization strategy that decouples observation trajectories from reward evaluation, enabling smooth and stable global corrections. We further regularize the policy with an adversarial motion prior to suppress unnatural behaviors. To support CLOT, we collect 20 hours of carefully curated human motion data for training the humanoid teleoperation policy. We design a transformer-based policy and train it for over 1300 GPU hours. The policy is deployed on a full-sized humanoid with 31 DoF (excluding hands). Both simulation and real-world experiments verify high-dynamic motion, high-precision tracking, and strong robustness in sim-to-real humanoid teleoperation. Motion data, demos and code can be found in our website.", "AI": {"tldr": "CLOT\u662f\u4e00\u4e2a\u5b9e\u65f6\u5168\u8eab\u4eba\u5f62\u673a\u5668\u4eba\u9065\u64cd\u4f5c\u7cfb\u7edf\uff0c\u901a\u8fc7\u9ad8\u9891\u5b9a\u4f4d\u53cd\u9988\u5b9e\u73b0\u95ed\u73af\u5168\u5c40\u8fd0\u52a8\u8ddf\u8e2a\uff0c\u89e3\u51b3\u4e86\u957f\u65f6\u95f4\u9065\u64cd\u4f5c\u4e2d\u7684\u5168\u5c40\u59ff\u6001\u6f02\u79fb\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u7684\u5b66\u4e60\u578b\u8ddf\u8e2a\u65b9\u6cd5\u901a\u5e38\u5728\u673a\u5668\u4eba\u5c40\u90e8\u5750\u6807\u7cfb\u4e2d\u8fd0\u884c\uff0c\u5ffd\u7565\u4e86\u5168\u5c40\u59ff\u6001\u53cd\u9988\uff0c\u5bfc\u81f4\u957f\u65f6\u95f4\u6267\u884c\u65f6\u51fa\u73b0\u6f02\u79fb\u548c\u4e0d\u7a33\u5b9a\u6027\u3002\u957f\u65f6\u7a0b\u5168\u8eab\u4eba\u5f62\u673a\u5668\u4eba\u9065\u64cd\u4f5c\u9762\u4e34\u5168\u5c40\u59ff\u6001\u6f02\u79fb\u7d2f\u79ef\u7684\u6311\u6218\u3002", "method": "\u63d0\u51faCLOT\u7cfb\u7edf\uff0c\u901a\u8fc7\u6570\u636e\u9a71\u52a8\u7684\u968f\u673a\u5316\u7b56\u7565\u5c06\u89c2\u5bdf\u8f68\u8ff9\u4e0e\u5956\u52b1\u8bc4\u4f30\u89e3\u8026\uff0c\u5b9e\u73b0\u5e73\u6ed1\u7a33\u5b9a\u7684\u5168\u5c40\u6821\u6b63\uff1b\u4f7f\u7528\u5bf9\u6297\u8fd0\u52a8\u5148\u9a8c\u6b63\u5219\u5316\u7b56\u7565\u6291\u5236\u4e0d\u81ea\u7136\u884c\u4e3a\uff1b\u6536\u96c620\u5c0f\u65f6\u4eba\u7c7b\u8fd0\u52a8\u6570\u636e\u8bad\u7ec3\u57fa\u4e8eTransformer\u7684\u7b56\u7565\u3002", "result": "\u572831\u81ea\u7531\u5ea6\u5168\u5c3a\u5bf8\u4eba\u5f62\u673a\u5668\u4eba\u4e0a\u90e8\u7f72\uff0c\u4eff\u771f\u548c\u771f\u5b9e\u4e16\u754c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u9ad8\u52a8\u6001\u8fd0\u52a8\u3001\u9ad8\u7cbe\u5ea6\u8ddf\u8e2a\u4ee5\u53ca\u5f3a\u5927\u7684sim-to-real\u9c81\u68d2\u6027\u3002\u8bad\u7ec3\u8d85\u8fc71300GPU\u5c0f\u65f6\u3002", "conclusion": "CLOT\u5b9e\u73b0\u4e86\u65e0\u6f02\u79fb\u7684\u4eba\u5bf9\u4eba\u5f62\u673a\u5668\u4eba\u6a21\u4eff\uff0c\u89e3\u51b3\u4e86\u957f\u65f6\u7a0b\u9065\u64cd\u4f5c\u4e2d\u7684\u5168\u5c40\u59ff\u6001\u6f02\u79fb\u95ee\u9898\uff0c\u4e3a\u5b9e\u65f6\u5168\u8eab\u4eba\u5f62\u673a\u5668\u4eba\u9065\u64cd\u4f5c\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u95ed\u73af\u5168\u5c40\u8ddf\u8e2a\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.15061", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.15061", "abs": "https://arxiv.org/abs/2602.15061", "authors": ["Zihan Zhang", "Haohui Que", "Junhan Chang", "Xin Zhang", "Hao Wei", "Tong Zhu"], "title": "Safe-SDL:Establishing Safety Boundaries and Control Mechanisms for AI-Driven Self-Driving Laboratories", "comment": null, "summary": "The emergence of Self-Driving Laboratories (SDLs) transforms scientific discovery methodology by integrating AI with robotic automation to create closed-loop experimental systems capable of autonomous hypothesis generation, experimentation, and analysis. While promising to compress research timelines from years to weeks, their deployment introduces unprecedented safety challenges differing from traditional laboratories or purely digital AI. This paper presents Safe-SDL, a comprehensive framework for establishing robust safety boundaries and control mechanisms in AI-driven autonomous laboratories. We identify and analyze the critical ``Syntax-to-Safety Gap'' -- the disconnect between AI-generated syntactically correct commands and their physical safety implications -- as the central challenge in SDL deployment. Our framework addresses this gap through three synergistic components: (1) formally defined Operational Design Domains (ODDs) that constrain system behavior within mathematically verified boundaries, (2) Control Barrier Functions (CBFs) that provide real-time safety guarantees through continuous state-space monitoring, and (3) a novel Transactional Safety Protocol (CRUTD) that ensures atomic consistency between digital planning and physical execution. We ground our theoretical contributions through analysis of existing implementations including UniLabOS and the Osprey architecture, demonstrating how these systems instantiate key safety principles. Evaluation against the LabSafety Bench reveals that current foundation models exhibit significant safety failures, demonstrating that architectural safety mechanisms are essential rather than optional. Our framework provides both theoretical foundations and practical implementation guidance for safe deployment of autonomous scientific systems, establishing the groundwork for responsible acceleration of AI-driven discovery.", "AI": {"tldr": "Safe-SDL\u6846\u67b6\u4e3a\u81ea\u4e3b\u79d1\u5b66\u5b9e\u9a8c\u5ba4\u63d0\u4f9b\u5b89\u5168\u4fdd\u969c\uff0c\u901a\u8fc7\u5f62\u5f0f\u5316\u64cd\u4f5c\u8bbe\u8ba1\u57df\u3001\u63a7\u5236\u5c4f\u969c\u51fd\u6570\u548c\u4e8b\u52a1\u5b89\u5168\u534f\u8bae\u89e3\u51b3AI\u751f\u6210\u6307\u4ee4\u4e0e\u7269\u7406\u5b89\u5168\u4e4b\u95f4\u7684\u9e3f\u6c9f\u3002", "motivation": "\u81ea\u4e3b\u9a7e\u9a76\u5b9e\u9a8c\u5ba4\uff08SDLs\uff09\u5c06AI\u4e0e\u673a\u5668\u4eba\u81ea\u52a8\u5316\u7ed3\u5408\uff0c\u80fd\u6781\u5927\u52a0\u901f\u79d1\u5b66\u53d1\u73b0\uff0c\u4f46\u90e8\u7f72\u65f6\u9762\u4e34\u72ec\u7279\u7684\u5b89\u5168\u6311\u6218\uff0c\u7279\u522b\u662f\"\u8bed\u6cd5\u5230\u5b89\u5168\u9e3f\u6c9f\"\u2014\u2014AI\u751f\u6210\u7684\u8bed\u6cd5\u6b63\u786e\u6307\u4ee4\u4e0e\u7269\u7406\u5b89\u5168\u5f71\u54cd\u4e4b\u95f4\u7684\u8131\u8282\u3002", "method": "\u63d0\u51faSafe-SDL\u6846\u67b6\uff0c\u5305\u542b\u4e09\u4e2a\u534f\u540c\u7ec4\u4ef6\uff1a1) \u5f62\u5f0f\u5316\u5b9a\u4e49\u7684\u64cd\u4f5c\u8bbe\u8ba1\u57df\uff08ODDs\uff09\uff0c\u5c06\u7cfb\u7edf\u884c\u4e3a\u9650\u5236\u5728\u6570\u5b66\u9a8c\u8bc1\u7684\u8fb9\u754c\u5185\uff1b2) \u63a7\u5236\u5c4f\u969c\u51fd\u6570\uff08CBFs\uff09\uff0c\u901a\u8fc7\u8fde\u7eed\u72b6\u6001\u7a7a\u95f4\u76d1\u63a7\u63d0\u4f9b\u5b9e\u65f6\u5b89\u5168\u4fdd\u8bc1\uff1b3) \u65b0\u9896\u7684\u4e8b\u52a1\u5b89\u5168\u534f\u8bae\uff08CRUTD\uff09\uff0c\u786e\u4fdd\u6570\u5b57\u89c4\u5212\u4e0e\u7269\u7406\u6267\u884c\u4e4b\u95f4\u7684\u539f\u5b50\u4e00\u81f4\u6027\u3002", "result": "\u901a\u8fc7\u5bf9UniLabOS\u548cOsprey\u67b6\u6784\u7b49\u73b0\u6709\u5b9e\u73b0\u7684\u5206\u6790\uff0c\u9a8c\u8bc1\u4e86\u5b89\u5168\u539f\u5219\u7684\u5b9e\u4f8b\u5316\u3002\u5728LabSafety Bench\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0c\u5f53\u524d\u57fa\u7840\u6a21\u578b\u5b58\u5728\u663e\u8457\u7684\u5b89\u5168\u5931\u8d25\uff0c\u8bc1\u660e\u67b6\u6784\u5b89\u5168\u673a\u5236\u662f\u5fc5\u9700\u800c\u975e\u53ef\u9009\u7684\u3002", "conclusion": "Safe-SDL\u6846\u67b6\u4e3a\u81ea\u4e3b\u79d1\u5b66\u7cfb\u7edf\u7684\u5b89\u5168\u90e8\u7f72\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u548c\u5b9e\u8df5\u6307\u5bfc\uff0c\u4e3a\u8d1f\u8d23\u4efb\u5730\u52a0\u901fAI\u9a71\u52a8\u7684\u79d1\u5b66\u53d1\u73b0\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2602.15092", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.15092", "abs": "https://arxiv.org/abs/2602.15092", "authors": ["Xuanyun Qiu", "Dorian Verdel", "Hector Cervantes-Culebro", "Alexis Devillard", "Etienne Burdet"], "title": "Augmenting Human Balance with Generic Supernumerary Robotic Limbs", "comment": null, "summary": "Supernumerary robotic limbs (SLs) have the potential to transform a wide range of human activities, yet their usability remains limited by key technical challenges, particularly in ensuring safety and achieving versatile control. Here, we address the critical problem of maintaining balance in the human-SLs system, a prerequisite for safe and comfortable augmentation tasks. Unlike previous approaches that developed SLs specifically for stability support, we propose a general framework for preserving balance with SLs designed for generic use. Our hierarchical three-layer architecture consists of: (i) a prediction layer that estimates human trunk and center of mass (CoM) dynamics, (ii) a planning layer that generates optimal CoM trajectories to counteract trunk movements and computes the corresponding SL control inputs, and (iii) a control layer that executes these inputs on the SL hardware. We evaluated the framework with ten participants performing forward and lateral bending tasks. The results show a clear reduction in stance instability, demonstrating the framework's effectiveness in enhancing balance. This work paves the path towards safe and versatile human-SLs interactions. [This paper has been submitted for publication to IEEE.]", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u7528\u6846\u67b6\uff0c\u901a\u8fc7\u4e09\u5c42\u5206\u5c42\u67b6\u6784\uff08\u9884\u6d4b\u3001\u89c4\u5212\u3001\u63a7\u5236\u5c42\uff09\u6765\u4fdd\u6301\u4eba\u673a\u7cfb\u7edf\u5e73\u8861\uff0c\u5b9e\u9a8c\u8bc1\u660e\u80fd\u6709\u6548\u51cf\u5c11\u7ad9\u7acb\u4e0d\u7a33\u5b9a\u6027", "motivation": "\u8d85\u9650\u673a\u5668\u4eba\u80a2\u4f53\uff08SLs\uff09\u5728\u5e7f\u6cdb\u5e94\u7528\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u5176\u53ef\u7528\u6027\u53d7\u5230\u5b89\u5168\u548c\u591a\u529f\u80fd\u63a7\u5236\u7b49\u5173\u952e\u6280\u672f\u6311\u6218\u7684\u9650\u5236\u3002\u7279\u522b\u662f\u4fdd\u6301\u4eba\u673a\u7cfb\u7edf\u5e73\u8861\u662f\u5b89\u5168\u8212\u9002\u589e\u5f3a\u4efb\u52a1\u7684\u524d\u63d0\u6761\u4ef6\uff0c\u800c\u73b0\u6709\u65b9\u6cd5\u591a\u9488\u5bf9\u7279\u5b9a\u7a33\u5b9a\u6027\u652f\u6301\u8bbe\u8ba1SLs\uff0c\u7f3a\u4e4f\u901a\u7528\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u901a\u7528\u4e09\u5c42\u5206\u5c42\u67b6\u6784\uff1a1\uff09\u9884\u6d4b\u5c42\uff1a\u4f30\u8ba1\u4eba\u4f53\u8eaf\u5e72\u548c\u8d28\u5fc3\u52a8\u529b\u5b66\uff1b2\uff09\u89c4\u5212\u5c42\uff1a\u751f\u6210\u6700\u4f18\u8d28\u5fc3\u8f68\u8ff9\u4ee5\u62b5\u6d88\u8eaf\u5e72\u8fd0\u52a8\uff0c\u5e76\u8ba1\u7b97\u76f8\u5e94\u7684SL\u63a7\u5236\u8f93\u5165\uff1b3\uff09\u63a7\u5236\u5c42\uff1a\u5728SL\u786c\u4ef6\u4e0a\u6267\u884c\u8fd9\u4e9b\u63a7\u5236\u8f93\u5165\u3002", "result": "\u572810\u540d\u53c2\u4e0e\u8005\u8fdb\u884c\u524d\u540e\u548c\u4fa7\u5411\u5f2f\u66f2\u4efb\u52a1\u7684\u5b9e\u9a8c\u4e2d\uff0c\u7ed3\u679c\u663e\u793a\u7ad9\u7acb\u4e0d\u7a33\u5b9a\u6027\u660e\u663e\u51cf\u5c11\uff0c\u8bc1\u660e\u4e86\u8be5\u6846\u67b6\u5728\u589e\u5f3a\u5e73\u8861\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u4e3a\u5b9e\u73b0\u5b89\u5168\u3001\u591a\u529f\u80fd\u7684\u4eba\u673a\u4ea4\u4e92\u94fa\u5e73\u4e86\u9053\u8def\uff0c\u4e3a\u89e3\u51b3\u8d85\u9650\u673a\u5668\u4eba\u80a2\u4f53\u7cfb\u7edf\u7684\u5e73\u8861\u95ee\u9898\u63d0\u4f9b\u4e86\u901a\u7528\u6846\u67b6\u3002"}}
{"id": "2602.15162", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.15162", "abs": "https://arxiv.org/abs/2602.15162", "authors": ["Fernando Ca\u00f1adas-Ar\u00e1nega", "Francisco J. Ma\u00f1as-\u00c1lvarez", "Jos\u00e9 L- Guzm\u00e1n", "Jos\u00e9 C. Moreno", "Jos\u00e9 L. Blanco-Claraco"], "title": "A ROS2 Benchmarking Framework for Hierarchical Control Strategies in Mobile Robots for Mediterranean Greenhouses", "comment": "53 pages", "summary": "Mobile robots operating in agroindustrial environments, such as Mediterranean greenhouses, are subject to challenging conditions, including uneven terrain, variable friction, payload changes, and terrain slopes, all of which significantly affect control performance and stability. Despite the increasing adoption of robotic platforms in agriculture, the lack of standardized, reproducible benchmarks impedes fair comparisons and systematic evaluations of control strategies under realistic operating conditions. This paper presents a comprehensive benchmarking framework for evaluating mobile robot controllers in greenhouse environments. The proposed framework integrates an accurate three dimensional model of the environment, a physics based simulator, and a hierarchical control architecture comprising low, mid, and high level control layers. Three benchmark categories are defined to enable modular assessment, ranging from actuator level control to full autonomous navigation. Additionally, three disturbance scenarios payload variation, terrain type, and slope are explicitly modeled to replicate real world agricultural conditions. To ensure objective and reproducible evaluation, standardized performance metrics are introduced, including the Squared Absolute Error (SAE), the Squared Control Input (SCI), and composite performance indices. Statistical analysis based on repeated trials is employed to mitigate the influence of sensor noise and environmental variability. The framework is further enhanced by a plugin based architecture that facilitates seamless integration of user defined controllers and planners. The proposed benchmark provides a robust and extensible tool for the quantitative comparison of classical, predictive, and planning based control strategies in realistic conditions, bridging the gap between simulation based analysis and real world agroindustrial applications.", "AI": {"tldr": "\u63d0\u51fa\u7528\u4e8e\u6e29\u5ba4\u73af\u5883\u4e0b\u79fb\u52a8\u673a\u5668\u4eba\u63a7\u5236\u5668\u7684\u7efc\u5408\u57fa\u51c6\u6d4b\u8bd5\u6846\u67b6\uff0c\u5305\u542b\u4e09\u7ef4\u73af\u5883\u6a21\u578b\u3001\u7269\u7406\u4eff\u771f\u5668\u548c\u5206\u5c42\u63a7\u5236\u67b6\u6784\uff0c\u5b9a\u4e49\u4e09\u7c7b\u57fa\u51c6\u6d4b\u8bd5\u548c\u4e09\u79cd\u5e72\u6270\u573a\u666f\uff0c\u5f15\u5165\u6807\u51c6\u5316\u6027\u80fd\u6307\u6807\u786e\u4fdd\u5ba2\u89c2\u53ef\u91cd\u590d\u8bc4\u4f30\u3002", "motivation": "\u519c\u4e1a\u73af\u5883\u4e2d\u79fb\u52a8\u673a\u5668\u4eba\u9762\u4e34\u4e0d\u5e73\u5766\u5730\u5f62\u3001\u53ef\u53d8\u6469\u64e6\u3001\u8d1f\u8f7d\u53d8\u5316\u548c\u5761\u5ea6\u7b49\u6311\u6218\u6761\u4ef6\uff0c\u5f71\u54cd\u63a7\u5236\u6027\u80fd\u548c\u7a33\u5b9a\u6027\u3002\u76ee\u524d\u7f3a\u4e4f\u6807\u51c6\u5316\u3001\u53ef\u91cd\u590d\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u963b\u788d\u4e86\u5728\u771f\u5b9e\u64cd\u4f5c\u6761\u4ef6\u4e0b\u63a7\u5236\u7b56\u7565\u7684\u516c\u5e73\u6bd4\u8f83\u548c\u7cfb\u7edf\u8bc4\u4f30\u3002", "method": "\u96c6\u6210\u7cbe\u786e\u7684\u4e09\u7ef4\u73af\u5883\u6a21\u578b\u3001\u57fa\u4e8e\u7269\u7406\u7684\u4eff\u771f\u5668\u548c\u5206\u5c42\u63a7\u5236\u67b6\u6784\uff08\u4f4e\u3001\u4e2d\u3001\u9ad8\u5c42\u63a7\u5236\u5c42\uff09\u3002\u5b9a\u4e49\u4e09\u7c7b\u57fa\u51c6\u6d4b\u8bd5\uff08\u4ece\u6267\u884c\u5668\u7ea7\u63a7\u5236\u5230\u5b8c\u5168\u81ea\u4e3b\u5bfc\u822a\uff09\uff0c\u660e\u786e\u5efa\u6a21\u4e09\u79cd\u5e72\u6270\u573a\u666f\uff08\u8d1f\u8f7d\u53d8\u5316\u3001\u5730\u5f62\u7c7b\u578b\u548c\u5761\u5ea6\uff09\u3002\u5f15\u5165\u6807\u51c6\u5316\u6027\u80fd\u6307\u6807\uff08SAE\u3001SCI\u548c\u7efc\u5408\u6027\u80fd\u6307\u6570\uff09\uff0c\u91c7\u7528\u57fa\u4e8e\u91cd\u590d\u8bd5\u9a8c\u7684\u7edf\u8ba1\u5206\u6790\u51cf\u5c11\u4f20\u611f\u5668\u566a\u58f0\u548c\u73af\u5883\u53d8\u5f02\u6027\u7684\u5f71\u54cd\uff0c\u5e76\u63d0\u4f9b\u63d2\u4ef6\u5f0f\u67b6\u6784\u4fbf\u4e8e\u7528\u6237\u81ea\u5b9a\u4e49\u63a7\u5236\u5668\u548c\u89c4\u5212\u5668\u7684\u96c6\u6210\u3002", "result": "\u8be5\u6846\u67b6\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7a33\u5065\u4e14\u53ef\u6269\u5c55\u7684\u5de5\u5177\uff0c\u7528\u4e8e\u5728\u771f\u5b9e\u6761\u4ef6\u4e0b\u5bf9\u7ecf\u5178\u3001\u9884\u6d4b\u548c\u57fa\u4e8e\u89c4\u5212\u7684\u63a7\u5236\u7b56\u7565\u8fdb\u884c\u5b9a\u91cf\u6bd4\u8f83\uff0c\u5f25\u5408\u4e86\u57fa\u4e8e\u4eff\u771f\u7684\u5206\u6790\u4e0e\u771f\u5b9e\u519c\u4e1a\u5de5\u4e1a\u5e94\u7528\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002", "conclusion": "\u63d0\u51fa\u7684\u57fa\u51c6\u6d4b\u8bd5\u6846\u67b6\u4e3a\u6e29\u5ba4\u73af\u5883\u4e0b\u79fb\u52a8\u673a\u5668\u4eba\u63a7\u5236\u5668\u7684\u8bc4\u4f30\u63d0\u4f9b\u4e86\u5ba2\u89c2\u3001\u53ef\u91cd\u590d\u7684\u6807\u51c6\u5316\u65b9\u6cd5\uff0c\u6709\u52a9\u4e8e\u63a8\u52a8\u519c\u4e1a\u673a\u5668\u4eba\u63a7\u5236\u7b56\u7565\u7684\u516c\u5e73\u6bd4\u8f83\u548c\u7cfb\u7edf\u6539\u8fdb\u3002"}}
{"id": "2602.15201", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.15201", "abs": "https://arxiv.org/abs/2602.15201", "authors": ["Ren\u00e9 Zurbr\u00fcgg", "Andrei Cramariuc", "Marco Hutter"], "title": "DexEvolve: Evolutionary Optimization for Robust and Diverse Dexterous Grasp Synthesis", "comment": null, "summary": "Dexterous grasping is fundamental to robotics, yet data-driven grasp prediction heavily relies on large, diverse datasets that are costly to generate and typically limited to a narrow set of gripper morphologies. Analytical grasp synthesis can be used to scale data collection, but necessary simplifying assumptions often yield physically infeasible grasps that need to be filtered in high-fidelity simulators, significantly reducing the total number of grasps and their diversity.\n  We propose a scalable generate-and-refine pipeline for synthesizing large-scale, diverse, and physically feasible grasps. Instead of using high-fidelity simulators solely for verification and filtering, we leverage them as an optimization stage that continuously improves grasp quality without discarding precomputed candidates. More specifically, we initialize an evolutionary search with a seed set of analytically generated, potentially suboptimal grasps. We then refine these proposals directly in a high-fidelity simulator (Isaac Sim) using an asynchronous, gradient-free evolutionary algorithm, improving stability while maintaining diversity. In addition, this refinement stage can be guided toward human preferences and/or domain-specific quality metrics without requiring a differentiable objective. We further distill the refined grasp distribution into a diffusion model for robust real-world deployment, and highlight the role of diversity for both effective training and during deployment. Experiments on a newly introduced Handles dataset and a DexGraspNet subset demonstrate that our approach achieves over 120 distinct stable grasps per object (a 1.7-6x improvement over unrefined analytical methods) while outperforming diffusion-based alternatives by 46-60\\% in unique grasp coverage.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u751f\u6210-\u7cbe\u70bc\u6d41\u7a0b\uff0c\u7528\u4e8e\u5408\u6210\u5927\u89c4\u6a21\u3001\u591a\u6837\u5316\u4e14\u7269\u7406\u53ef\u884c\u7684\u6293\u53d6\uff0c\u901a\u8fc7\u8fdb\u5316\u7b97\u6cd5\u5728\u4eff\u771f\u4e2d\u4f18\u5316\u5206\u6790\u751f\u6210\u7684\u6293\u53d6\uff0c\u5e76\u5c06\u7ed3\u679c\u84b8\u998f\u5230\u6269\u6563\u6a21\u578b\u4e2d", "motivation": "\u6570\u636e\u9a71\u52a8\u7684\u6293\u53d6\u9884\u6d4b\u4f9d\u8d56\u6602\u8d35\u7684\u6570\u636e\u96c6\u4e14\u901a\u5e38\u5c40\u9650\u4e8e\u7279\u5b9a\u5939\u722a\u5f62\u6001\uff0c\u800c\u5206\u6790\u65b9\u6cd5\u56e0\u7b80\u5316\u5047\u8bbe\u5e38\u4ea7\u751f\u7269\u7406\u4e0d\u53ef\u884c\u7684\u6293\u53d6\uff0c\u9700\u8981\u9ad8\u4fdd\u771f\u4eff\u771f\u8fc7\u6ee4\uff0c\u8fd9\u51cf\u5c11\u4e86\u6293\u53d6\u6570\u91cf\u548c\u591a\u6837\u6027", "method": "\u63d0\u51fa\u53ef\u6269\u5c55\u7684\u751f\u6210-\u7cbe\u70bc\u6d41\u7a0b\uff1a1) \u7528\u5206\u6790\u65b9\u6cd5\u751f\u6210\u79cd\u5b50\u6293\u53d6\u96c6\uff1b2) \u5728Isaac Sim\u9ad8\u4fdd\u771f\u4eff\u771f\u4e2d\u4f7f\u7528\u5f02\u6b65\u65e0\u68af\u5ea6\u8fdb\u5316\u7b97\u6cd5\u4f18\u5316\u6293\u53d6\u8d28\u91cf\uff1b3) \u5c06\u7cbe\u70bc\u540e\u7684\u6293\u53d6\u5206\u5e03\u84b8\u998f\u5230\u6269\u6563\u6a21\u578b\u4e2d\u7528\u4e8e\u5b9e\u9645\u90e8\u7f72", "result": "\u5728Handles\u6570\u636e\u96c6\u548cDexGraspNet\u5b50\u96c6\u4e0a\uff0c\u6bcf\u4e2a\u7269\u4f53\u83b7\u5f97\u8d85\u8fc7120\u4e2a\u4e0d\u540c\u7684\u7a33\u5b9a\u6293\u53d6\uff08\u6bd4\u672a\u7cbe\u70bc\u5206\u6790\u65b9\u6cd5\u63d0\u53471.7-6\u500d\uff09\uff0c\u5728\u72ec\u7279\u6293\u53d6\u8986\u76d6\u7387\u4e0a\u6bd4\u57fa\u4e8e\u6269\u6563\u7684\u65b9\u6cd5\u63d0\u534746-60%", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u5c06\u9ad8\u4fdd\u771f\u4eff\u771f\u4f5c\u4e3a\u4f18\u5316\u9636\u6bb5\u800c\u975e\u4ec5\u7528\u4e8e\u9a8c\u8bc1\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6293\u53d6\u6570\u91cf\u3001\u591a\u6837\u6027\u548c\u7269\u7406\u53ef\u884c\u6027\uff0c\u540c\u65f6\u5f3a\u8c03\u4e86\u591a\u6837\u6027\u5728\u8bad\u7ec3\u548c\u90e8\u7f72\u4e2d\u7684\u91cd\u8981\u6027"}}
{"id": "2602.15309", "categories": ["cs.RO", "physics.med-ph"], "pdf": "https://arxiv.org/pdf/2602.15309", "abs": "https://arxiv.org/abs/2602.15309", "authors": ["Mostafa A. Atalla", "Anand S. Sekar", "Remi van Starkenburg", "David J. Jager", "Aim\u00e9e Sakes", "Micha\u00ebl Wiertlewski", "Paul Breedveld"], "title": "OSCAR: An Ovipositor-Inspired Self-Propelling Capsule Robot for Colonoscopy", "comment": null, "summary": "Self-propelling robotic capsules eliminate shaft looping of conventional colonoscopy, reducing patient discomfort. However, reliably moving within the slippery, viscoelastic environment of the colon remains a significant challenge. We present OSCAR, an ovipositor-inspired self-propelling capsule robot that translates the transport strategy of parasitic wasps into a propulsion mechanism for colonoscopy. OSCAR mechanically encodes the ovipositor-inspired motion pattern through a spring-loaded cam system that drives twelve circumferential sliders in a coordinated, phase-shifted sequence. By tuning the motion profile to maximize the retract phase relative to the advance phase, the capsule creates a controlled friction anisotropy at the interface that generates net forward thrust. We developed an analytical model incorporating a Kelvin-Voigt formulation to capture the viscoelastic stick--slip interactions between the sliders and the tissue, linking the asymmetry between advance and retract phase durations to mean thrust, and slider-reversal synchronization to thrust stability. Comprehensive force characterization experiments in ex-vivo porcine colon revealed a mean steady-state traction force of 0.85 N, closely matching the model. Furthermore, experiments confirmed that thrust generation is speed-independent and scales linearly with the phase asymmetry, in agreement with theoretical predictions, underscoring the capsule's predictable performance and scalability. In locomotion validation experiments, OSCAR demonstrated robust performance, achieving an average speed of 3.08 mm/s, a velocity sufficient to match the cecal intubation times of conventional colonoscopy. By coupling phase-encoded friction anisotropy with a predictive model, OSCAR delivers controllable thrust generation at low normal loads, enabling safer and more robust self-propelling locomotion for robotic capsule colonoscopy.", "AI": {"tldr": "OSCAR\u662f\u4e00\u79cd\u53d7\u5bc4\u751f\u8702\u4ea7\u5375\u5668\u542f\u53d1\u7684\u81ea\u63a8\u8fdb\u80f6\u56ca\u673a\u5668\u4eba\uff0c\u901a\u8fc7\u76f8\u4f4d\u7f16\u7801\u7684\u6469\u64e6\u5404\u5411\u5f02\u6027\u5728\u7ed3\u80a0\u4e2d\u4ea7\u751f\u53ef\u63a7\u63a8\u529b\uff0c\u5b9e\u73b0\u65e0\u8f74\u7ed3\u80a0\u955c\u68c0\u67e5\u3002", "motivation": "\u4f20\u7edf\u7ed3\u80a0\u955c\u68c0\u67e5\u5b58\u5728\u8f74\u7ba1\u6253\u7ed3\u95ee\u9898\uff0c\u5bfc\u81f4\u60a3\u8005\u4e0d\u9002\u3002\u5728\u7ed3\u80a0\u7684\u6e7f\u6ed1\u3001\u7c98\u5f39\u6027\u73af\u5883\u4e2d\u53ef\u9760\u79fb\u52a8\u662f\u4e00\u4e2a\u91cd\u5927\u6311\u6218\u3002\u9700\u8981\u5f00\u53d1\u4e00\u79cd\u80fd\u591f\u5728\u4f4e\u6cd5\u5411\u8f7d\u8377\u4e0b\u4ea7\u751f\u53ef\u63a7\u63a8\u529b\u7684\u81ea\u63a8\u8fdb\u80f6\u56ca\u673a\u5668\u4eba\u3002", "method": "OSCAR\u91c7\u7528\u53d7\u5bc4\u751f\u8702\u4ea7\u5375\u5668\u542f\u53d1\u7684\u8fd0\u52a8\u6a21\u5f0f\uff0c\u901a\u8fc7\u5f39\u7c27\u52a0\u8f7d\u7684\u51f8\u8f6e\u7cfb\u7edf\u9a71\u52a812\u4e2a\u5468\u5411\u6ed1\u5757\u8fdb\u884c\u534f\u8c03\u3001\u76f8\u4f4d\u504f\u79fb\u7684\u5e8f\u5217\u8fd0\u52a8\u3002\u901a\u8fc7\u8c03\u6574\u8fd0\u52a8\u8f6e\u5ed3\u4ee5\u6700\u5927\u5316\u56de\u7f29\u9636\u6bb5\u76f8\u5bf9\u4e8e\u524d\u8fdb\u9636\u6bb5\u7684\u6301\u7eed\u65f6\u95f4\uff0c\u5728\u754c\u9762\u5904\u521b\u5efa\u53d7\u63a7\u7684\u6469\u64e6\u5404\u5411\u5f02\u6027\uff0c\u4ece\u800c\u4ea7\u751f\u51c0\u5411\u524d\u63a8\u529b\u3002\u5f00\u53d1\u4e86\u5305\u542bKelvin-Voigt\u516c\u5f0f\u7684\u89e3\u6790\u6a21\u578b\u6765\u6355\u6349\u6ed1\u5757\u4e0e\u7ec4\u7ec7\u4e4b\u95f4\u7684\u7c98\u5f39\u6027\u7c98\u6ed1\u76f8\u4e92\u4f5c\u7528\u3002", "result": "\u5728\u79bb\u4f53\u732a\u7ed3\u80a0\u4e2d\u7684\u7efc\u5408\u529b\u8868\u5f81\u5b9e\u9a8c\u663e\u793a\u5e73\u5747\u7a33\u6001\u7275\u5f15\u529b\u4e3a0.85N\uff0c\u4e0e\u6a21\u578b\u9884\u6d4b\u76f8\u7b26\u3002\u63a8\u529b\u751f\u6210\u4e0e\u901f\u5ea6\u65e0\u5173\uff0c\u5e76\u968f\u76f8\u4f4d\u4e0d\u5bf9\u79f0\u6027\u7ebf\u6027\u7f29\u653e\u3002\u5728\u8fd0\u52a8\u9a8c\u8bc1\u5b9e\u9a8c\u4e2d\uff0cOSCAR\u5b9e\u73b0\u4e86\u5e73\u5747\u901f\u5ea63.08mm/s\uff0c\u8db3\u4ee5\u5339\u914d\u4f20\u7edf\u7ed3\u80a0\u955c\u7684\u76f2\u80a0\u63d2\u7ba1\u65f6\u95f4\u3002", "conclusion": "\u901a\u8fc7\u5c06\u76f8\u4f4d\u7f16\u7801\u7684\u6469\u64e6\u5404\u5411\u5f02\u6027\u4e0e\u9884\u6d4b\u6a21\u578b\u76f8\u7ed3\u5408\uff0cOSCAR\u80fd\u591f\u5728\u4f4e\u6cd5\u5411\u8f7d\u8377\u4e0b\u5b9e\u73b0\u53ef\u63a7\u7684\u63a8\u529b\u751f\u6210\uff0c\u4e3a\u673a\u5668\u4eba\u80f6\u56ca\u7ed3\u80a0\u955c\u68c0\u67e5\u63d0\u4f9b\u66f4\u5b89\u5168\u3001\u66f4\u7a33\u5065\u7684\u81ea\u63a8\u8fdb\u8fd0\u52a8\u3002"}}
{"id": "2602.15351", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.15351", "abs": "https://arxiv.org/abs/2602.15351", "authors": ["Kei Takahashi", "Hikaru Sasaki", "Takamitsu Matsubara"], "title": "Feasibility-aware Imitation Learning from Observation with Multimodal Feedback", "comment": null, "summary": "Imitation learning frameworks that learn robot control policies from demonstrators' motions via hand-mounted demonstration interfaces have attracted increasing attention. However, due to differences in physical characteristics between demonstrators and robots, this approach faces two limitations: i) the demonstration data do not include robot actions, and ii) the demonstrated motions may be infeasible for robots. These limitations make policy learning difficult. To address them, we propose Feasibility-Aware Behavior Cloning from Observation (FABCO). FABCO integrates behavior cloning from observation, which complements robot actions using robot dynamics models, with feasibility estimation. In feasibility estimation, the demonstrated motions are evaluated using a robot-dynamics model, learned from the robot's execution data, to assess reproducibility under the robot's dynamics. The estimated feasibility is used for multimodal feedback and feasibility-aware policy learning to improve the demonstrator's motions and learn robust policies. Multimodal feedback provides feasibility through the demonstrator's visual and haptic senses to promote feasible demonstrated motions. Feasibility-aware policy learning reduces the influence of demonstrated motions that are infeasible for robots, enabling the learning of policies that robots can execute stably. We conducted experiments with 15 participants on two tasks and confirmed that FABCO improves imitation learning performance by more than 3.2 times compared to the case without feasibility feedback.", "AI": {"tldr": "FABCO\u6846\u67b6\u901a\u8fc7\u53ef\u884c\u6027\u4f30\u8ba1\u548c\u591a\u6a21\u6001\u53cd\u9988\uff0c\u89e3\u51b3\u6a21\u4eff\u5b66\u4e60\u4e2d\u6f14\u793a\u8005\u4e0e\u673a\u5668\u4eba\u7269\u7406\u7279\u6027\u5dee\u5f02\u5bfc\u81f4\u7684\u6570\u636e\u7f3a\u5931\u548c\u52a8\u4f5c\u4e0d\u53ef\u884c\u95ee\u9898\uff0c\u63d0\u5347\u6a21\u4eff\u5b66\u4e60\u6027\u80fd3.2\u500d\u4ee5\u4e0a\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u624b\u90e8\u6f14\u793a\u63a5\u53e3\u7684\u6a21\u4eff\u5b66\u4e60\u9762\u4e34\u4e24\u4e2a\u4e3b\u8981\u95ee\u9898\uff1a1\uff09\u6f14\u793a\u6570\u636e\u4e0d\u5305\u542b\u673a\u5668\u4eba\u52a8\u4f5c\uff1b2\uff09\u6f14\u793a\u52a8\u4f5c\u53ef\u80fd\u5bf9\u673a\u5668\u4eba\u4e0d\u53ef\u884c\u3002\u8fd9\u4e9b\u9650\u5236\u4f7f\u5f97\u7b56\u7565\u5b66\u4e60\u53d8\u5f97\u56f0\u96be\u3002", "method": "\u63d0\u51fa\u53ef\u884c\u6027\u611f\u77e5\u884c\u4e3a\u514b\u9686\u89c2\u5bdf\uff08FABCO\uff09\u6846\u67b6\uff0c\u6574\u5408\u884c\u4e3a\u514b\u9686\u89c2\u5bdf\uff08\u4f7f\u7528\u673a\u5668\u4eba\u52a8\u529b\u5b66\u6a21\u578b\u8865\u5145\u673a\u5668\u4eba\u52a8\u4f5c\uff09\u548c\u53ef\u884c\u6027\u4f30\u8ba1\u3002\u53ef\u884c\u6027\u4f30\u8ba1\u901a\u8fc7\u4ece\u673a\u5668\u4eba\u6267\u884c\u6570\u636e\u5b66\u4e60\u7684\u52a8\u529b\u5b66\u6a21\u578b\u8bc4\u4f30\u6f14\u793a\u52a8\u4f5c\u7684\u53ef\u91cd\u73b0\u6027\u3002\u4f7f\u7528\u591a\u6a21\u6001\u53cd\u9988\uff08\u89c6\u89c9\u548c\u89e6\u89c9\uff09\u4fc3\u8fdb\u53ef\u884c\u6f14\u793a\u52a8\u4f5c\uff0c\u5e76\u901a\u8fc7\u53ef\u884c\u6027\u611f\u77e5\u7b56\u7565\u5b66\u4e60\u51cf\u5c11\u4e0d\u53ef\u884c\u6f14\u793a\u52a8\u4f5c\u7684\u5f71\u54cd\u3002", "result": "\u572815\u540d\u53c2\u4e0e\u8005\u7684\u4e24\u4e2a\u4efb\u52a1\u5b9e\u9a8c\u4e2d\uff0cFABCO\u76f8\u6bd4\u65e0\u53ef\u884c\u6027\u53cd\u9988\u7684\u60c5\u51b5\uff0c\u5c06\u6a21\u4eff\u5b66\u4e60\u6027\u80fd\u63d0\u5347\u4e863.2\u500d\u4ee5\u4e0a\u3002", "conclusion": "FABCO\u901a\u8fc7\u6574\u5408\u53ef\u884c\u6027\u4f30\u8ba1\u548c\u591a\u6a21\u6001\u53cd\u9988\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u6f14\u793a\u8005\u4e0e\u673a\u5668\u4eba\u7269\u7406\u7279\u6027\u5dee\u5f02\u5e26\u6765\u7684\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u4eff\u5b66\u4e60\u7684\u6027\u80fd\u548c\u7a33\u5b9a\u6027\u3002"}}
{"id": "2602.15354", "categories": ["cs.RO", "eess.SY"], "pdf": "https://arxiv.org/pdf/2602.15354", "abs": "https://arxiv.org/abs/2602.15354", "authors": ["Jose Luis Peralta-Cabezas", "Miguel Torres-Torriti", "Marcelo Guarini-Hermann"], "title": "A Comparison of Bayesian Prediction Techniques for Mobile Robot Trajectory Tracking", "comment": "Accepted in Robotica (Dec. 2007), vol. 26, n. 5, pp. 571-585 (c) 2008 Cambridge University Press. https://doi.org/10.1017/S0263574708004153", "summary": "This paper presents a performance comparison of different estimation and prediction techniques applied to the problem of tracking multiple robots. The main performance criteria are the magnitude of the estimation or prediction error, the computational effort and the robustness of each method to non-Gaussian noise. Among the different techniques compared are the well known Kalman filters and their different variants (e.g. extended and unscented), and the more recent techniques relying on Sequential Monte Carlo Sampling methods, such as particle filters and Gaussian Mixture Sigma Point Particle Filter.", "AI": {"tldr": "\u672c\u6587\u6bd4\u8f83\u4e86\u591a\u79cd\u4f30\u8ba1\u548c\u9884\u6d4b\u6280\u672f\u5728\u8ddf\u8e2a\u591a\u4e2a\u673a\u5668\u4eba\u95ee\u9898\u4e0a\u7684\u6027\u80fd\uff0c\u4e3b\u8981\u8bc4\u4f30\u6807\u51c6\u5305\u62ec\u8bef\u5dee\u5927\u5c0f\u3001\u8ba1\u7b97\u91cf\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u7814\u7a76\u4e0d\u540c\u4f30\u8ba1\u548c\u9884\u6d4b\u6280\u672f\u5728\u591a\u673a\u5668\u4eba\u8ddf\u8e2a\u95ee\u9898\u4e2d\u7684\u6027\u80fd\u8868\u73b0\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u6280\u672f\u9009\u62e9\u4f9d\u636e\u3002", "method": "\u6bd4\u8f83\u4e86\u5361\u5c14\u66fc\u6ee4\u6ce2\u5668\u53ca\u5176\u53d8\u4f53\uff08\u6269\u5c55\u5361\u5c14\u66fc\u6ee4\u6ce2\u3001\u65e0\u8ff9\u5361\u5c14\u66fc\u6ee4\u6ce2\uff09\u4e0e\u57fa\u4e8e\u5e8f\u5217\u8499\u7279\u5361\u6d1b\u91c7\u6837\u7684\u65b9\u6cd5\uff08\u7c92\u5b50\u6ee4\u6ce2\u3001\u9ad8\u65af\u6df7\u5408\u897f\u683c\u739b\u70b9\u7c92\u5b50\u6ee4\u6ce2\uff09\u3002", "result": "\u5bf9\u5404\u79cd\u65b9\u6cd5\u5728\u4f30\u8ba1\u8bef\u5dee\u3001\u8ba1\u7b97\u590d\u6742\u5ea6\u548c\u975e\u9ad8\u65af\u566a\u58f0\u9c81\u68d2\u6027\u65b9\u9762\u8fdb\u884c\u4e86\u7cfb\u7edf\u6bd4\u8f83\u3002", "conclusion": "\u4e3a\u591a\u673a\u5668\u4eba\u8ddf\u8e2a\u95ee\u9898\u63d0\u4f9b\u4e86\u4e0d\u540c\u4f30\u8ba1\u548c\u9884\u6d4b\u6280\u672f\u7684\u6027\u80fd\u8bc4\u4f30\uff0c\u5e2e\u52a9\u7528\u6237\u6839\u636e\u5177\u4f53\u9700\u6c42\u9009\u62e9\u5408\u9002\u7684\u65b9\u6cd5\u3002"}}
{"id": "2602.15357", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.15357", "abs": "https://arxiv.org/abs/2602.15357", "authors": ["Xinhao Chen", "Hongkun Yao", "Anuruddha Bhattacharjee", "Suraj Raval", "Lamar O. Mair", "Yancy Diaz-Mercado", "Axel Krieger"], "title": "Fluoroscopy-Constrained Magnetic Robot Control via Zernike-Based Field Modeling and Nonlinear MPC", "comment": null, "summary": "Magnetic actuation enables surgical robots to navigate complex anatomical pathways while reducing tissue trauma and improving surgical precision. However, clinical deployment is limited by the challenges of controlling such systems under fluoroscopic imaging, which provides low frame rate and noisy pose feedback. This paper presents a control framework that remains accurate and stable under such conditions by combining a nonlinear model predictive control (NMPC) framework that directly outputs coil currents, an analytically differentiable magnetic field model based on Zernike polynomials, and a Kalman filter to estimate the robot state. Experimental validation is conducted with two magnetic robots in a 3D-printed fluid workspace and a spine phantom replicating drug delivery in the epidural space. Results show the proposed control method remains highly accurate when feedback is downsampled to 3 Hz with added Gaussian noise (sigma = 2 mm), mimicking clinical fluoroscopy. In the spine phantom experiments, the proposed method successfully executed a drug delivery trajectory with a root mean square (RMS) position error of 1.18 mm while maintaining safe clearance from critical anatomical boundaries.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u78c1\u9a71\u52a8\u624b\u672f\u673a\u5668\u4eba\u7684\u63a7\u5236\u6846\u67b6\uff0c\u80fd\u591f\u5728\u4f4e\u5e27\u7387\u3001\u566a\u58f0\u5927\u7684\u8367\u5149\u6210\u50cf\u6761\u4ef6\u4e0b\u4fdd\u6301\u7cbe\u786e\u7a33\u5b9a\u63a7\u5236\uff0c\u901a\u8fc7NMPC\u3001\u53ef\u5fae\u78c1\u573a\u6a21\u578b\u548c\u5361\u5c14\u66fc\u6ee4\u6ce2\u5668\u7684\u7ec4\u5408\u5b9e\u73b0\u3002", "motivation": "\u78c1\u9a71\u52a8\u624b\u672f\u673a\u5668\u4eba\u80fd\u591f\u5bfc\u822a\u590d\u6742\u7684\u89e3\u5256\u8def\u5f84\uff0c\u51cf\u5c11\u7ec4\u7ec7\u521b\u4f24\u5e76\u63d0\u9ad8\u624b\u672f\u7cbe\u5ea6\uff0c\u4f46\u5176\u4e34\u5e8a\u90e8\u7f72\u53d7\u5230\u8367\u5149\u6210\u50cf\u6761\u4ef6\u4e0b\u63a7\u5236\u7684\u9650\u5236\uff0c\u56e0\u4e3a\u8367\u5149\u6210\u50cf\u63d0\u4f9b\u4f4e\u5e27\u7387\u548c\u566a\u58f0\u5927\u7684\u59ff\u6001\u53cd\u9988\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u63a7\u5236\u6846\u67b6\uff0c\u7ed3\u5408\u4e86\u76f4\u63a5\u8f93\u51fa\u7ebf\u5708\u7535\u6d41\u7684\u975e\u7ebf\u6027\u6a21\u578b\u9884\u6d4b\u63a7\u5236(NMPC)\u3001\u57fa\u4e8eZernike\u591a\u9879\u5f0f\u7684\u89e3\u6790\u53ef\u5fae\u78c1\u573a\u6a21\u578b\uff0c\u4ee5\u53ca\u7528\u4e8e\u4f30\u8ba1\u673a\u5668\u4eba\u72b6\u6001\u7684\u5361\u5c14\u66fc\u6ee4\u6ce2\u5668\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u8868\u660e\uff0c\u5f53\u53cd\u9988\u964d\u91c7\u6837\u81f33Hz\u5e76\u6dfb\u52a0\u9ad8\u65af\u566a\u58f0(\u03c3=2mm)\u4ee5\u6a21\u62df\u4e34\u5e8a\u8367\u5149\u6210\u50cf\u65f6\uff0c\u8be5\u65b9\u6cd5\u4ecd\u4fdd\u6301\u9ad8\u7cbe\u5ea6\u3002\u5728\u810a\u67f1\u6a21\u578b\u5b9e\u9a8c\u4e2d\uff0c\u6210\u529f\u6267\u884c\u4e86\u836f\u7269\u8f93\u9001\u8f68\u8ff9\uff0c\u5747\u65b9\u6839\u4f4d\u7f6e\u8bef\u5dee\u4e3a1.18mm\uff0c\u540c\u65f6\u4fdd\u6301\u4e0e\u5173\u952e\u89e3\u5256\u8fb9\u754c\u7684\u5b89\u5168\u8ddd\u79bb\u3002", "conclusion": "\u8be5\u63a7\u5236\u6846\u67b6\u80fd\u591f\u5728\u4e34\u5e8a\u8367\u5149\u6210\u50cf\u7684\u6311\u6218\u6027\u6761\u4ef6\u4e0b\u5b9e\u73b0\u78c1\u9a71\u52a8\u624b\u672f\u673a\u5668\u4eba\u7684\u7cbe\u786e\u7a33\u5b9a\u63a7\u5236\uff0c\u4e3a\u78c1\u9a71\u52a8\u624b\u672f\u673a\u5668\u4eba\u7684\u4e34\u5e8a\u90e8\u7f72\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.15397", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.15397", "abs": "https://arxiv.org/abs/2602.15397", "authors": ["Zibin Dong", "Yicheng Liu", "Shiduo Zhang", "Baijun Ye", "Yifu Yuan", "Fei Ni", "Jingjing Gong", "Xipeng Qiu", "Hang Zhao", "Yinchuan Li", "Jianye Hao"], "title": "ActionCodec: What Makes for Good Action Tokenizers", "comment": null, "summary": "Vision-Language-Action (VLA) models leveraging the native autoregressive paradigm of Vision-Language Models (VLMs) have demonstrated superior instruction-following and training efficiency. Central to this paradigm is action tokenization, yet its design has primarily focused on reconstruction fidelity, failing to address its direct impact on VLA optimization. Consequently, the fundamental question of \\textit{what makes for good action tokenizers} remains unanswered. In this paper, we bridge this gap by establishing design principles specifically from the perspective of VLA optimization. We identify a set of best practices based on information-theoretic insights, including maximized temporal token overlap, minimized vocabulary redundancy, enhanced multimodal mutual information, and token independence. Guided by these principles, we introduce \\textbf{ActionCodec}, a high-performance action tokenizer that significantly enhances both training efficiency and VLA performance across diverse simulation and real-world benchmarks. Notably, on LIBERO, a SmolVLM2-2.2B fine-tuned with ActionCodec achieves a 95.5\\% success rate without any robotics pre-training. With advanced architectural enhancements, this reaches 97.4\\%, representing a new SOTA for VLA models without robotics pre-training. We believe our established design principles, alongside the released model, will provide a clear roadmap for the community to develop more effective action tokenizers.", "AI": {"tldr": "\u672c\u6587\u9488\u5bf9VLA\u6a21\u578b\u4e2d\u7684\u52a8\u4f5c\u5206\u8bcd\u5668\u8bbe\u8ba1\u95ee\u9898\uff0c\u4ece\u4f18\u5316\u89d2\u5ea6\u63d0\u51fa\u4e86\u4fe1\u606f\u8bba\u6307\u5bfc\u7684\u8bbe\u8ba1\u539f\u5219\uff0c\u5e76\u5f00\u53d1\u4e86ActionCodec\u5206\u8bcd\u5668\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8bad\u7ec3\u6548\u7387\u548c\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u5f53\u524dVLA\u6a21\u578b\u4e2d\u7684\u52a8\u4f5c\u5206\u8bcd\u5668\u8bbe\u8ba1\u4e3b\u8981\u5173\u6ce8\u91cd\u5efa\u4fdd\u771f\u5ea6\uff0c\u5ffd\u89c6\u4e86\u5176\u5bf9VLA\u4f18\u5316\u7684\u76f4\u63a5\u5f71\u54cd\uff0c\u7f3a\u4e4f\u7cfb\u7edf\u6027\u7684\u8bbe\u8ba1\u539f\u5219\u6765\u56de\u7b54\"\u4ec0\u4e48\u6784\u6210\u597d\u7684\u52a8\u4f5c\u5206\u8bcd\u5668\"\u8fd9\u4e00\u6839\u672c\u95ee\u9898\u3002", "method": "\u57fa\u4e8e\u4fe1\u606f\u8bba\u6d1e\u5bdf\u5efa\u7acb\u52a8\u4f5c\u5206\u8bcd\u5668\u7684\u8bbe\u8ba1\u539f\u5219\uff0c\u5305\u62ec\u6700\u5927\u5316\u65f6\u95f4token\u91cd\u53e0\u3001\u6700\u5c0f\u5316\u8bcd\u6c47\u5197\u4f59\u3001\u589e\u5f3a\u591a\u6a21\u6001\u4e92\u4fe1\u606f\u548ctoken\u72ec\u7acb\u6027\uff0c\u5e76\u636e\u6b64\u5f00\u53d1\u4e86ActionCodec\u5206\u8bcd\u5668\u3002", "result": "ActionCodec\u663e\u8457\u63d0\u5347\u4e86\u8bad\u7ec3\u6548\u7387\u548cVLA\u6027\u80fd\uff0c\u5728LIBERO\u57fa\u51c6\u4e0a\uff0cSmolVLM2-2.2B\u4f7f\u7528ActionCodec\u5fae\u8c03\u8fbe\u523095.5%\u6210\u529f\u7387\uff1b\u7ed3\u5408\u67b6\u6784\u589e\u5f3a\u540e\u8fbe\u523097.4%\uff0c\u521b\u4e0b\u4e86\u65e0\u673a\u5668\u4eba\u9884\u8bad\u7ec3\u7684VLA\u6a21\u578b\u65b0SOTA\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u8bbe\u8ba1\u539f\u5219\u548cActionCodec\u4e3a\u793e\u533a\u5f00\u53d1\u66f4\u6709\u6548\u7684\u52a8\u4f5c\u5206\u8bcd\u5668\u63d0\u4f9b\u4e86\u6e05\u6670\u8def\u7ebf\u56fe\uff0c\u89e3\u51b3\u4e86\u52a8\u4f5c\u5206\u8bcd\u5668\u8bbe\u8ba1\u4e2d\u7684\u5173\u952e\u4f18\u5316\u95ee\u9898\u3002"}}
{"id": "2602.15398", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.15398", "abs": "https://arxiv.org/abs/2602.15398", "authors": ["Abdelrahman Metwally", "Monijesu James", "Aleksey Fedoseev", "Miguel Altamirano Cabrera", "Dzmitry Tsetserukou", "Andrey Somov"], "title": "Hybrid F' and ROS2 Architecture for Vision-Based Autonomous Flight: Design and Experimental Validation", "comment": "Paper accepted to ICIT 2026", "summary": "Autonomous aerospace systems require architectures that balance deterministic real-time control with advanced perception capabilities. This paper presents an integrated system combining NASA's F' flight software framework with ROS2 middleware via Protocol Buffers bridging. We evaluate the architecture through a 32.25-minute indoor quadrotor flight test using vision-based navigation. The vision system achieved 87.19 Hz position estimation with 99.90\\% data continuity and 11.47 ms mean latency, validating real-time performance requirements. All 15 ground commands executed successfully with 100 % success rate, demonstrating robust F'--PX4 integration. System resource utilization remained low (15.19 % CPU, 1,244 MB RAM) with zero stale telemetry messages, confirming efficient operation on embedded platforms. Results validate the feasibility of hybrid flight-software architectures combining certification-grade determinism with flexible autonomy for autonomous aerial vehicles.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408NASA F'\u98de\u884c\u8f6f\u4ef6\u6846\u67b6\u4e0eROS2\u4e2d\u95f4\u4ef6\u7684\u6df7\u5408\u67b6\u6784\uff0c\u901a\u8fc7\u5ba4\u5185\u56db\u65cb\u7ffc\u98de\u884c\u6d4b\u8bd5\u9a8c\u8bc1\u4e86\u8be5\u67b6\u6784\u5728\u5b9e\u65f6\u6027\u80fd\u3001\u6570\u636e\u8fde\u7eed\u6027\u548c\u7cfb\u7edf\u8d44\u6e90\u5229\u7528\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "motivation": "\u81ea\u4e3b\u822a\u7a7a\u822a\u5929\u7cfb\u7edf\u9700\u8981\u5e73\u8861\u786e\u5b9a\u6027\u5b9e\u65f6\u63a7\u5236\u4e0e\u5148\u8fdb\u611f\u77e5\u80fd\u529b\u7684\u67b6\u6784\u3002\u4f20\u7edf\u8ba4\u8bc1\u7ea7\u98de\u884c\u8f6f\u4ef6\u7f3a\u4e4f\u7075\u6d3b\u6027\uff0c\u800c\u73b0\u4ee3\u81ea\u4e3b\u7cfb\u7edf\u9700\u8981\u7ed3\u5408\u786e\u5b9a\u6027\u63a7\u5236\u4e0e\u7075\u6d3b\u7684\u611f\u77e5\u5904\u7406\u80fd\u529b\u3002", "method": "\u91c7\u7528NASA\u7684F'\u98de\u884c\u8f6f\u4ef6\u6846\u67b6\u4e0eROS2\u4e2d\u95f4\u4ef6\u901a\u8fc7Protocol Buffers\u6865\u63a5\u7684\u96c6\u6210\u7cfb\u7edf\u3002\u901a\u8fc732.25\u5206\u949f\u7684\u5ba4\u5185\u56db\u65cb\u7ffc\u98de\u884c\u6d4b\u8bd5\uff0c\u4f7f\u7528\u57fa\u4e8e\u89c6\u89c9\u7684\u5bfc\u822a\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u89c6\u89c9\u7cfb\u7edf\u8fbe\u523087.19 Hz\u4f4d\u7f6e\u4f30\u8ba1\u9891\u7387\uff0c99.90%\u6570\u636e\u8fde\u7eed\u6027\uff0c11.47 ms\u5e73\u5747\u5ef6\u8fdf\u3002\u6240\u670915\u4e2a\u5730\u9762\u547d\u4ee4\u6267\u884c\u6210\u529f\uff08100%\u6210\u529f\u7387\uff09\u3002\u7cfb\u7edf\u8d44\u6e90\u5229\u7528\u7387\u4f4e\uff0815.19% CPU\uff0c1244 MB RAM\uff09\uff0c\u65e0\u9648\u65e7\u9065\u6d4b\u6d88\u606f\u3002", "conclusion": "\u7ed3\u679c\u9a8c\u8bc1\u4e86\u6df7\u5408\u98de\u884c\u8f6f\u4ef6\u67b6\u6784\u7684\u53ef\u884c\u6027\uff0c\u80fd\u591f\u7ed3\u5408\u8ba4\u8bc1\u7ea7\u786e\u5b9a\u6027\u4e0e\u7075\u6d3b\u81ea\u4e3b\u6027\uff0c\u9002\u7528\u4e8e\u81ea\u4e3b\u98de\u884c\u5668\u7cfb\u7edf\u3002"}}
{"id": "2602.15400", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.15400", "abs": "https://arxiv.org/abs/2602.15400", "authors": ["Zerui Li", "Hongpei Zheng", "Fangguo Zhao", "Aidan Chan", "Jian Zhou", "Sihao Lin", "Shijie Li", "Qi Wu"], "title": "One Agent to Guide Them All: Empowering MLLMs for Vision-and-Language Navigation via Explicit World Representation", "comment": null, "summary": "A navigable agent needs to understand both high-level semantic instructions and precise spatial perceptions. Building navigation agents centered on Multimodal Large Language Models (MLLMs) demonstrates a promising solution due to their powerful generalization ability. However, the current tightly coupled design dramatically limits system performance. In this work, we propose a decoupled design that separates low-level spatial state estimation from high-level semantic planning. Unlike previous methods that rely on predefined, oversimplified textual maps, we introduce an interactive metric world representation that maintains rich and consistent information, allowing MLLMs to interact with and reason on it for decision-making. Furthermore, counterfactual reasoning is introduced to further elicit MLLMs' capacity, while the metric world representation ensures the physical validity of the produced actions. We conduct comprehensive experiments in both simulated and real-world environments. Our method establishes a new zero-shot state-of-the-art, achieving 48.8\\% Success Rate (SR) in R2R-CE and 42.2\\% in RxR-CE benchmarks. Furthermore, to validate the versatility of our metric representation, we demonstrate zero-shot sim-to-real transfer across diverse embodiments, including a wheeled TurtleBot 4 and a custom-built aerial drone. These real-world deployments verify that our decoupled framework serves as a robust, domain-invariant interface for embodied Vision-and-Language navigation.", "AI": {"tldr": "\u63d0\u51fa\u89e3\u8026\u5bfc\u822a\u6846\u67b6\uff0c\u5c06\u7a7a\u95f4\u72b6\u6001\u4f30\u8ba1\u4e0e\u8bed\u4e49\u89c4\u5212\u5206\u79bb\uff0c\u5f15\u5165\u4ea4\u4e92\u5f0f\u5ea6\u91cf\u4e16\u754c\u8868\u793a\uff0c\u5b9e\u73b0\u96f6\u6837\u672cSOTA\u6027\u80fd\u5e76\u9a8c\u8bc1\u8de8\u5e73\u53f0\u8fc1\u79fb\u80fd\u529b\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8eMLLM\u7684\u5bfc\u822a\u7cfb\u7edf\u91c7\u7528\u7d27\u8026\u5408\u8bbe\u8ba1\uff0c\u9650\u5236\u4e86\u7cfb\u7edf\u6027\u80fd\u3002\u9700\u8981\u5206\u79bb\u4f4e\u5c42\u7a7a\u95f4\u72b6\u6001\u4f30\u8ba1\u4e0e\u9ad8\u5c42\u8bed\u4e49\u89c4\u5212\uff0c\u540c\u65f6\u9700\u8981\u8d85\u8d8a\u7b80\u5316\u6587\u672c\u5730\u56fe\u7684\u4e30\u5bcc\u7a7a\u95f4\u8868\u793a\u3002", "method": "\u63d0\u51fa\u89e3\u8026\u8bbe\u8ba1\uff1a1) \u5206\u79bb\u7a7a\u95f4\u72b6\u6001\u4f30\u8ba1\u4e0e\u8bed\u4e49\u89c4\u5212\uff1b2) \u5f15\u5165\u4ea4\u4e92\u5f0f\u5ea6\u91cf\u4e16\u754c\u8868\u793a\uff0c\u4fdd\u6301\u4e30\u5bcc\u4e00\u81f4\u7684\u7a7a\u95f4\u4fe1\u606f\uff1b3) \u5f15\u5165\u53cd\u4e8b\u5b9e\u63a8\u7406\u6fc0\u53d1MLLM\u80fd\u529b\uff1b4) \u5ea6\u91cf\u8868\u793a\u786e\u4fdd\u52a8\u4f5c\u7684\u7269\u7406\u6709\u6548\u6027\u3002", "result": "\u5728R2R-CE\u548cRxR-CE\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5206\u522b\u8fbe\u523048.8%\u548c42.2%\u7684\u6210\u529f\u7387\uff0c\u5b9e\u73b0\u96f6\u6837\u672cSOTA\u3002\u5728\u771f\u5b9e\u4e16\u754c\u9a8c\u8bc1\u4e86\u8de8\u5e73\u53f0\u8fc1\u79fb\u80fd\u529b\uff0c\u5305\u62ec\u8f6e\u5f0f\u673a\u5668\u4eba\u548c\u65e0\u4eba\u673a\u3002", "conclusion": "\u89e3\u8026\u6846\u67b6\u7ed3\u5408\u4ea4\u4e92\u5f0f\u5ea6\u91cf\u4e16\u754c\u8868\u793a\uff0c\u4e3a\u5177\u8eab\u89c6\u89c9\u8bed\u8a00\u5bfc\u822a\u63d0\u4f9b\u4e86\u9c81\u68d2\u3001\u9886\u57df\u4e0d\u53d8\u7684\u63a5\u53e3\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5bfc\u822a\u6027\u80fd\u5e76\u652f\u6301\u8de8\u5e73\u53f0\u90e8\u7f72\u3002"}}
{"id": "2602.15424", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.15424", "abs": "https://arxiv.org/abs/2602.15424", "authors": ["Branimir \u0106aran", "Vladimir Mili\u0107", "Bojan Jerbi\u0107"], "title": "Lyapunov-Based $\\mathcal{L}_2$-Stable PI-Like Control of a Four-Wheel Independently Driven and Steered Robot", "comment": "SUBMITTED FOR POTENTIAL PUBLICATION IN IEEE ROBOTICS AND AUTOMATION LETTERS. PREPRINT VERSION", "summary": "In this letter, Lyapunov-based synthesis of a PI-like controller is proposed for $\\mathcal{L}_2$-stable motion control of an independently driven and steered four-wheel mobile robot. An explicit, structurally verified model is used to enable systematic controller design with stability and performance guarantees suitable for real-time operation. A Lyapunov function is constructed to yield explicit bounds and $\\mathcal{L}_2$ stability results, supporting feedback synthesis that reduces configuration dependent effects. The resulting control law maintains a PI-like form suitable for standard embedded implementation while preserving rigorous stability properties. Effectiveness and robustness are demonstrated experimentally on a real four-wheel mobile robot platform.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8eLyapunov\u7684PI\u7c7b\u63a7\u5236\u5668\u8bbe\u8ba1\u65b9\u6cd5\uff0c\u7528\u4e8e\u56db\u8f6e\u72ec\u7acb\u9a71\u52a8\u8f6c\u5411\u79fb\u52a8\u673a\u5668\u4eba\u7684L\u2082\u7a33\u5b9a\u8fd0\u52a8\u63a7\u5236", "motivation": "\u4e3a\u56db\u8f6e\u72ec\u7acb\u9a71\u52a8\u8f6c\u5411\u79fb\u52a8\u673a\u5668\u4eba\u5f00\u53d1\u5177\u6709\u4e25\u683c\u7a33\u5b9a\u6027\u4fdd\u8bc1\u7684\u8fd0\u52a8\u63a7\u5236\u5668\uff0c\u89e3\u51b3\u914d\u7f6e\u4f9d\u8d56\u6548\u5e94\u95ee\u9898\uff0c\u9002\u5408\u5b9e\u65f6\u5d4c\u5165\u5f0f\u5b9e\u73b0", "method": "\u57fa\u4e8e\u663e\u5f0f\u7ed3\u6784\u9a8c\u8bc1\u6a21\u578b\uff0c\u6784\u9020Lyapunov\u51fd\u6570\uff0c\u63a8\u5bfc\u663e\u5f0f\u8fb9\u754c\u548cL\u2082\u7a33\u5b9a\u6027\u7ed3\u679c\uff0c\u8bbe\u8ba1\u4fdd\u6301PI\u5f62\u5f0f\u7684\u63a7\u5236\u5f8b", "result": "\u63a7\u5236\u5668\u5728\u771f\u5b9e\u56db\u8f6e\u79fb\u52a8\u673a\u5668\u4eba\u5e73\u53f0\u4e0a\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6709\u6548\u6027\u548c\u9c81\u68d2\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u6807\u51c6\u5d4c\u5165\u5f0f\u5b9e\u73b0\u7684\u9002\u7528\u6027", "conclusion": "\u63d0\u51fa\u7684Lyapunov-based PI\u7c7b\u63a7\u5236\u5668\u6210\u529f\u5b9e\u73b0\u4e86\u56db\u8f6e\u79fb\u52a8\u673a\u5668\u4eba\u7684L\u2082\u7a33\u5b9a\u8fd0\u52a8\u63a7\u5236\uff0c\u517c\u5177\u7406\u8bba\u4e25\u683c\u6027\u548c\u5de5\u7a0b\u5b9e\u7528\u6027"}}
{"id": "2602.15543", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.15543", "abs": "https://arxiv.org/abs/2602.15543", "authors": ["Young-Chae Son", "Jung-Woo Lee", "Yoon-Ji Choi", "Dae-Kwan Ko", "Soo-Chul Lim"], "title": "Selective Perception for Robot: Task-Aware Attention in Multimodal VLA", "comment": null, "summary": "In robotics, Vision-Language-Action (VLA) models that integrate diverse multimodal signals from multi-view inputs have emerged as an effective approach. However, most prior work adopts static fusion that processes all visual inputs uniformly, which incurs unnecessary computational overhead and allows task-irrelevant background information to act as noise. Inspired by the principles of human active perception, we propose a dynamic information fusion framework designed to maximize the efficiency and robustness of VLA models. Our approach introduces a lightweight adaptive routing architecture that analyzes the current text prompt and observations from a wrist-mounted camera in real-time to predict the task-relevance of multiple camera views. By conditionally attenuating computations for views with low informational utility and selectively providing only essential visual features to the policy network, Our framework achieves computation efficiency proportional to task relevance. Furthermore, to efficiently secure large-scale annotation data for router training, we established an automated labeling pipeline utilizing Vision-Language Models (VLMs) to minimize data collection and annotation costs. Experimental results in real-world robotic manipulation scenarios demonstrate that the proposed approach achieves significant improvements in both inference efficiency and control performance compared to existing VLA models, validating the effectiveness and practicality of dynamic information fusion in resource-constrained, real-time robot control environments.", "AI": {"tldr": "\u63d0\u51fa\u52a8\u6001\u4fe1\u606f\u878d\u5408\u6846\u67b6\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u81ea\u9002\u5e94\u8def\u7531\u67b6\u6784\u5b9e\u65f6\u5206\u6790\u4efb\u52a1\u76f8\u5173\u6027\uff0c\u6761\u4ef6\u6027\u8870\u51cf\u4f4e\u4fe1\u606f\u6548\u7528\u89c6\u56fe\u7684\u8ba1\u7b97\uff0c\u5b9e\u73b0\u8ba1\u7b97\u6548\u7387\u4e0e\u4efb\u52a1\u76f8\u5173\u6027\u6210\u6b63\u6bd4", "motivation": "\u73b0\u6709VLA\u6a21\u578b\u91c7\u7528\u9759\u6001\u878d\u5408\u5904\u7406\u6240\u6709\u89c6\u89c9\u8f93\u5165\uff0c\u5bfc\u81f4\u4e0d\u5fc5\u8981\u7684\u8ba1\u7b97\u5f00\u9500\u548c\u4efb\u52a1\u65e0\u5173\u80cc\u666f\u4fe1\u606f\u4f5c\u4e3a\u566a\u58f0\uff0c\u9700\u8981\u63d0\u9ad8\u6548\u7387\u548c\u9c81\u68d2\u6027", "method": "1) \u8f7b\u91cf\u7ea7\u81ea\u9002\u5e94\u8def\u7531\u67b6\u6784\u5b9e\u65f6\u5206\u6790\u6587\u672c\u63d0\u793a\u548c\u8155\u90e8\u6444\u50cf\u5934\u89c2\u5bdf\uff0c\u9884\u6d4b\u591a\u6444\u50cf\u5934\u89c6\u56fe\u7684\u4efb\u52a1\u76f8\u5173\u6027\uff1b2) \u6761\u4ef6\u6027\u8870\u51cf\u4f4e\u4fe1\u606f\u6548\u7528\u89c6\u56fe\u7684\u8ba1\u7b97\uff1b3) \u5229\u7528VLM\u5efa\u7acb\u81ea\u52a8\u6807\u6ce8\u7ba1\u9053\u964d\u4f4e\u6570\u636e\u6536\u96c6\u548c\u6807\u6ce8\u6210\u672c", "result": "\u5728\u771f\u5b9e\u673a\u5668\u4eba\u64cd\u4f5c\u573a\u666f\u4e2d\uff0c\u76f8\u6bd4\u73b0\u6709VLA\u6a21\u578b\uff0c\u5728\u63a8\u7406\u6548\u7387\u548c\u63a7\u5236\u6027\u80fd\u65b9\u9762\u5747\u6709\u663e\u8457\u63d0\u5347", "conclusion": "\u52a8\u6001\u4fe1\u606f\u878d\u5408\u5728\u8d44\u6e90\u53d7\u9650\u7684\u5b9e\u65f6\u673a\u5668\u4eba\u63a7\u5236\u73af\u5883\u4e2d\u5177\u6709\u6709\u6548\u6027\u548c\u5b9e\u7528\u6027"}}
{"id": "2602.15549", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.15549", "abs": "https://arxiv.org/abs/2602.15549", "authors": ["Guoqin Tang", "Qingxuan Jia", "Gang Chen", "Tong Li", "Zeyuan Huang", "Zihang Lv", "Ning Ji"], "title": "VLM-DEWM: Dynamic External World Model for Verifiable and Resilient Vision-Language Planning in Manufacturing", "comment": null, "summary": "Vision-language model (VLM) shows promise for high-level planning in smart manufacturing, yet their deployment in dynamic workcells faces two critical challenges: (1) stateless operation, they cannot persistently track out-of-view states, causing world-state drift; and (2) opaque reasoning, failures are difficult to diagnose, leading to costly blind retries. This paper presents VLM-DEWM, a cognitive architecture that decouples VLM reasoning from world-state management through a persistent, queryable Dynamic External World Model (DEWM). Each VLM decision is structured into an Externalizable Reasoning Trace (ERT), comprising action proposal, world belief, and causal assumption, which is validated against DEWM before execution. When failures occur, discrepancy analysis between predicted and observed states enables targeted recovery instead of global replanning. We evaluate VLM-DEWM on multi-station assembly, large-scale facility exploration, and real-robot recovery under induced failures. Compared to baseline memory-augmented VLM systems, VLM DEWM improves state-tracking accuracy from 56% to 93%, increases recovery success rate from below 5% to 95%, and significantly reduces computational overhead through structured memory. These results establish VLM-DEWM as a verifiable and resilient solution for long-horizon robotic operations in dynamic manufacturing environments.", "AI": {"tldr": "VLM-DEWM\u63d0\u51fa\u4e86\u4e00\u79cd\u8ba4\u77e5\u67b6\u6784\uff0c\u901a\u8fc7\u6301\u4e45\u5316\u7684\u52a8\u6001\u5916\u90e8\u4e16\u754c\u6a21\u578b\u5c06\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u4e0e\u4e16\u754c\u72b6\u6001\u7ba1\u7406\u89e3\u8026\uff0c\u89e3\u51b3\u4e86\u667a\u80fd\u5236\u9020\u4e2dVLM\u7684\u72b6\u6001\u6f02\u79fb\u548c\u4e0d\u900f\u660e\u63a8\u7406\u95ee\u9898\u3002", "motivation": "\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u667a\u80fd\u5236\u9020\u9ad8\u5c42\u89c4\u5212\u4e2d\u8868\u73b0\u51fa\u6f5c\u529b\uff0c\u4f46\u5728\u52a8\u6001\u5de5\u4f5c\u5355\u5143\u90e8\u7f72\u9762\u4e34\u4e24\u4e2a\u5173\u952e\u6311\u6218\uff1a1) \u65e0\u72b6\u6001\u64cd\u4f5c\uff0c\u65e0\u6cd5\u6301\u7eed\u8ddf\u8e2a\u89c6\u91ce\u5916\u72b6\u6001\uff0c\u5bfc\u81f4\u4e16\u754c\u72b6\u6001\u6f02\u79fb\uff1b2) \u4e0d\u900f\u660e\u63a8\u7406\uff0c\u6545\u969c\u96be\u4ee5\u8bca\u65ad\uff0c\u5bfc\u81f4\u6602\u8d35\u7684\u76f2\u76ee\u91cd\u8bd5\u3002", "method": "\u63d0\u51faVLM-DEWM\u8ba4\u77e5\u67b6\u6784\uff0c\u901a\u8fc7\u6301\u4e45\u5316\u3001\u53ef\u67e5\u8be2\u7684\u52a8\u6001\u5916\u90e8\u4e16\u754c\u6a21\u578b(DEWM)\u5c06VLM\u63a8\u7406\u4e0e\u4e16\u754c\u72b6\u6001\u7ba1\u7406\u89e3\u8026\u3002\u6bcf\u4e2aVLM\u51b3\u7b56\u88ab\u7ed3\u6784\u5316\u4e3a\u5916\u90e8\u5316\u63a8\u7406\u8f68\u8ff9(ERT)\uff0c\u5305\u542b\u884c\u52a8\u63d0\u8bae\u3001\u4e16\u754c\u4fe1\u5ff5\u548c\u56e0\u679c\u5047\u8bbe\uff0c\u5728\u6267\u884c\u524d\u4e0eDEWM\u8fdb\u884c\u9a8c\u8bc1\u3002\u6545\u969c\u53d1\u751f\u65f6\uff0c\u901a\u8fc7\u9884\u6d4b\u72b6\u6001\u4e0e\u89c2\u6d4b\u72b6\u6001\u4e4b\u95f4\u7684\u5dee\u5f02\u5206\u6790\u5b9e\u73b0\u9488\u5bf9\u6027\u6062\u590d\u800c\u975e\u5168\u5c40\u91cd\u89c4\u5212\u3002", "result": "\u5728\u591a\u7ad9\u88c5\u914d\u3001\u5927\u89c4\u6a21\u8bbe\u65bd\u63a2\u7d22\u548c\u771f\u5b9e\u673a\u5668\u4eba\u6545\u969c\u6062\u590d\u5b9e\u9a8c\u4e2d\uff0c\u76f8\u6bd4\u57fa\u7ebf\u8bb0\u5fc6\u589e\u5f3aVLM\u7cfb\u7edf\uff0cVLM-DEWM\u5c06\u72b6\u6001\u8ddf\u8e2a\u51c6\u786e\u7387\u4ece56%\u63d0\u5347\u523093%\uff0c\u6062\u590d\u6210\u529f\u7387\u4ece\u4f4e\u4e8e5%\u63d0\u5347\u523095%\uff0c\u5e76\u901a\u8fc7\u7ed3\u6784\u5316\u5185\u5b58\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u5f00\u9500\u3002", "conclusion": "VLM-DEWM\u4e3a\u52a8\u6001\u5236\u9020\u73af\u5883\u4e2d\u7684\u957f\u65f6\u7a0b\u673a\u5668\u4eba\u64cd\u4f5c\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u9a8c\u8bc1\u4e14\u5177\u6709\u5f39\u6027\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u89e3\u8026\u63a8\u7406\u4e0e\u4e16\u754c\u72b6\u6001\u7ba1\u7406\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7cfb\u7edf\u7684\u53ef\u9760\u6027\u548c\u6548\u7387\u3002"}}
{"id": "2602.15567", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.15567", "abs": "https://arxiv.org/abs/2602.15567", "authors": ["Jieting Long", "Dechuan Liu", "Weidong Cai", "Ian Manchester", "Weiming Zhi"], "title": "Constraining Streaming Flow Models for Adapting Learned Robot Trajectory Distributions", "comment": "8 pages, 8 figure", "summary": "Robot motion distributions often exhibit multi-modality and require flexible generative models for accurate representation. Streaming Flow Policies (SFPs) have recently emerged as a powerful paradigm for generating robot trajectories by integrating learned velocity fields directly in action space, enabling smooth and reactive control. However, existing formulations lack mechanisms for adapting trajectories post-training to enforce safety and task-specific constraints. We propose Constraint-Aware Streaming Flow (CASF), a framework that augments streaming flow policies with constraint-dependent metrics that reshape the learned velocity field during execution. CASF models each constraint, defined in either the robot's workspace or configuration space, as a differentiable distance function that is converted into a local metric and pulled back into the robot's control space. Far from restricted regions, the resulting metric reduces to the identity; near constraint boundaries, it smoothly attenuates or redirects motion, effectively deforming the underlying flow to maintain safety. This allows trajectories to be adapted in real time, ensuring that robot actions respect joint limits, avoid collisions, and remain within feasible workspaces, while preserving the multi-modal and reactive properties of streaming flow policies. We demonstrate CASF in simulated and real-world manipulation tasks, showing that it produces constraint-satisfying trajectories that remain smooth, feasible, and dynamically consistent, outperforming standard post-hoc projection baselines.", "AI": {"tldr": "CASF\u6846\u67b6\u901a\u8fc7\u7ea6\u675f\u76f8\u5173\u5ea6\u91cf\u589e\u5f3a\u6d41\u5f0f\u6d41\u7b56\u7565\uff0c\u5728\u8fd0\u884c\u65f6\u91cd\u5851\u5b66\u4e60\u7684\u901f\u5ea6\u573a\uff0c\u5b9e\u73b0\u5b9e\u65f6\u8f68\u8ff9\u8c03\u6574\u4ee5\u6ee1\u8db3\u5b89\u5168\u7ea6\u675f\uff0c\u540c\u65f6\u4fdd\u6301\u591a\u6a21\u6001\u548c\u53cd\u5e94\u6027\u3002", "motivation": "\u73b0\u6709\u6d41\u5f0f\u6d41\u7b56\u7565\u7f3a\u4e4f\u8bad\u7ec3\u540e\u8c03\u6574\u8f68\u8ff9\u4ee5\u5f3a\u5236\u6267\u884c\u5b89\u5168\u548c\u4efb\u52a1\u7279\u5b9a\u7ea6\u675f\u7684\u673a\u5236\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u5b9e\u65f6\u9002\u5e94\u7ea6\u675f\u7684\u6846\u67b6\u3002", "method": "\u5c06\u6bcf\u4e2a\u7ea6\u675f\u5efa\u6a21\u4e3a\u53ef\u5fae\u8ddd\u79bb\u51fd\u6570\uff0c\u8f6c\u6362\u4e3a\u5c40\u90e8\u5ea6\u91cf\u5e76\u62c9\u56de\u5230\u673a\u5668\u4eba\u63a7\u5236\u7a7a\u95f4\uff0c\u5728\u8fdc\u79bb\u9650\u5236\u533a\u57df\u65f6\u5ea6\u91cf\u7b80\u5316\u4e3a\u5355\u4f4d\u77e9\u9635\uff0c\u5728\u7ea6\u675f\u8fb9\u754c\u9644\u8fd1\u5e73\u6ed1\u8870\u51cf\u6216\u91cd\u5b9a\u5411\u8fd0\u52a8\u3002", "result": "\u5728\u6a21\u62df\u548c\u771f\u5b9e\u4e16\u754c\u64cd\u4f5c\u4efb\u52a1\u4e2d\uff0cCASF\u751f\u6210\u6ee1\u8db3\u7ea6\u675f\u7684\u8f68\u8ff9\uff0c\u4fdd\u6301\u5e73\u6ed1\u3001\u53ef\u884c\u548c\u52a8\u6001\u4e00\u81f4\u6027\uff0c\u4f18\u4e8e\u6807\u51c6\u540e\u5904\u7406\u6295\u5f71\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "CASF\u6210\u529f\u589e\u5f3a\u4e86\u6d41\u5f0f\u6d41\u7b56\u7565\u7684\u7ea6\u675f\u611f\u77e5\u80fd\u529b\uff0c\u5b9e\u73b0\u4e86\u5b9e\u65f6\u8f68\u8ff9\u9002\u5e94\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u539f\u59cb\u7b56\u7565\u7684\u591a\u6a21\u6001\u548c\u53cd\u5e94\u7279\u6027\u3002"}}
{"id": "2602.15608", "categories": ["cs.RO", "physics.app-ph"], "pdf": "https://arxiv.org/pdf/2602.15608", "abs": "https://arxiv.org/abs/2602.15608", "authors": ["Mostafa A. Atalla", "Daan van Bemmel", "Jack Cummings", "Paul Breedveld", "Micha\u00ebl Wiertlewski", "Aim\u00e9e Sakes"], "title": "Grip as Needed, Glide on Demand: Ultrasonic Lubrication for Robotic Locomotion", "comment": "Accepted for publication in the 2026 IEEE International Conference on Robotics and Automation (ICRA) in Vienna", "summary": "Friction is the essential mediator of terrestrial locomotion, yet in robotic systems it is almost always treated as a passive property fixed by surface materials and conditions. Here, we introduce ultrasonic lubrication as a method to actively control friction in robotic locomotion. By exciting resonant structures at ultrasonic frequencies, contact interfaces can dynamically switch between \"grip\" and \"slip\" states, enabling locomotion. We developed two friction control modules, a cylindrical design for lumen-like environments and a flat-plate design for external surfaces, and integrated them into bio-inspired systems modeled after inchworm and wasp ovipositor locomotion. Both systems achieved bidirectional locomotion with nearly perfect locomotion efficiencies that exceeded 90%. Friction characterization experiments further demonstrated substantial friction reduction across various surfaces, including rigid, soft, granular, and biological tissue interfaces, under dry and wet conditions, and on surfaces with different levels of roughness, confirming the broad applicability of ultrasonic lubrication to locomotion tasks. These findings establish ultrasonic lubrication as a viable active friction control mechanism for robotic locomotion, with the potential to reduce design complexity and improve efficiency of robotic locomotion systems.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u8d85\u58f0\u6ce2\u6da6\u6ed1\u4e3b\u52a8\u63a7\u5236\u6469\u64e6\u529b\u7684\u673a\u5668\u4eba\u8fd0\u52a8\u65b9\u6cd5\uff0c\u901a\u8fc7\u6fc0\u53d1\u5171\u632f\u7ed3\u6784\u5728\u8d85\u58f0\u9891\u7387\u4e0b\u52a8\u6001\u5207\u6362\u63a5\u89e6\u754c\u9762\u7684\"\u6293\u63e1\"\u548c\"\u6ed1\u52a8\"\u72b6\u6001\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u53cc\u5411\u8fd0\u52a8\u3002", "motivation": "\u6469\u64e6\u529b\u662f\u9646\u5730\u8fd0\u52a8\u7684\u5173\u952e\u4e2d\u4ecb\uff0c\u4f46\u5728\u673a\u5668\u4eba\u7cfb\u7edf\u4e2d\u901a\u5e38\u88ab\u89c6\u4e3a\u7531\u8868\u9762\u6750\u6599\u548c\u6761\u4ef6\u51b3\u5b9a\u7684\u88ab\u52a8\u5c5e\u6027\u3002\u7814\u7a76\u8005\u5e0c\u671b\u5f00\u53d1\u4e00\u79cd\u80fd\u591f\u4e3b\u52a8\u63a7\u5236\u6469\u64e6\u529b\u7684\u65b9\u6cd5\uff0c\u4ee5\u63d0\u9ad8\u673a\u5668\u4eba\u8fd0\u52a8\u7cfb\u7edf\u7684\u6548\u7387\u548c\u7b80\u5316\u8bbe\u8ba1\u3002", "method": "\u91c7\u7528\u8d85\u58f0\u6ce2\u6da6\u6ed1\u6280\u672f\uff0c\u901a\u8fc7\u6fc0\u53d1\u5171\u632f\u7ed3\u6784\u5728\u8d85\u58f0\u9891\u7387\u4e0b\u52a8\u6001\u63a7\u5236\u63a5\u89e6\u754c\u9762\u72b6\u6001\u3002\u5f00\u53d1\u4e86\u4e24\u79cd\u6469\u64e6\u63a7\u5236\u6a21\u5757\uff1a\u5706\u67f1\u5f62\u8bbe\u8ba1\u7528\u4e8e\u7ba1\u8154\u73af\u5883\uff0c\u5e73\u677f\u8bbe\u8ba1\u7528\u4e8e\u5916\u90e8\u8868\u9762\u3002\u5c06\u8fd9\u4e9b\u6a21\u5757\u96c6\u6210\u5230\u4eff\u751f\u7cfb\u7edf\u4e2d\uff0c\u6a21\u62df\u5c3a\u8816\u548c\u9ec4\u8702\u4ea7\u5375\u5668\u7684\u8fd0\u52a8\u673a\u5236\u3002", "result": "\u4e24\u79cd\u7cfb\u7edf\u90fd\u5b9e\u73b0\u4e86\u53cc\u5411\u8fd0\u52a8\uff0c\u8fd0\u52a8\u6548\u7387\u8d85\u8fc790%\uff0c\u63a5\u8fd1\u5b8c\u7f8e\u3002\u6469\u64e6\u7279\u6027\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u5728\u5404\u79cd\u8868\u9762\uff08\u521a\u6027\u3001\u67d4\u8f6f\u3001\u9897\u7c92\u72b6\u3001\u751f\u7269\u7ec4\u7ec7\uff09\u3001\u5e72\u6e7f\u6761\u4ef6\u4ee5\u53ca\u4e0d\u540c\u7c97\u7cd9\u5ea6\u8868\u9762\u4e0a\u663e\u8457\u964d\u4f4e\u6469\u64e6\u529b\uff0c\u8bc1\u5b9e\u4e86\u8d85\u58f0\u6ce2\u6da6\u6ed1\u5728\u8fd0\u52a8\u4efb\u52a1\u4e2d\u7684\u5e7f\u6cdb\u9002\u7528\u6027\u3002", "conclusion": "\u8d85\u58f0\u6ce2\u6da6\u6ed1\u88ab\u786e\u7acb\u4e3a\u4e00\u79cd\u53ef\u884c\u7684\u673a\u5668\u4eba\u8fd0\u52a8\u4e3b\u52a8\u6469\u64e6\u63a7\u5236\u673a\u5236\uff0c\u5177\u6709\u964d\u4f4e\u8bbe\u8ba1\u590d\u6742\u6027\u548c\u63d0\u9ad8\u673a\u5668\u4eba\u8fd0\u52a8\u7cfb\u7edf\u6548\u7387\u7684\u6f5c\u529b\u3002"}}
{"id": "2602.15633", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.15633", "abs": "https://arxiv.org/abs/2602.15633", "authors": ["Haichao Liu", "Yufeng Hu", "Shuang Wang", "Kangjun Guo", "Jun Ma", "Jinni Zhou"], "title": "SpecFuse: A Spectral-Temporal Fusion Predictive Control Framework for UAV Landing on Oscillating Marine Platforms", "comment": "8 pages, 5 figures, 4 tables", "summary": "Autonomous landing of Uncrewed Aerial Vehicles (UAVs) on oscillating marine platforms is severely constrained by wave-induced multi-frequency oscillations, wind disturbances, and prediction phase lags in motion prediction. Existing methods either treat platform motion as a general random process or lack explicit modeling of wave spectral characteristics, leading to suboptimal performance under dynamic sea conditions. To address these limitations, we propose SpecFuse: a novel spectral-temporal fusion predictive control framework that integrates frequency-domain wave decomposition with time-domain recursive state estimation for high-precision 6-DoF motion forecasting of Uncrewed Surface Vehicles (USVs). The framework explicitly models dominant wave harmonics to mitigate phase lags, refining predictions in real time via IMU data without relying on complex calibration. Additionally, we design a hierarchical control architecture featuring a sampling-based HPO-RRT* algorithm for dynamic trajectory planning under non-convex constraints and a learning-augmented predictive controller that fuses data-driven disturbance compensation with optimization-based execution. Extensive validations (2,000 simulations + 8 lake experiments) show our approach achieves a 3.2 cm prediction error, 4.46 cm landing deviation, 98.7% / 87.5% success rates (simulation / real-world), and 82 ms latency on embedded hardware, outperforming state-of-the-art methods by 44%-48% in accuracy. Its robustness to wave-wind coupling disturbances supports critical maritime missions such as search and rescue and environmental monitoring. All code, experimental configurations, and datasets will be released as open-source to facilitate reproducibility.", "AI": {"tldr": "\u63d0\u51faSpecFuse\u6846\u67b6\uff0c\u901a\u8fc7\u9891\u8c31-\u65f6\u95f4\u878d\u5408\u9884\u6d4b\u63a7\u5236\u89e3\u51b3\u65e0\u4eba\u673a\u5728\u632f\u8361\u6d77\u6d0b\u5e73\u53f0\u4e0a\u7684\u81ea\u4e3b\u7740\u9646\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u9884\u6d4b\u7cbe\u5ea6\u548c\u7740\u9646\u6210\u529f\u7387\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5c06\u5e73\u53f0\u8fd0\u52a8\u89c6\u4e3a\u4e00\u822c\u968f\u673a\u8fc7\u7a0b\u6216\u7f3a\u4e4f\u5bf9\u6ce2\u6d6a\u9891\u8c31\u7279\u6027\u7684\u663e\u5f0f\u5efa\u6a21\uff0c\u5bfc\u81f4\u5728\u52a8\u6001\u6d77\u51b5\u4e0b\u6027\u80fd\u4e0d\u4f73\uff0c\u65e0\u6cd5\u6709\u6548\u5904\u7406\u6ce2\u6d6a\u5f15\u8d77\u7684\u591a\u9891\u632f\u8361\u3001\u98ce\u6270\u548c\u9884\u6d4b\u76f8\u4f4d\u6ede\u540e\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u9891\u8c31-\u65f6\u95f4\u878d\u5408\u9884\u6d4b\u63a7\u5236\u6846\u67b6\uff1a1) \u9891\u57df\u6ce2\u6d6a\u5206\u89e3\u4e0e\u65f6\u95f4\u57df\u9012\u5f52\u72b6\u6001\u4f30\u8ba1\u7ed3\u5408\u8fdb\u884c6\u81ea\u7531\u5ea6\u8fd0\u52a8\u9884\u6d4b\uff1b2) \u5206\u5c42\u63a7\u5236\u67b6\u6784\uff0c\u5305\u62ec\u57fa\u4e8e\u91c7\u6837\u7684HPO-RRT*\u7b97\u6cd5\u8fdb\u884c\u52a8\u6001\u8f68\u8ff9\u89c4\u5212\uff1b3) \u5b66\u4e60\u589e\u5f3a\u9884\u6d4b\u63a7\u5236\u5668\uff0c\u878d\u5408\u6570\u636e\u9a71\u52a8\u7684\u6270\u52a8\u8865\u507f\u4e0e\u4f18\u5316\u6267\u884c\u3002", "result": "\u57282000\u6b21\u4eff\u771f+8\u6b21\u6e56\u4e0a\u5b9e\u9a8c\u4e2d\uff0c\u5b9e\u73b03.2\u5398\u7c73\u9884\u6d4b\u8bef\u5dee\u30014.46\u5398\u7c73\u7740\u9646\u504f\u5dee\u300198.7%/87.5%\u6210\u529f\u7387\uff08\u4eff\u771f/\u73b0\u5b9e\uff09\u300182\u6beb\u79d2\u5d4c\u5165\u5f0f\u786c\u4ef6\u5ef6\u8fdf\uff0c\u6bd4\u73b0\u6709\u65b9\u6cd5\u7cbe\u5ea6\u63d0\u534744%-48%\u3002", "conclusion": "SpecFuse\u6846\u67b6\u901a\u8fc7\u663e\u5f0f\u5efa\u6a21\u6ce2\u6d6a\u9891\u8c31\u7279\u6027\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u6d77\u6d0b\u5e73\u53f0\u81ea\u4e3b\u7740\u9646\u4e2d\u7684\u9884\u6d4b\u76f8\u4f4d\u6ede\u540e\u95ee\u9898\uff0c\u5728\u6ce2\u6d6a-\u98ce\u8026\u5408\u6270\u52a8\u4e0b\u8868\u73b0\u51fa\u5f3a\u9c81\u68d2\u6027\uff0c\u652f\u6301\u641c\u6551\u548c\u73af\u5883\u76d1\u6d4b\u7b49\u5173\u952e\u6d77\u4e8b\u4efb\u52a1\u3002"}}
{"id": "2602.15642", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.15642", "abs": "https://arxiv.org/abs/2602.15642", "authors": ["Alexander Wachter", "Alexander Willert", "Marc-Philip Ecker", "Christian Hartl-Nesic"], "title": "Spatially-Aware Adaptive Trajectory Optimization with Controller-Guided Feedback for Autonomous Racing", "comment": "Accepted at ICRA 2026", "summary": "We present a closed-loop framework for autonomous raceline optimization that combines NURBS-based trajectory representation, CMA-ES global trajectory optimization, and controller-guided spatial feedback. Instead of treating tracking errors as transient disturbances, our method exploits them as informative signals of local track characteristics via a Kalman-inspired spatial update. This enables the construction of an adaptive, acceleration-based constraint map that iteratively refines trajectories toward near-optimal performance under spatially varying track and vehicle behavior. In simulation, our approach achieves a 17.38% lap time reduction compared to a controller parametrized with maximum static acceleration. On real hardware, tested with different tire compounds ranging from high to low friction, we obtain a 7.60% lap time improvement without explicitly parametrizing friction. This demonstrates robustness to changing grip conditions in real-world scenarios.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u7ed3\u5408NURBS\u8f68\u8ff9\u8868\u793a\u3001CMA-ES\u5168\u5c40\u4f18\u5316\u548c\u63a7\u5236\u5668\u5f15\u5bfc\u7a7a\u95f4\u53cd\u9988\u7684\u95ed\u73af\u81ea\u52a8\u9a7e\u9a76\u8d5b\u8f66\u7ebf\u4f18\u5316\u6846\u67b6\uff0c\u5229\u7528\u8ddf\u8e2a\u8bef\u5dee\u4f5c\u4e3a\u5c40\u90e8\u8d5b\u9053\u7279\u5f81\u4fe1\u53f7\uff0c\u5b9e\u73b0\u9002\u5e94\u4e0d\u540c\u6469\u64e6\u6761\u4ef6\u7684\u9c81\u68d2\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u5c06\u8ddf\u8e2a\u8bef\u5dee\u89c6\u4e3a\u77ac\u6001\u5e72\u6270\uff0c\u800c\u672c\u6587\u8ba4\u4e3a\u8fd9\u4e9b\u8bef\u5dee\u5305\u542b\u6709\u4ef7\u503c\u7684\u5c40\u90e8\u8d5b\u9053\u7279\u5f81\u4fe1\u606f\u3002\u9700\u8981\u5f00\u53d1\u4e00\u4e2a\u80fd\u591f\u9002\u5e94\u7a7a\u95f4\u53d8\u5316\u8d5b\u9053\u6761\u4ef6\u548c\u8f66\u8f86\u884c\u4e3a\u7684\u81ea\u9002\u5e94\u4f18\u5316\u6846\u67b6\uff0c\u7279\u522b\u662f\u5728\u771f\u5b9e\u4e16\u754c\u4e2d\u6469\u64e6\u6761\u4ef6\u53d8\u5316\u7684\u60c5\u51b5\u4e0b\u3002", "method": "1. \u4f7f\u7528NURBS\uff08\u975e\u5747\u5300\u6709\u7406B\u6837\u6761\uff09\u8868\u793a\u8f68\u8ff9\uff1b2. \u91c7\u7528CMA-ES\uff08\u534f\u65b9\u5dee\u77e9\u9635\u81ea\u9002\u5e94\u8fdb\u5316\u7b56\u7565\uff09\u8fdb\u884c\u5168\u5c40\u8f68\u8ff9\u4f18\u5316\uff1b3. \u5f15\u5165\u63a7\u5236\u5668\u5f15\u5bfc\u7684\u7a7a\u95f4\u53cd\u9988\u673a\u5236\uff0c\u901a\u8fc7\u5361\u5c14\u66fc\u6ee4\u6ce2\u5668\u542f\u53d1\u7684\u7a7a\u95f4\u66f4\u65b0\u5c06\u8ddf\u8e2a\u8bef\u5dee\u8f6c\u5316\u4e3a\u5c40\u90e8\u8d5b\u9053\u7279\u5f81\u4fe1\u53f7\uff1b4. \u6784\u5efa\u81ea\u9002\u5e94\u52a0\u901f\u5ea6\u7ea6\u675f\u56fe\uff0c\u6839\u636e\u7a7a\u95f4\u53d8\u5316\u7684\u8d5b\u9053\u548c\u8f66\u8f86\u884c\u4e3a\u8fed\u4ee3\u4f18\u5316\u8f68\u8ff9\u3002", "result": "\u4eff\u771f\u4e2d\u76f8\u6bd4\u4f7f\u7528\u6700\u5927\u9759\u6001\u52a0\u901f\u5ea6\u53c2\u6570\u5316\u7684\u63a7\u5236\u5668\uff0c\u5708\u901f\u63d0\u534717.38%\u3002\u5728\u771f\u5b9e\u786c\u4ef6\u6d4b\u8bd5\u4e2d\uff0c\u4f7f\u7528\u4ece\u9ad8\u5230\u4f4e\u4e0d\u540c\u6469\u64e6\u7cfb\u6570\u7684\u8f6e\u80ce\uff0c\u65e0\u9700\u663e\u5f0f\u53c2\u6570\u5316\u6469\u64e6\u7cfb\u6570\u5373\u53ef\u83b7\u5f977.60%\u7684\u5708\u901f\u63d0\u5347\uff0c\u5c55\u793a\u4e86\u5728\u771f\u5b9e\u573a\u666f\u4e2d\u9002\u5e94\u53d8\u5316\u6293\u5730\u529b\u6761\u4ef6\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "\u8be5\u95ed\u73af\u6846\u67b6\u6210\u529f\u5c06\u8ddf\u8e2a\u8bef\u5dee\u8f6c\u5316\u4e3a\u6709\u4ef7\u503c\u7684\u8d5b\u9053\u7279\u5f81\u4fe1\u606f\uff0c\u5b9e\u73b0\u4e86\u5728\u7a7a\u95f4\u53d8\u5316\u6761\u4ef6\u4e0b\u7684\u81ea\u9002\u5e94\u8f68\u8ff9\u4f18\u5316\u3002\u65b9\u6cd5\u5bf9\u771f\u5b9e\u4e16\u754c\u6469\u64e6\u6761\u4ef6\u53d8\u5316\u5177\u6709\u9c81\u68d2\u6027\uff0c\u65e0\u9700\u663e\u5f0f\u53c2\u6570\u5316\u6469\u64e6\u7cfb\u6570\u5373\u53ef\u663e\u8457\u63d0\u5347\u6027\u80fd\uff0c\u4e3a\u81ea\u52a8\u9a7e\u9a76\u8d5b\u8f66\u7ebf\u4f18\u5316\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.15733", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.15733", "abs": "https://arxiv.org/abs/2602.15733", "authors": ["Qiang Zhang", "Jiahao Ma", "Peiran Liu", "Shuai Shi", "Zeran Su", "Zifan Wang", "Jingkai Sun", "Wei Cui", "Jialin Yu", "Gang Han", "Wen Zhao", "Pihai Sun", "Kangning Yin", "Jiaxu Wang", "Jiahang Cao", "Lingfeng Zhang", "Hao Cheng", "Xiaoshuai Hao", "Yiding Ji", "Junwei Liang", "Jian Tang", "Renjing Xu", "Yijie Guo"], "title": "MeshMimic: Geometry-Aware Humanoid Motion Learning through 3D Scene Reconstruction", "comment": "17 pages, 6 figures", "summary": "Humanoid motion control has witnessed significant breakthroughs in recent years, with deep reinforcement learning (RL) emerging as a primary catalyst for achieving complex, human-like behaviors. However, the high dimensionality and intricate dynamics of humanoid robots make manual motion design impractical, leading to a heavy reliance on expensive motion capture (MoCap) data. These datasets are not only costly to acquire but also frequently lack the necessary geometric context of the surrounding physical environment. Consequently, existing motion synthesis frameworks often suffer from a decoupling of motion and scene, resulting in physical inconsistencies such as contact slippage or mesh penetration during terrain-aware tasks. In this work, we present MeshMimic, an innovative framework that bridges 3D scene reconstruction and embodied intelligence to enable humanoid robots to learn coupled \"motion-terrain\" interactions directly from video. By leveraging state-of-the-art 3D vision models, our framework precisely segments and reconstructs both human trajectories and the underlying 3D geometry of terrains and objects. We introduce an optimization algorithm based on kinematic consistency to extract high-quality motion data from noisy visual reconstructions, alongside a contact-invariant retargeting method that transfers human-environment interaction features to the humanoid agent. Experimental results demonstrate that MeshMimic achieves robust, highly dynamic performance across diverse and challenging terrains. Our approach proves that a low-cost pipeline utilizing only consumer-grade monocular sensors can facilitate the training of complex physical interactions, offering a scalable path toward the autonomous evolution of humanoid robots in unstructured environments.", "AI": {"tldr": "MeshMimic\u6846\u67b6\u901a\u8fc73D\u89c6\u89c9\u6a21\u578b\u4ece\u89c6\u9891\u4e2d\u91cd\u5efa\u573a\u666f\u51e0\u4f55\u548c\u4eba\u4f53\u8f68\u8ff9\uff0c\u5229\u7528\u4f18\u5316\u7b97\u6cd5\u63d0\u53d6\u9ad8\u8d28\u91cf\u8fd0\u52a8\u6570\u636e\uff0c\u5e76\u901a\u8fc7\u63a5\u89e6\u4e0d\u53d8\u91cd\u5b9a\u5411\u65b9\u6cd5\u5c06\u4eba-\u73af\u5883\u4ea4\u4e92\u7279\u5f81\u8f6c\u79fb\u5230\u4eba\u5f62\u673a\u5668\u4eba\u4e0a\uff0c\u5b9e\u73b0\u4f4e\u6210\u672c\u3001\u53ef\u6269\u5c55\u7684\u8fd0\u52a8\u5b66\u4e60\u3002", "motivation": "\u5f53\u524d\u4eba\u5f62\u673a\u5668\u4eba\u8fd0\u52a8\u63a7\u5236\u4e3b\u8981\u4f9d\u8d56\u6602\u8d35\u7684\u52a8\u4f5c\u6355\u6349\u6570\u636e\uff0c\u8fd9\u4e9b\u6570\u636e\u7f3a\u4e4f\u5468\u56f4\u7269\u7406\u73af\u5883\u7684\u51e0\u4f55\u4e0a\u4e0b\u6587\uff0c\u5bfc\u81f4\u8fd0\u52a8\u4e0e\u573a\u666f\u89e3\u8026\uff0c\u5728\u590d\u6742\u5730\u5f62\u4efb\u52a1\u4e2d\u51fa\u73b0\u7269\u7406\u4e0d\u4e00\u81f4\u95ee\u9898\uff08\u5982\u63a5\u89e6\u6ed1\u52a8\u3001\u7f51\u683c\u7a7f\u900f\uff09\u3002\u9700\u8981\u4e00\u79cd\u4f4e\u6210\u672c\u3001\u80fd\u5b66\u4e60\u8026\u5408\"\u8fd0\u52a8-\u5730\u5f62\"\u4ea4\u4e92\u7684\u65b9\u6cd5\u3002", "method": "1. \u5229\u7528\u5148\u8fdb\u76843D\u89c6\u89c9\u6a21\u578b\u4ece\u89c6\u9891\u4e2d\u7cbe\u786e\u5206\u5272\u548c\u91cd\u5efa\u4eba\u4f53\u8f68\u8ff9\u53ca\u5730\u5f62/\u7269\u4f53\u76843D\u51e0\u4f55\u7ed3\u6784\uff1b2. \u57fa\u4e8e\u8fd0\u52a8\u5b66\u4e00\u81f4\u6027\u7684\u4f18\u5316\u7b97\u6cd5\u4ece\u566a\u58f0\u89c6\u89c9\u91cd\u5efa\u4e2d\u63d0\u53d6\u9ad8\u8d28\u91cf\u8fd0\u52a8\u6570\u636e\uff1b3. \u63a5\u89e6\u4e0d\u53d8\u91cd\u5b9a\u5411\u65b9\u6cd5\u5c06\u4eba-\u73af\u5883\u4ea4\u4e92\u7279\u5f81\u8f6c\u79fb\u5230\u4eba\u5f62\u667a\u80fd\u4f53\uff1b4. \u4ec5\u4f7f\u7528\u6d88\u8d39\u7ea7\u5355\u76ee\u4f20\u611f\u5668\u6784\u5efa\u4f4e\u6210\u672c\u8bad\u7ec3\u7ba1\u9053\u3002", "result": "MeshMimic\u5728\u591a\u6837\u5316\u548c\u5177\u6709\u6311\u6218\u6027\u7684\u5730\u5f62\u4e0a\u5b9e\u73b0\u4e86\u7a33\u5065\u3001\u9ad8\u5ea6\u52a8\u6001\u7684\u6027\u80fd\u3002\u5b9e\u9a8c\u8bc1\u660e\uff0c\u4ec5\u4f7f\u7528\u6d88\u8d39\u7ea7\u5355\u76ee\u4f20\u611f\u5668\u7684\u4f4e\u6210\u672c\u7ba1\u9053\u80fd\u591f\u8bad\u7ec3\u590d\u6742\u7684\u7269\u7406\u4ea4\u4e92\uff0c\u4e3a\u4eba\u5f62\u673a\u5668\u4eba\u5728\u975e\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u7684\u81ea\u4e3b\u8fdb\u5316\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u8def\u5f84\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u6210\u529f\u5730\u5c063D\u573a\u666f\u91cd\u5efa\u4e0e\u5177\u8eab\u667a\u80fd\u76f8\u7ed3\u5408\uff0c\u4f7f\u673a\u5668\u4eba\u80fd\u591f\u76f4\u63a5\u4ece\u89c6\u9891\u4e2d\u5b66\u4e60\u8026\u5408\u7684\"\u8fd0\u52a8-\u5730\u5f62\"\u4ea4\u4e92\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u4e2d\u8fd0\u52a8\u4e0e\u573a\u666f\u89e3\u8026\u7684\u95ee\u9898\uff0c\u4e3a\u4f4e\u6210\u672c\u3001\u53ef\u6269\u5c55\u7684\u4eba\u5f62\u673a\u5668\u4eba\u8fd0\u52a8\u5b66\u4e60\u63d0\u4f9b\u4e86\u521b\u65b0\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.15767", "categories": ["cs.RO", "cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2602.15767", "abs": "https://arxiv.org/abs/2602.15767", "authors": ["Atharva S Kashyap", "Ugne Aleksandra Morkute", "Patricia Alves-Oliveira"], "title": "Robot-Assisted Social Dining as a White Glove Service", "comment": "20 pages, 9 figures. Proceedings of the 2026 CHI Conference on Human Factors in Computing Systems (CHI '26)", "summary": "Robot-assisted feeding enables people with disabilities who require assistance eating to enjoy a meal independently and with dignity. However, existing systems have only been tested in-lab or in-home, leaving in-the-wild social dining contexts (e.g., restaurants) largely unexplored. Designing a robot for such contexts presents unique challenges, such as dynamic and unsupervised dining environments that a robot needs to account for and respond to. Through speculative participatory design with people with disabilities, supported by semi-structured interviews and a custom AI-based visual storyboarding tool, we uncovered ideal scenarios for in-the-wild social dining. Our key insight suggests that such systems should: embody the principles of a white glove service where the robot (1) supports multimodal inputs and unobtrusive outputs; (2) has contextually sensitive social behavior and prioritizes the user; (3) has expanded roles beyond feeding; (4) adapts to other relationships at the dining table. Our work has implications for in-the-wild and group contexts of robot-assisted feeding.", "AI": {"tldr": "\u673a\u5668\u4eba\u8f85\u52a9\u5582\u98df\u7cfb\u7edf\u5728\u793e\u4ea4\u7528\u9910\u73af\u5883\u4e2d\u7684\u8bbe\u8ba1\u7814\u7a76\uff0c\u901a\u8fc7\u53c2\u4e0e\u5f0f\u8bbe\u8ba1\u63a2\u7d22\u9910\u5385\u7b49\u516c\u5171\u573a\u6240\u7684\u5e94\u7528\u573a\u666f", "motivation": "\u73b0\u6709\u673a\u5668\u4eba\u8f85\u52a9\u5582\u98df\u7cfb\u7edf\u4e3b\u8981\u5728\u5b9e\u9a8c\u5ba4\u6216\u5bb6\u5ead\u73af\u5883\u4e2d\u6d4b\u8bd5\uff0c\u7f3a\u4e4f\u5bf9\u9910\u5385\u7b49\u793e\u4ea4\u7528\u9910\u573a\u666f\u7684\u63a2\u7d22\uff0c\u8fd9\u4e9b\u73af\u5883\u5177\u6709\u52a8\u6001\u3001\u65e0\u76d1\u7763\u7684\u7279\u70b9\uff0c\u9700\u8981\u7279\u6b8a\u8bbe\u8ba1\u8003\u8651", "method": "\u91c7\u7528\u63a8\u6d4b\u6027\u53c2\u4e0e\u5f0f\u8bbe\u8ba1\u65b9\u6cd5\uff0c\u7ed3\u5408\u534a\u7ed3\u6784\u5316\u8bbf\u8c08\u548c\u57fa\u4e8eAI\u7684\u89c6\u89c9\u6545\u4e8b\u677f\u5de5\u5177\uff0c\u4e0e\u6b8b\u969c\u4eba\u58eb\u5171\u540c\u63a2\u7d22\u7406\u60f3\u7684\u793e\u4ea4\u7528\u9910\u573a\u666f", "result": "\u53d1\u73b0\u793e\u4ea4\u7528\u9910\u573a\u666f\u4e2d\u7684\u673a\u5668\u4eba\u7cfb\u7edf\u5e94\u4f53\u73b0\"\u767d\u624b\u5957\u670d\u52a1\"\u539f\u5219\uff1a\u652f\u6301\u591a\u6a21\u6001\u8f93\u5165\u548c\u975e\u4fb5\u5165\u6027\u8f93\u51fa\uff1b\u5177\u6709\u60c5\u5883\u654f\u611f\u7684\u793e\u4ea4\u884c\u4e3a\u5e76\u4f18\u5148\u8003\u8651\u7528\u6237\uff1b\u6269\u5c55\u5582\u98df\u4ee5\u5916\u7684\u89d2\u8272\uff1b\u9002\u5e94\u9910\u684c\u4e0a\u7684\u5176\u4ed6\u5173\u7cfb", "conclusion": "\u7814\u7a76\u4e3a\u673a\u5668\u4eba\u8f85\u52a9\u5582\u98df\u7cfb\u7edf\u5728\u793e\u4ea4\u7528\u9910\u73af\u5883\u548c\u7fa4\u4f53\u573a\u666f\u4e2d\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u8bbe\u8ba1\u6307\u5bfc\uff0c\u5f3a\u8c03\u4e86\u60c5\u5883\u654f\u611f\u6027\u548c\u793e\u4ea4\u9002\u5e94\u6027\u7684\u91cd\u8981\u6027"}}
{"id": "2602.15813", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.15813", "abs": "https://arxiv.org/abs/2602.15813", "authors": ["Haochen Zhang", "Nirav Savaliya", "Faizan Siddiqui", "Enna Sachdeva"], "title": "FAST-EQA: Efficient Embodied Question Answering with Global and Local Region Relevancy", "comment": "WACV 2026", "summary": "Embodied Question Answering (EQA) combines visual scene understanding, goal-directed exploration, spatial and temporal reasoning under partial observability. A central challenge is to confine physical search to question-relevant subspaces while maintaining a compact, actionable memory of observations. Furthermore, for real-world deployment, fast inference time during exploration is crucial. We introduce FAST-EQA, a question-conditioned framework that (i) identifies likely visual targets, (ii) scores global regions of interest to guide navigation, and (iii) employs Chain-of-Thought (CoT) reasoning over visual memory to answer confidently. FAST-EQA maintains a bounded scene memory that stores a fixed-capacity set of region-target hypotheses and updates them online, enabling robust handling of both single and multi-target questions without unbounded growth. To expand coverage efficiently, a global exploration policy treats narrow openings and doors as high-value frontiers, complementing local target seeking with minimal computation. Together, these components focus the agent's attention, improve scene coverage, and improve answer reliability while running substantially faster than prior approaches. On HMEQA and EXPRESS-Bench, FAST-EQA achieves state-of-the-art performance, while performing competitively on OpenEQA and MT-HM3D.", "AI": {"tldr": "FAST-EQA\u662f\u4e00\u4e2a\u7528\u4e8e\u5177\u8eab\u95ee\u7b54\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u8bc6\u522b\u89c6\u89c9\u76ee\u6807\u3001\u8bc4\u5206\u611f\u5174\u8da3\u533a\u57df\u6765\u5f15\u5bfc\u5bfc\u822a\uff0c\u5e76\u4f7f\u7528\u601d\u7ef4\u94fe\u63a8\u7406\u89c6\u89c9\u8bb0\u5fc6\u6765\u56de\u7b54\u95ee\u9898\uff0c\u540c\u65f6\u4fdd\u6301\u6709\u754c\u573a\u666f\u8bb0\u5fc6\u4ee5\u5b9e\u73b0\u5feb\u901f\u63a8\u7406\u3002", "motivation": "\u5177\u8eab\u95ee\u7b54\u9700\u8981\u5728\u90e8\u5206\u53ef\u89c2\u6d4b\u73af\u5883\u4e0b\u7ed3\u5408\u89c6\u89c9\u573a\u666f\u7406\u89e3\u3001\u76ee\u6807\u5bfc\u5411\u63a2\u7d22\u548c\u65f6\u7a7a\u63a8\u7406\u3002\u4e3b\u8981\u6311\u6218\u662f\u5c06\u7269\u7406\u641c\u7d22\u9650\u5236\u5728\u95ee\u9898\u76f8\u5173\u5b50\u7a7a\u95f4\uff0c\u540c\u65f6\u4fdd\u6301\u7d27\u51d1\u3001\u53ef\u64cd\u4f5c\u7684\u89c2\u5bdf\u8bb0\u5fc6\uff0c\u5e76\u4e14\u5b9e\u9645\u90e8\u7f72\u9700\u8981\u5feb\u901f\u63a8\u7406\u65f6\u95f4\u3002", "method": "FAST-EQA\u6846\u67b6\u5305\u542b\u4e09\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a(1)\u8bc6\u522b\u53ef\u80fd\u7684\u89c6\u89c9\u76ee\u6807\uff1b(2)\u5bf9\u5168\u5c40\u611f\u5174\u8da3\u533a\u57df\u8bc4\u5206\u4ee5\u5f15\u5bfc\u5bfc\u822a\uff1b(3)\u4f7f\u7528\u601d\u7ef4\u94fe\u63a8\u7406\u5728\u89c6\u89c9\u8bb0\u5fc6\u4e0a\u8fdb\u884c\u63a8\u7406\u3002\u6846\u67b6\u7ef4\u62a4\u6709\u754c\u573a\u666f\u8bb0\u5fc6\uff0c\u5b58\u50a8\u56fa\u5b9a\u5bb9\u91cf\u7684\u533a\u57df-\u76ee\u6807\u5047\u8bbe\u5e76\u5728\u7ebf\u66f4\u65b0\uff0c\u540c\u65f6\u91c7\u7528\u5168\u5c40\u63a2\u7d22\u7b56\u7565\u5c06\u72ed\u7a84\u5f00\u53e3\u548c\u95e8\u89c6\u4e3a\u9ad8\u4ef7\u503c\u8fb9\u754c\u3002", "result": "\u5728HMEQA\u548cEXPRESS-Bench\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u5728OpenEQA\u548cMT-HM3D\u4e0a\u8868\u73b0\u6709\u7ade\u4e89\u529b\u3002\u76f8\u6bd4\u5148\u524d\u65b9\u6cd5\uff0c\u8fd0\u884c\u901f\u5ea6\u663e\u8457\u66f4\u5feb\uff0c\u540c\u65f6\u63d0\u9ad8\u4e86\u573a\u666f\u8986\u76d6\u7387\u548c\u7b54\u6848\u53ef\u9760\u6027\u3002", "conclusion": "FAST-EQA\u901a\u8fc7\u6709\u754c\u8bb0\u5fc6\u7ba1\u7406\u3001\u76ee\u6807\u5bfc\u5411\u63a2\u7d22\u548c\u601d\u7ef4\u94fe\u63a8\u7406\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5177\u8eab\u95ee\u7b54\u4e2d\u7684\u641c\u7d22\u7a7a\u95f4\u9650\u5236\u548c\u63a8\u7406\u901f\u5ea6\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6027\u80fd\u548c\u5feb\u901f\u63a8\u7406\u7684\u5e73\u8861\u3002"}}
{"id": "2602.15827", "categories": ["cs.RO", "cs.AI", "cs.LG", "eess.SY"], "pdf": "https://arxiv.org/pdf/2602.15827", "abs": "https://arxiv.org/abs/2602.15827", "authors": ["Zhen Wu", "Xiaoyu Huang", "Lujie Yang", "Yuanhang Zhang", "Koushil Sreenath", "Xi Chen", "Pieter Abbeel", "Rocky Duan", "Angjoo Kanazawa", "Carmelo Sferrazza", "Guanya Shi", "C. Karen Liu"], "title": "Perceptive Humanoid Parkour: Chaining Dynamic Human Skills via Motion Matching", "comment": null, "summary": "While recent advances in humanoid locomotion have achieved stable walking on varied terrains, capturing the agility and adaptivity of highly dynamic human motions remains an open challenge. In particular, agile parkour in complex environments demands not only low-level robustness, but also human-like motion expressiveness, long-horizon skill composition, and perception-driven decision-making. In this paper, we present Perceptive Humanoid Parkour (PHP), a modular framework that enables humanoid robots to autonomously perform long-horizon, vision-based parkour across challenging obstacle courses. Our approach first leverages motion matching, formulated as nearest-neighbor search in a feature space, to compose retargeted atomic human skills into long-horizon kinematic trajectories. This framework enables the flexible composition and smooth transition of complex skill chains while preserving the elegance and fluidity of dynamic human motions. Next, we train motion-tracking reinforcement learning (RL) expert policies for these composed motions, and distill them into a single depth-based, multi-skill student policy, using a combination of DAgger and RL. Crucially, the combination of perception and skill composition enables autonomous, context-aware decision-making: using only onboard depth sensing and a discrete 2D velocity command, the robot selects and executes whether to step over, climb onto, vault or roll off obstacles of varying geometries and heights. We validate our framework with extensive real-world experiments on a Unitree G1 humanoid robot, demonstrating highly dynamic parkour skills such as climbing tall obstacles up to 1.25m (96% robot height), as well as long-horizon multi-obstacle traversal with closed-loop adaptation to real-time obstacle perturbations.", "AI": {"tldr": "\u63d0\u51faPerceptive Humanoid Parkour (PHP)\u6846\u67b6\uff0c\u4f7f\u4eba\u5f62\u673a\u5668\u4eba\u80fd\u591f\u57fa\u4e8e\u89c6\u89c9\u81ea\u4e3b\u6267\u884c\u957f\u65f6\u7a0b\u8dd1\u9177\uff0c\u901a\u8fc7\u8fd0\u52a8\u5339\u914d\u7ec4\u5408\u4eba\u7c7b\u6280\u80fd\uff0c\u7ed3\u5408\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u611f\u77e5\u7b56\u7565\uff0c\u5b9e\u73b0\u52a8\u6001\u969c\u788d\u7a7f\u8d8a\u3002", "motivation": "\u5f53\u524d\u4eba\u5f62\u673a\u5668\u4eba\u8fd0\u52a8\u63a7\u5236\u867d\u7136\u80fd\u5728\u4e0d\u540c\u5730\u5f62\u7a33\u5b9a\u884c\u8d70\uff0c\u4f46\u96be\u4ee5\u5b9e\u73b0\u4eba\u7c7b\u52a8\u6001\u8fd0\u52a8\u7684\u654f\u6377\u6027\u548c\u9002\u5e94\u6027\u3002\u590d\u6742\u73af\u5883\u4e2d\u7684\u8dd1\u9177\u9700\u8981\u4f4e\u5c42\u9c81\u68d2\u6027\u3001\u4eba\u7c7b\u8fd0\u52a8\u8868\u73b0\u529b\u3001\u957f\u65f6\u7a0b\u6280\u80fd\u7ec4\u5408\u548c\u611f\u77e5\u9a71\u52a8\u51b3\u7b56\u3002", "method": "1. \u4f7f\u7528\u8fd0\u52a8\u5339\u914d\uff08\u6700\u8fd1\u90bb\u641c\u7d22\uff09\u5c06\u91cd\u5b9a\u5411\u7684\u4eba\u7c7b\u539f\u5b50\u6280\u80fd\u7ec4\u5408\u6210\u957f\u65f6\u7a0b\u8fd0\u52a8\u8f68\u8ff9\uff1b2. \u8bad\u7ec3\u8fd0\u52a8\u8ddf\u8e2a\u5f3a\u5316\u5b66\u4e60\u4e13\u5bb6\u7b56\u7565\uff0c\u901a\u8fc7DAgger\u548cRL\u84b8\u998f\u6210\u5355\u4e00\u6df1\u5ea6\u611f\u77e5\u591a\u6280\u80fd\u5b66\u751f\u7b56\u7565\uff1b3. \u4ec5\u4f7f\u7528\u673a\u8f7d\u6df1\u5ea6\u611f\u77e5\u548c2D\u901f\u5ea6\u6307\u4ee4\uff0c\u81ea\u4e3b\u9009\u62e9\u8de8\u8d8a\u3001\u6500\u722c\u3001\u7ffb\u8d8a\u6216\u6eda\u4e0b\u4e0d\u540c\u51e0\u4f55\u5f62\u72b6\u548c\u9ad8\u5ea6\u7684\u969c\u788d\u7269\u3002", "result": "\u5728Unitree G1\u4eba\u5f62\u673a\u5668\u4eba\u4e0a\u8fdb\u884c\u5927\u91cf\u771f\u5b9e\u4e16\u754c\u5b9e\u9a8c\u9a8c\u8bc1\uff1a1. \u6500\u722c\u9ad8\u8fbe1.25\u7c73\uff08\u673a\u5668\u4eba\u8eab\u9ad8\u768496%\uff09\u7684\u969c\u788d\u7269\uff1b2. \u957f\u65f6\u7a0b\u591a\u969c\u788d\u7269\u7a7f\u8d8a\uff0c\u80fd\u591f\u95ed\u73af\u9002\u5e94\u5b9e\u65f6\u969c\u788d\u7269\u6270\u52a8\u3002", "conclusion": "PHP\u6846\u67b6\u6210\u529f\u5b9e\u73b0\u4e86\u4eba\u5f62\u673a\u5668\u4eba\u7684\u611f\u77e5\u9a71\u52a8\u8dd1\u9177\uff0c\u5c55\u793a\u4e86\u52a8\u6001\u6280\u80fd\u7ec4\u5408\u3001\u81ea\u4e3b\u51b3\u7b56\u548c\u590d\u6742\u73af\u5883\u9002\u5e94\u80fd\u529b\uff0c\u4e3a\u4eba\u5f62\u673a\u5668\u4eba\u7684\u654f\u6377\u8fd0\u52a8\u63a7\u5236\u63d0\u4f9b\u4e86\u65b0\u65b9\u6cd5\u3002"}}
{"id": "2602.15828", "categories": ["cs.RO", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.15828", "abs": "https://arxiv.org/abs/2602.15828", "authors": ["Yuxuan Kuang", "Sungjae Park", "Katerina Fragkiadaki", "Shubham Tulsiani"], "title": "Dex4D: Task-Agnostic Point Track Policy for Sim-to-Real Dexterous Manipulation", "comment": "Project page: https://dex4d.github.io/", "summary": "Learning generalist policies capable of accomplishing a plethora of everyday tasks remains an open challenge in dexterous manipulation. In particular, collecting large-scale manipulation data via real-world teleoperation is expensive and difficult to scale. While learning in simulation provides a feasible alternative, designing multiple task-specific environments and rewards for training is similarly challenging. We propose Dex4D, a framework that instead leverages simulation for learning task-agnostic dexterous skills that can be flexibly recomposed to perform diverse real-world manipulation tasks. Specifically, Dex4D learns a domain-agnostic 3D point track conditioned policy capable of manipulating any object to any desired pose. We train this 'Anypose-to-Anypose' policy in simulation across thousands of objects with diverse pose configurations, covering a broad space of robot-object interactions that can be composed at test time. At deployment, this policy can be zero-shot transferred to real-world tasks without finetuning, simply by prompting it with desired object-centric point tracks extracted from generated videos. During execution, Dex4D uses online point tracking for closed-loop perception and control. Extensive experiments in simulation and on real robots show that our method enables zero-shot deployment for diverse dexterous manipulation tasks and yields consistent improvements over prior baselines. Furthermore, we demonstrate strong generalization to novel objects, scene layouts, backgrounds, and trajectories, highlighting the robustness and scalability of the proposed framework.", "AI": {"tldr": "Dex4D\u662f\u4e00\u4e2a\u5b66\u4e60\u4efb\u52a1\u65e0\u5173\u7075\u5de7\u64cd\u4f5c\u6280\u80fd\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u6a21\u62df\u8bad\u7ec33D\u70b9\u8f68\u8ff9\u6761\u4ef6\u7b56\u7565\uff0c\u5b9e\u73b0\u96f6\u6837\u672c\u8fc1\u79fb\u5230\u771f\u5b9e\u4e16\u754c\u4efb\u52a1", "motivation": "\u5b66\u4e60\u80fd\u591f\u5b8c\u6210\u591a\u79cd\u65e5\u5e38\u4efb\u52a1\u7684\u901a\u7528\u7b56\u7565\u5728\u7075\u5de7\u64cd\u4f5c\u4e2d\u4ecd\u7136\u662f\u4e00\u4e2a\u5f00\u653e\u6311\u6218\u3002\u771f\u5b9e\u4e16\u754c\u9065\u64cd\u4f5c\u6536\u96c6\u5927\u89c4\u6a21\u6570\u636e\u6602\u8d35\u4e14\u96be\u4ee5\u6269\u5c55\uff0c\u800c\u6a21\u62df\u5b66\u4e60\u9700\u8981\u8bbe\u8ba1\u591a\u4e2a\u4efb\u52a1\u7279\u5b9a\u73af\u5883\u548c\u5956\u52b1\u540c\u6837\u5177\u6709\u6311\u6218\u6027", "method": "\u63d0\u51faDex4D\u6846\u67b6\uff0c\u5728\u6a21\u62df\u4e2d\u5b66\u4e60\u9886\u57df\u65e0\u5173\u76843D\u70b9\u8f68\u8ff9\u6761\u4ef6\u7b56\u7565\uff0c\u80fd\u591f\u64cd\u7eb5\u4efb\u4f55\u7269\u4f53\u5230\u4efb\u4f55\u671f\u671b\u59ff\u6001\u3002\u5728\u6570\u5343\u4e2a\u5177\u6709\u4e0d\u540c\u59ff\u6001\u914d\u7f6e\u7684\u7269\u4f53\u4e0a\u8fdb\u884c\u8bad\u7ec3\uff0c\u8986\u76d6\u5e7f\u6cdb\u7684\u673a\u5668\u4eba-\u7269\u4f53\u4ea4\u4e92\u7a7a\u95f4\u3002\u90e8\u7f72\u65f6\u901a\u8fc7\u4ece\u751f\u6210\u89c6\u9891\u4e2d\u63d0\u53d6\u7684\u7269\u4f53\u4e2d\u5fc3\u70b9\u8f68\u8ff9\u63d0\u793a\u7b56\u7565\uff0c\u4f7f\u7528\u5728\u7ebf\u70b9\u8ddf\u8e2a\u8fdb\u884c\u95ed\u73af\u611f\u77e5\u548c\u63a7\u5236", "result": "\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u652f\u6301\u96f6\u6837\u672c\u90e8\u7f72\u5230\u591a\u6837\u7075\u5de7\u64cd\u4f5c\u4efb\u52a1\uff0c\u76f8\u6bd4\u5148\u524d\u57fa\u7ebf\u6709\u6301\u7eed\u6539\u8fdb\u3002\u5c55\u793a\u4e86\u5bf9\u65b0\u7269\u4f53\u3001\u573a\u666f\u5e03\u5c40\u3001\u80cc\u666f\u548c\u8f68\u8ff9\u7684\u5f3a\u6cdb\u5316\u80fd\u529b", "conclusion": "Dex4D\u6846\u67b6\u901a\u8fc7\u6a21\u62df\u5b66\u4e60\u4efb\u52a1\u65e0\u5173\u7075\u5de7\u6280\u80fd\uff0c\u80fd\u591f\u7075\u6d3b\u91cd\u7ec4\u6267\u884c\u591a\u6837\u771f\u5b9e\u4e16\u754c\u64cd\u4f5c\u4efb\u52a1\uff0c\u5c55\u793a\u4e86\u6240\u63d0\u6846\u67b6\u7684\u9c81\u68d2\u6027\u548c\u53ef\u6269\u5c55\u6027"}}
