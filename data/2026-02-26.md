<div id=toc></div>

# Table of Contents

- [cs.RO](#cs.RO) [Total: 29]


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [1] [Cross domain Persistent Monitoring for Hybrid Aerial Underwater Vehicles](https://arxiv.org/abs/2602.21259)
*Ricardo B. Grando,Victor A. Kich,Alisson H. Kolling,Junior C. D. Jesus,Rodrigo S. Guerra,Paulo L. J. Drews-Jr*

Main category: cs.RO

TL;DR: 本文提出了一种结合深度强化学习和迁移学习的混合无人空中水下飞行器持续监测方法，使用共享DRL架构处理空中激光雷达和水下声纳数据，实现跨域适应性。


<details>
  <summary>Details</summary>
Motivation: 混合无人空中水下飞行器(HUAUVs)能够在空中和水下环境中运行，在检查、测绘、搜索和救援等应用中具有潜力，但由于空气和水域的不同动力学特性和约束，开发新方法面临重大挑战。

Method: 采用深度强化学习和迁移学习相结合的方法，使用共享的DRL架构，在激光雷达传感器数据（空中）和声纳数据（水下）上进行训练，为两种环境开发统一策略。

Result: 该方法显示出有希望的结果，能够考虑环境不确定性和多个移动目标的动态特性，证明了在两种环境中使用统一策略的可行性。

Conclusion: 所提出的框架为基于深度强化学习的混合空中水下飞行器可扩展自主持续监测解决方案奠定了基础。

Abstract: Hybrid Unmanned Aerial Underwater Vehicles (HUAUVs) have emerged as platforms capable of operating in both aerial and underwater environments, enabling applications such as inspection, mapping, search, and rescue in challenging scenarios. However, the development of novel methodologies poses significant challenges due to the distinct dynamics and constraints of the air and water domains. In this work, we present persistent monitoring tasks for HUAUVs by combining Deep Reinforcement Learning (DRL) and Transfer Learning to enable cross-domain adaptability. Our approach employs a shared DRL architecture trained on Lidar sensor data (on air) and Sonar data (underwater), demonstrating the feasibility of a unified policy for both environments. We further show that the methodology presents promising results, taking into account the uncertainty of the environment and the dynamics of multiple mobile targets. The proposed framework lays the groundwork for scalable autonomous persistent monitoring solutions based on DRL for hybrid aerial-underwater vehicles.

</details>


### [2] [Dual-Branch INS/GNSS Fusion with Inequality and Equality Constraints](https://arxiv.org/abs/2602.21266)
*Mor Levenhar,Itzik Klein*

Main category: cs.RO

TL;DR: 提出一种双分支信息辅助框架，融合等式和不等式运动约束，通过方差加权方案改善城市环境中车辆导航的精度和鲁棒性，无需额外传感器或硬件。


<details>
  <summary>Details</summary>
Motivation: 城市环境中卫星信号频繁受阻，低成本惯性传感器在长时间信号中断时误差累积迅速。现有非完整约束等方法基于刚性的等式假设，在动态城市驾驶条件下可能被违反，限制了其鲁棒性。

Method: 提出双分支信息辅助框架，融合等式和不等式运动约束，通过方差加权方案集成到现有导航滤波器中。该方法仅需软件修改，无需额外传感器或硬件。

Result: 在四个公开城市数据集上评估，覆盖4.3小时数据。在完全GNSS可用时，垂直位置误差减少16.7%，高度精度提高50.1%；在GNSS不可用时，垂直漂移减少24.2%，高度精度提高20.2%。

Conclusion: 用物理驱动的不等式边界替代硬性运动等式假设，是一种实用且无成本的策略，可提高导航的弹性、连续性和漂移鲁棒性，无需依赖额外传感器、地图数据或学习模型。

Abstract: Reliable vehicle navigation in urban environments remains a challenging problem due to frequent satellite signal blockages caused by tall buildings and complex infrastructure. While fusing inertial reading with satellite positioning in an extended Kalman filter provides short-term navigation continuity, low-cost inertial sensors suffer from rapid error accumulation during prolonged outages. Existing information aiding approaches, such as the non-holonomic constraint, impose rigid equality assumptions on vehicle motion that may be violated under dynamic urban driving conditions, limiting their robustness precisely when aiding is most needed. In this paper, we propose a dual-branch information aiding framework that fuses equality and inequality motion constraints through a variance-weighted scheme, requiring only a software modification to an existing navigation filter with no additional sensors or hardware. The proposed method is evaluated on four publicly available urban datasets featuring various inertial sensors, road conditions, and dynamics, covering a total duration of 4.3 hours of recorded data. Under Full GNSS availability, the method reduces vertical position error by 16.7% and improves altitude accuracy by 50.1% over the standard non-holonomic constraint. Under GNSS-denied conditions, vertical drift is reduced by 24.2% and altitude accuracy improves by 20.2%. These results demonstrate that replacing hard motion equality assumptions with physically motivated inequality bounds is a practical and cost-free strategy for improving navigation resilience, continuity, and drift robustness without relying on additional sensors, map data, or learned models.

</details>


### [3] [Learning Deformable Object Manipulation Using Task-Level Iterative Learning Control](https://arxiv.org/abs/2602.21302)
*Krishna Suresh,Chris Atkeson*

Main category: cs.RO

TL;DR: 提出了一种任务级迭代学习控制方法，用于可变形物体的动态操作，在飞结任务中通过少量演示和简化模型实现快速学习，在多种绳索上达到100%成功率


<details>
  <summary>Details</summary>
Motivation: 可变形物体的动态操作对人类和机器人都具有挑战性，因为它们具有无限自由度且表现出欠驱动特性。现有方法通常需要大量演示数据或仿真，难以在硬件上直接学习

Method: 提出任务级迭代学习控制方法，使用单个人类演示和简化绳索模型，通过求解二次规划构建机器人和绳索的局部逆模型，将任务空间误差传播到动作更新中

Result: 在7种不同类型的绳索上评估，包括链条、乳胶手术管、编织和扭曲绳索等（厚度7-25mm，密度0.013-0.5 kg/m），所有绳索在10次试验内达到100%成功率，且大多数绳索类型间可在2-5次试验内成功迁移

Conclusion: 该方法能够在硬件上直接学习可变形物体的动态操作，无需大量演示数据或仿真，展示了在多种绳索类型上的高效学习和迁移能力

Abstract: Dynamic manipulation of deformable objects is challenging for humans and robots because they have infinite degrees of freedom and exhibit underactuated dynamics. We introduce a Task-Level Iterative Learning Control method for dynamic manipulation of deformable objects. We demonstrate this method on a non-planar rope manipulation task called the flying knot. Using a single human demonstration and a simplified rope model, the method learns directly on hardware without reliance on large amounts of demonstration data or massive amounts of simulation. At each iteration, the algorithm constructs a local inverse model of the robot and rope by solving a quadratic program to propagate task-space errors into action updates. We evaluate performance across 7 different kinds of ropes, including chain, latex surgical tubing, and braided and twisted ropes, ranging in thicknesses of 7--25mm and densities of 0.013--0.5 kg/m. Learning achieves a 100\% success rate within 10 trials on all ropes. Furthermore, the method can successfully transfer between most rope types in approximately 2--5 trials. https://flying-knots.github.io

</details>


### [4] [Unified Complementarity-Based Contact Modeling and Planning for Soft Robots](https://arxiv.org/abs/2602.21316)
*Milad Azizkhani,Yue Chen*

Main category: cs.RO

TL;DR: 提出一个统一的互补性框架CUSP，用于软体机器人的接触建模和规划，解决接触冗余、病态条件等问题，并通过三阶段条件化管道和运动学引导的预热策略实现有效的接触丰富操作。


<details>
  <summary>Details</summary>
Motivation: 软体机器人需要安全、自适应的环境交互，这依赖于接触。然而，软体机器人的接触丰富交互建模和规划仍然具有挑战性：身体上的密集接触候选点会产生冗余约束和秩亏线性互补问题，而高刚度和低摩擦之间的差异会导致严重的病态条件。现有方法依赖于特定问题的近似或基于惩罚的处理。

Method: 提出了一个统一的基于互补性的软体机器人接触建模和规划框架。开发了针对离散化软体机器人的鲁棒线性互补问题模型，采用三阶段条件化管道：惯性秩选择去除冗余接触、Ruiz均衡校正尺度差异和病态条件、对法向块进行轻量级Tikhonov正则化。基于相同公式，引入了运动学引导的预热策略，使用带互补约束的数学规划实现接触中的动态轨迹优化。

Result: CUSP框架为软体机器人提供了统一的接触建模、模拟和规划基础，在接触丰富的球操作任务中展示了其有效性。

Conclusion: CUSP为软体机器人中的接触建模、模拟和规划统一提供了新的基础，解决了现有方法在接触冗余和病态条件方面的挑战。

Abstract: Soft robots were introduced in large part to enable safe, adaptive interaction with the environment, and this interaction relies fundamentally on contact. However, modeling and planning contact-rich interactions for soft robots remain challenging: dense contact candidates along the body create redundant constraints and rank-deficient LCPs, while the disparity between high stiffness and low friction introduces severe ill-conditioning. Existing approaches rely on problem-specific approximations or penalty-based treatments. This letter presents a unified complementarity-based framework for soft-robot contact modeling and planning that brings contact modeling, manipulation, and planning into a unified, physically consistent formulation. We develop a robust Linear Complementarity Problem (LCP) model tailored to discretized soft robots and address these challenges with a three-stage conditioning pipeline: inertial rank selection to remove redundant contacts, Ruiz equilibration to correct scale disparity and ill-conditioning, and lightweight Tikhonov regularization on normal blocks. Building on the same formulation, we introduce a kinematically guided warm-start strategy that enables dynamic trajectory optimization through contact using Mathematical Programs with Complementarity Constraints (MPCC) and demonstrate its effectiveness on contact-rich ball manipulation tasks. In conclusion, CUSP provides a new foundation for unifying contact modeling, simulation, and planning in soft robotics.

</details>


### [5] [Autonomous Sea Turtle Robot for Marine Fieldwork](https://arxiv.org/abs/2602.21389)
*Zach J. Patterson,Emily Sologuren,Levi Cai,Daniel Kim,Alaa Maalouf,Pascal Spino,Daniela Rus*

Main category: cs.RO

TL;DR: 研究人员开发了一款海龟仿生自主水下机器人，结合了仿生运动与现场自主控制，能够在复杂珊瑚礁环境中安全导航并追踪移动目标。


<details>
  <summary>Details</summary>
Motivation: 自主机器人在海洋生态系统观测中具有巨大潜力，但在珊瑚礁等复杂栖息地中近距离操作面临挑战，包括需要安全靠近动物和脆弱结构、应对水流、变化光照和有限传感等问题。现有仿生平台在可部署自主性方面存在局限。

Method: 开发了海龟仿生自主水下机器人，采用紧密集成的视觉驱动控制堆栈，结合深度-航向稳定控制、避障和目标中心控制，能够在复杂地形中追踪和交互移动物体。

Result: 在受控水池实验和新英格兰水族馆活珊瑚礁展区验证了系统性能，展示了稳定操作和可靠追踪快速移动海洋动物及潜水员的能力。在无缆实验中，实现了91%的避障成功率，并引入了低计算量的机载追踪模式。

Conclusion: 该研究为软硬混合仿生水下机器人提供了一条实用路径，使其能够在敏感生态系统中进行最小干扰的探索和近距离监测，首次实现了集成的仿生机器人系统在自然环境中追踪真实海洋动物。

Abstract: Autonomous robots can transform how we observe marine ecosystems, but close-range operation in reefs and other cluttered habitats remains difficult. Vehicles must maneuver safely near animals and fragile structures while coping with currents, variable illumination and limited sensing. Previous approaches simplify these problems by leveraging soft materials and bioinspired swimming designs, but such platforms remain limited in terms of deployable autonomy. Here we present a sea turtle-inspired autonomous underwater robot that closed the gap between bioinspired locomotion and field-ready autonomy through a tightly integrated, vision-driven control stack. The robot combines robust depth-heading stabilization with obstacle avoidance and target-centric control, enabling it to track and interact with moving objects in complex terrain. We validate the robot in controlled pool experiments and in a live coral reef exhibit at the New England Aquarium, demonstrating stable operation and reliable tracking of fast-moving marine animals and human divers. To the best of our knowledge, this is the first integrated biomimetic robotic system, combining novel hardware, control, and field experiments, deployed to track and monitor real marine animals in their natural environment. During off-tether experiments, we demonstrate safe navigation around obstacles (91\% success rate in the aquarium exhibit) and introduce a low-compute onboard tracking mode. Together, these results establish a practical route toward soft-rigid hybrid, bioinspired underwater robots capable of minimally disruptive exploration and close-range monitoring in sensitive ecosystems.

</details>


### [6] [Constructive Vector Fields for Path Following in Fully-Actuated Systems on Matrix Lie Groups](https://arxiv.org/abs/2602.21450)
*Felipe Bartelt,Vinicius M. Gonçalves,Luciano C. A. Pimenta*

Main category: cs.RO

TL;DR: 提出了一种在连通矩阵李群上控制全驱动系统的新向量场策略，确保系统收敛到并沿着群上定义的曲线运动，特别适用于SE(3)群上的控制应用。


<details>
  <summary>Details</summary>
Motivation: 扩展先前在欧几里得空间平移群上的工作到更一般的矩阵李群，利用李群特性实现非冗余控制输入，特别针对SE(3)群（刚体运动群）的应用需求。

Method: 基于李群特性的向量场控制策略，通过李群性质扩展收敛和遍历分量的正交性证明，提供非冗余控制输入（维度与李群维度匹配），并为SE(3)群提供了高效计算算法。

Result: 成功将向量场控制策略推广到一般连通矩阵李群，在SE(3)群上实现了与机械扭转对应的非冗余控制输入，通过机器人机械臂实验验证了方法的有效性。

Conclusion: 提出的向量场策略为全驱动系统在矩阵李群上的曲线跟踪控制提供了通用框架，特别适用于SE(3)群上的刚体运动控制，具有实际应用价值。

Abstract: This paper presents a novel vector field strategy for controlling fully-actuated systems on connected matrix Lie groups, ensuring convergence to and traversal along a curve defined on the group. Our approach generalizes our previous work (Rezende et al., 2022) and reduces to it when considering the Lie group of translations in Euclidean space. Since the proofs in Rezende et al. (2022) rely on key properties such as the orthogonality between the convergent and traversal components, we extend these results by leveraging Lie group properties. These properties also allow the control input to be non-redundant, meaning it matches the dimension of the Lie group, rather than the potentially larger dimension of the space in which the group is embedded. This can lead to more practical control inputs in certain scenarios. A particularly notable application of our strategy is in controlling systems on SE(3) -- in this case, the non-redundant input corresponds to the object's mechanical twist -- making it well-suited for controlling objects that can move and rotate freely, such as omnidirectional drones. In this case, we provide an efficient algorithm to compute the vector field. We experimentally validate the proposed method using a robotic manipulator to demonstrate its effectiveness.

</details>


### [7] [LiLo-VLA: Compositional Long-Horizon Manipulation via Linked Object-Centric Policies](https://arxiv.org/abs/2602.21531)
*Yue Yang,Shuo Cheng,Yu Fang,Homanga Bharadhwaj,Mingyu Ding,Gedas Bertasius,Daniel Szafir*

Main category: cs.RO

TL;DR: LiLo-VLA是一个模块化框架，通过分离运输和交互来解决长时程操作任务，实现零样本泛化，在仿真和真实世界任务中显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 通用机器人需要掌握涉及多个运动结构变化的长时程操作任务。现有的视觉-语言-动作模型虽然能掌握多样化的原子技能，但在组合这些技能时面临组合复杂性，并且容易因环境敏感性导致级联失败。

Method: 提出LiLo-VLA模块化框架，将运输与交互解耦：一个Reaching模块处理全局运动，一个Interaction模块使用以物体为中心的VLA处理感兴趣的孤立物体。这种模块化设计支持动态重规划和技能重用，有效缓解端到端方法中的级联错误。

Result: 在包含LIBERO-Long++和Ultra-Long的21个任务仿真基准测试中，LiLo-VLA达到69%的平均成功率，比Pi0.5高41%，比OpenVLA-OFT高67%。在8个真实世界长时程任务中，平均成功率达到85%。

Conclusion: LiLo-VLA通过模块化设计有效解决了长时程操作任务中的组合复杂性和环境敏感性问题，实现了零样本泛化能力，在仿真和真实世界任务中都表现出优越性能。

Abstract: General-purpose robots must master long-horizon manipulation, defined as tasks involving multiple kinematic structure changes (e.g., attaching or detaching objects) in unstructured environments. While Vision-Language-Action (VLA) models offer the potential to master diverse atomic skills, they struggle with the combinatorial complexity of sequencing them and are prone to cascading failures due to environmental sensitivity. To address these challenges, we propose LiLo-VLA (Linked Local VLA), a modular framework capable of zero-shot generalization to novel long-horizon tasks without ever being trained on them. Our approach decouples transport from interaction: a Reaching Module handles global motion, while an Interaction Module employs an object-centric VLA to process isolated objects of interest, ensuring robustness against irrelevant visual features and invariance to spatial configurations. Crucially, this modularity facilitates robust failure recovery through dynamic replanning and skill reuse, effectively mitigating the cascading errors common in end-to-end approaches. We introduce a 21-task simulation benchmark consisting of two challenging suites: LIBERO-Long++ and Ultra-Long. In these simulations, LiLo-VLA achieves a 69% average success rate, outperforming Pi0.5 by 41% and OpenVLA-OFT by 67%. Furthermore, real-world evaluations across 8 long-horizon tasks demonstrate an average success rate of 85%. Project page: https://yy-gx.github.io/LiLo-VLA/.

</details>


### [8] [Learning Agile and Robust Omnidirectional Aerial Motion on Overactuated Tiltable-Quadrotors](https://arxiv.org/abs/2602.21583)
*Wentao Zhang,Zhaoqi Ma,Jinjie Li,Huayi Wang,Haokun Liu,Junichiro Sugihara,Chen Chen,Yicheng Chen,Moju Zhao*

Main category: cs.RO

TL;DR: 该研究提出了一种基于强化学习的倾斜旋翼无人机全向运动控制框架，通过系统辨识和物理一致的领域随机化实现可靠的仿真到现实迁移，在保持运动精度的同时显著提升了鲁棒性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 倾斜旋翼无人机虽然能通过推力矢量实现全向机动，但由于关节和旋翼动力学的强耦合关系，带来了显著的控制挑战。基于模型的控制器在理想条件下能达到高精度，但在存在干扰和建模不确定性时，其鲁棒性和响应性会下降。

Method: 提出了一个基于学习的控制框架，用于高效获取协调的旋翼-关节行为以实现SE(3)空间中的目标姿态跟踪。通过系统辨识结合最小化和物理一致的领域随机化，实现了可靠的仿真到现实迁移。

Result: 与最先进的NMPC控制器相比，该方法实现了相当的六自由度姿态跟踪精度，同时表现出更优越的鲁棒性和跨任务泛化能力，能够在真实硬件上实现零样本部署。

Conclusion: 强化学习为过驱动倾斜四旋翼无人机的全向运动控制提供了一种有效的解决方案，在保持精度的同时显著提升了系统的鲁棒性和适应性，实现了可靠的仿真到现实迁移。

Abstract: Tilt-rotor aerial robots enable omnidirectional maneuvering through thrust vectoring, but introduce significant control challenges due to the strong coupling between joint and rotor dynamics. While model-based controllers can achieve high motion accuracy under nominal conditions, their robustness and responsiveness often degrade in the presence of disturbances and modeling uncertainties. This work investigates reinforcement learning for omnidirectional aerial motion control on over-actuated tiltable quadrotors that prioritizes robustness and agility. We present a learning-based control framework that enables efficient acquisition of coordinated rotor-joint behaviors for reaching target poses in the $SE(3)$ space. To achieve reliable sim-to-real transfer while preserving motion accuracy, we integrate system identification with minimal and physically consistent domain randomization. Compared with a state-of-the-art NMPC controller, the proposed method achieves comparable six-degree-of-freedom pose tracking accuracy, while demonstrating superior robustness and generalization across diverse tasks, enabling zero-shot deployment on real hardware.

</details>


### [9] [SPOC: Safety-Aware Planning Under Partial Observability And Physical Constraints](https://arxiv.org/abs/2602.21595)
*Hyungmin Kim,Hobeom Jeon,Dohyung Kim,Minsu Jang,Jeahong Kim*

Main category: cs.RO

TL;DR: SPOC是一个安全感知的具身任务规划基准，整合了部分可观测性、物理约束和逐步规划，专注于评估家庭环境中的安全风险。


<details>
  <summary>Details</summary>
Motivation: 现有具身任务规划基准往往忽视真实世界环境中的安全挑战，如部分可观测性和物理约束，限制了评估规划可行性和安全性的能力。

Method: 引入SPOC基准，整合严格的部分可观测性、物理约束、逐步规划，以及基于目标条件的评估。覆盖火灾、液体、伤害、物体损坏和污染等多种家庭危害。

Result: 实验表明，当前最先进的大语言模型在确保安全感知规划方面存在困难，特别是在处理隐式约束时表现不佳。

Conclusion: SPOC基准为安全感知的具身任务规划提供了严格的评估框架，揭示了现有模型在真实世界安全规划方面的局限性，相关代码和数据集已开源。

Abstract: Embodied Task Planning with large language models faces safety challenges in real-world environments, where partial observability and physical constraints must be respected. Existing benchmarks often overlook these critical factors, limiting their ability to evaluate both feasibility and safety. We introduce SPOC, a benchmark for safety-aware embodied task planning, which integrates strict partial observability, physical constraints, step-by-step planning, and goal-condition-based evaluation. Covering diverse household hazards such as fire, fluid, injury, object damage, and pollution, SPOC enables rigorous assessment through both state and constraint-based online metrics. Experiments with state-of-the-art LLMs reveal that current models struggle to ensure safety-aware planning, particularly under implicit constraints. Code and dataset are available at https://github.com/khm159/SPOC

</details>


### [10] [Iterative Closed-Loop Motion Synthesis for Scaling the Capabilities of Humanoid Control](https://arxiv.org/abs/2602.21599)
*Weisheng Xu,Qiwei Wu,Jiaxi Zhang,Tan Jing,Yangfan Li,Yuetong Fang,Jiaqi Xiong,Kai Wu,Rong Ou,Renjing Xu*

Main category: cs.RO

TL;DR: 提出闭环自动化运动数据生成与迭代框架，解决物理人形控制中数据集难度分布固定和高质量数据获取成本高的问题，显著降低失败率。


<details>
  <summary>Details</summary>
Motivation: 物理人形控制依赖多样化的运动数据集，但现有数据集的固定难度分布限制了控制策略的性能上限。同时，通过专业动作捕捉系统获取高质量数据的成本高昂，难以实现大规模扩展。

Method: 提出闭环自动化运动数据生成与迭代框架，能够生成具有丰富动作语义的高质量运动数据（包括武术、舞蹈、格斗、运动、体操等）。通过物理指标和目标评估实现策略和数据的难度迭代，使训练跟踪器突破原有难度限制。

Result: 在PHC单基元跟踪器上，仅使用约1/10的AMASS数据集大小，测试集（2201个片段）的平均失败率相比基线降低了45%。通过全面的消融和对比实验验证了框架的合理性和优势。

Conclusion: 提出的闭环自动化运动数据生成与迭代框架有效解决了物理人形控制中的数据质量和可扩展性问题，显著提升了控制性能并突破了原有难度限制。

Abstract: Physics-based humanoid control relies on training with motion datasets that have diverse data distributions. However, the fixed difficulty distribution of datasets limits the performance ceiling of the trained control policies. Additionally, the method of acquiring high-quality data through professional motion capture systems is constrained by costs, making it difficult to achieve large-scale scalability. To address these issues, we propose a closed-loop automated motion data generation and iterative framework. It can generate high-quality motion data with rich action semantics, including martial arts, dance, combat, sports, gymnastics, and more. Furthermore, our framework enables difficulty iteration of policies and data through physical metrics and objective evaluations, allowing the trained tracker to break through its original difficulty limits. On the PHC single-primitive tracker, using only approximately 1/10 of the AMASS dataset size, the average failure rate on the test set (2201 clips) is reduced by 45\% compared to the baseline. Finally, we conduct comprehensive ablation and comparative experiments to highlight the rationality and advantages of our framework.

</details>


### [11] [Jumping Control for a Quadrupedal Wheeled-Legged Robot via NMPC and DE Optimization](https://arxiv.org/abs/2602.21612)
*Xuanqi Zeng,Lingwei Zhang,Linzhu Yue,Zhitao Song,Hongbo Zhang,Tianlin Zhang,Yun-Hui Liu*

Main category: cs.RO

TL;DR: 本文提出了一种用于四足轮腿机器人的新型运动控制框架，结合非线性模型预测控制（NMPC）进行运动控制，以及基于差分进化（DE）的轨迹优化进行跳跃控制，实现了垂直跳跃、前向跳跃和后空翻等多种敏捷动作。


<details>
  <summary>Details</summary>
Motivation: 四足轮腿机器人结合了腿式和轮式运动的优势，具有卓越的机动性，但由于轮腿引入了额外的自由度，执行动态跳跃仍然是一个重大挑战。本文旨在开发一种小型轮腿机器人并建立有效的运动控制框架来解决这一挑战。

Method: 提出了一种集成非线性模型预测控制（NMPC）用于运动控制，以及基于差分进化（DE）的轨迹优化用于跳跃控制的运动控制框架。该控制器利用轮子运动和运动控制来提升跳跃性能。

Result: 通过大量仿真和真实世界实验验证了该框架的有效性，实现了越过0.12米障碍物的前向跳跃和达到0.5米高度的垂直跳跃，并展示了垂直跳跃、前向跳跃和后空翻等多种敏捷动作。

Conclusion: 该研究成功开发了一种小型四足轮腿机器人及其运动控制框架，有效解决了轮腿机器人动态跳跃的挑战，为轮腿机器人的敏捷运动控制提供了新的解决方案。

Abstract: Quadrupedal wheeled-legged robots combine the advantages of legged and wheeled locomotion to achieve superior mobility, but executing dynamic jumps remains a significant challenge due to the additional degrees of freedom introduced by wheeled legs. This paper develops a mini-sized wheeled-legged robot for agile motion and presents a novel motion control framework that integrates the Nonlinear Model Predictive Control (NMPC) for locomotion and the Differential Evolution (DE) based trajectory optimization for jumping in quadrupedal wheeled-legged robots. The proposed controller utilizes wheel motion and locomotion to enhance jumping performance, achieving versatile maneuvers such as vertical jumping, forward jumping, and backflips. Extensive simulations and real-world experiments validate the effectiveness of the framework, demonstrating a forward jump over a 0.12 m obstacle and a vertical jump reaching 0.5 m.

</details>


### [12] [Self-Correcting VLA: Online Action Refinement via Sparse World Imagination](https://arxiv.org/abs/2602.21633)
*Chenyv Liu,Wentao Tan,Lei Zhu,Fengling Li,Jingjing Li,Guoli Yang,Heng Tao Shen*

Main category: cs.RO

TL;DR: SC-VLA提出了一种自校正的视觉-语言-动作模型，通过稀疏想象实现自我改进，在机器人操作任务中取得了最先进的性能表现。


<details>
  <summary>Details</summary>
Motivation: 现有的VLA模型依赖于统计数据先验，对物理动态的理解有限；强化学习虽然通过探索增强物理基础，但依赖外部奖励信号且与智能体内部状态隔离；世界动作模型缺乏明确的自我改进机制。

Method: 1) 设计稀疏世界想象：集成辅助预测头来预测当前任务进展和未来轨迹趋势，约束策略编码短期物理演化；2) 引入在线动作细化模块：基于预测的稀疏未来状态重塑进展相关的密集奖励，调整轨迹方向。

Result: 在模拟基准和真实世界设置的挑战性机器人操作任务评估中，SC-VLA实现了最先进的性能：任务吞吐量最高，步骤减少16%，成功率提高9%，在真实世界实验中提升14%。

Conclusion: SC-VLA通过稀疏想象实现自我校正，解决了现有VLA模型物理理解有限、强化学习外部奖励依赖以及世界动作模型缺乏明确自我改进机制的问题，在机器人操作任务中表现出优越性能。

Abstract: Standard vision-language-action (VLA) models rely on fitting statistical data priors, limiting their robust understanding of underlying physical dynamics. Reinforcement learning enhances physical grounding through exploration yet typically relies on external reward signals that remain isolated from the agent's internal states. World action models have emerged as a promising paradigm that integrates imagination and control to enable predictive planning. However, they rely on implicit context modeling, lacking explicit mechanisms for self-improvement. To solve these problems, we propose Self-Correcting VLA (SC-VLA), which achieve self-improvement by intrinsically guiding action refinement through sparse imagination. We first design sparse world imagination by integrating auxiliary predictive heads to forecast current task progress and future trajectory trends, thereby constraining the policy to encode short-term physical evolution. Then we introduce the online action refinement module to reshape progress-dependent dense rewards, adjusting trajectory orientation based on the predicted sparse future states. Evaluations on challenging robot manipulation tasks from simulation benchmarks and real-world settings demonstrate that SC-VLA achieve state-of-the-art performance, yielding the highest task throughput with 16% fewer steps and a 9% higher success rate than the best-performing baselines, alongside a 14% gain in real-world experiments. Code is available at https://github.com/Kisaragi0/SC-VLA.

</details>


### [13] [DAGS-SLAM: Dynamic-Aware 3DGS SLAM via Spatiotemporal Motion Probability and Uncertainty-Aware Scheduling](https://arxiv.org/abs/2602.21644)
*Li Zhang,Yu-An Liu,Xijia Jiang,Conghao Huang,Danyang Li,Yanyong Zhang*

Main category: cs.RO

TL;DR: DAGS-SLAM：一种动态感知的3D高斯泼溅SLAM系统，通过时空运动概率状态和按需语义触发机制，在移动设备上实现实时动态场景重建与定位。


<details>
  <summary>Details</summary>
Motivation: 移动机器人和物联网设备需要在有限的计算和能耗预算下实现实时定位和密集重建。现有动态3DGS-SLAM方法依赖计算量大的光流和逐帧分割，不适合移动部署且在挑战性光照下表现脆弱。

Method: DAGS-SLAM为每个高斯点维护时空运动概率状态，通过不确定性感知调度器按需触发语义处理。融合轻量级YOLO实例先验与几何线索估计运动概率，将运动概率传播到前端进行动态感知对应点选择，在后端通过运动概率引导优化抑制动态伪影。

Result: 在公开动态RGB-D基准测试中，DAGS-SLAM展示了改进的重建质量和鲁棒跟踪能力，同时在商用GPU上保持实时吞吐量，实现了实用的速度-精度权衡并减少了语义调用次数。

Conclusion: DAGS-SLAM通过运动概率状态和按需语义调度机制，为移动部署提供了高效的动态感知3DGS-SLAM解决方案，在保持实时性能的同时提升了动态场景下的重建和跟踪鲁棒性。

Abstract: Mobile robots and IoT devices demand real-time localization and dense reconstruction under tight compute and energy budgets. While 3D Gaussian Splatting (3DGS) enables efficient dense SLAM, dynamic objects and occlusions still degrade tracking and mapping. Existing dynamic 3DGS-SLAM often relies on heavy optical flow and per-frame segmentation, which is costly for mobile deployment and brittle under challenging illumination. We present DAGS-SLAM, a dynamic-aware 3DGS-SLAM system that maintains a spatiotemporal motion probability (MP) state per Gaussian and triggers semantics on demand via an uncertainty-aware scheduler. DAGS-SLAM fuses lightweight YOLO instance priors with geometric cues to estimate and temporally update MP, propagates MP to the front-end for dynamic-aware correspondence selection, and suppresses dynamic artifacts in the back-end via MP-guided optimization. Experiments on public dynamic RGB-D benchmarks show improved reconstruction and robust tracking while sustaining real-time throughput on a commodity GPU, demonstrating a practical speed-accuracy tradeoff with reduced semantic invocations toward mobile deployment.

</details>


### [14] [Biomechanical Comparisons Reveal Divergence of Human and Humanoid Gaits](https://arxiv.org/abs/2602.21666)
*Luying Feng,Yaochu Jin,Hanze Hu,Wei Chen*

Main category: cs.RO

TL;DR: 该研究提出了步态差异分析框架(GDAF)，用于系统量化人类与双足机器人之间的运动学和动力学差异，发现尽管现代人形控制器能生成视觉上类似人类的运动，但在步态对称性、能量分布和关节协调方面仍存在显著差异。


<details>
  <summary>Details</summary>
Motivation: 由于生物结构与机械结构之间的根本差异，实现类人步态在足式机器人中仍然具有挑战性。虽然模仿学习已成为生成自然机器人运动的有前途方法，但简单地复制关节角度轨迹无法捕捉人类运动的底层原理。

Method: 提出了步态差异分析框架(GDAF)，这是一个统一的生物力学评估框架，系统量化人类与双足机器人之间的运动学和动力学差异。应用GDAF在28种步行速度下系统比较人类和人形机器人的步态，收集并发布了来自最先进人形控制器的速度连续人形步态数据集，并提供了GDAF的开源实现。

Result: 结果显示，尽管现代人形控制器能生成视觉上类似人类的运动，但在不同速度下仍存在显著的生物力学差异。机器人表现出步态对称性、能量分布和关节协调方面的系统性偏差，表明在人形步态的生物力学保真度和能量效率方面仍有很大改进空间。

Conclusion: 该工作为人形步态评估提供了定量基准，并提供数据和多功能工具来支持开发更类人且能量效率更高的步态控制器。数据和代码将在论文接受后公开。

Abstract: It remains challenging to achieve human-like locomotion in legged robots due to fundamental discrepancies between biological and mechanical structures. Although imitation learning has emerged as a promising approach for generating natural robotic movements, simply replicating joint angle trajectories fails to capture the underlying principles of human motion. This study proposes a Gait Divergence Analysis Framework (GDAF), a unified biomechanical evaluation framework that systematically quantifies kinematic and kinetic discrepancies between humans and bipedal robots. We apply GDAF to systematically compare human and humanoid locomotion across 28 walking speeds. To enable reproducible analysis, we collect and release a speed-continuous humanoid locomotion dataset from a state-of-the-art humanoid controller. We further provide an open-source implementation of GDAF, including analysis, visualization, and MuJoCo-based tools, enabling quantitative, interpretable, and reproducible biomechanical analysis of humanoid locomotion. Results demonstrate that despite visually human-like motion generated by modern humanoid controllers, significant biomechanical divergence persists across speeds. Robots exhibit systematic deviations in gait symmetry, energy distribution, and joint coordination, indicating that substantial room remains for improving the biomechanical fidelity and energetic efficiency of humanoid locomotion. This work provides a quantitative benchmark for evaluating humanoid locomotion and offers data and versatile tools to support the development of more human-like and energetically efficient locomotion controllers. The data and code will be made publicly available upon acceptance of the paper.

</details>


### [15] [Hierarchical LLM-Based Multi-Agent Framework with Prompt Optimization for Multi-Robot Task Planning](https://arxiv.org/abs/2602.21670)
*Tomoya Kawabe,Rin Takano*

Main category: cs.RO

TL;DR: 提出了一种基于LLM的分层多智能体任务规划器，通过提示优化和元提示共享，结合经典PDDL规划器，显著提升了多机器人任务规划的成功率。


<details>
  <summary>Details</summary>
Motivation: 传统PDDL规划器在处理模糊或长时程任务时存在困难，而LLM虽然能解释指令并提出计划，但容易产生幻觉或不可行动作。需要结合两者优势，提高多机器人任务规划的准确性和可行性。

Method: 采用分层多智能体LLM架构：上层分解任务并分配给下层智能体，下层生成PDDL问题由经典规划器求解。当计划失败时，使用TextGrad启发的文本梯度更新优化每个智能体的提示。同时，在同一层内学习和共享元提示，实现多智能体环境下的高效提示优化。

Result: 在MAT-THOR基准测试中，复合任务成功率0.95，复杂任务0.84，模糊任务0.60，分别比之前的SOTA LaMMA-P提高了2、7和15个百分点。消融研究表明，分层结构、提示优化和元提示共享分别贡献约+59、+37和+4个百分点的成功率提升。

Conclusion: 提出的分层多智能体LLM规划器结合了LLM的语义理解能力和经典规划器的严谨性，通过提示优化和元提示共享机制，显著提升了多机器人任务规划的性能，特别是在处理模糊和复杂任务方面表现优异。

Abstract: Multi-robot task planning requires decomposing natural-language instructions into executable actions for heterogeneous robot teams. Conventional Planning Domain Definition Language (PDDL) planners provide rigorous guarantees but struggle to handle ambiguous or long-horizon missions, while large language models (LLMs) can interpret instructions and propose plans but may hallucinate or produce infeasible actions. We present a hierarchical multi-agent LLM-based planner with prompt optimization: an upper layer decomposes tasks and assigns them to lower-layer agents, which generate PDDL problems solved by a classical planner. When plans fail, the system applies TextGrad-inspired textual-gradient updates to optimize each agent's prompt and thereby improve planning accuracy. In addition, meta-prompts are learned and shared across agents within the same layer, enabling efficient prompt optimization in multi-agent settings. On the MAT-THOR benchmark, our planner achieves success rates of 0.95 on compound tasks, 0.84 on complex tasks, and 0.60 on vague tasks, improving over the previous state-of-the-art LaMMA-P by 2, 7, and 15 percentage points respectively. An ablation study shows that the hierarchical structure, prompt optimization, and meta-prompt sharing contribute roughly +59, +37, and +4 percentage points to the overall success rate.

</details>


### [16] [SunnyParking: Multi-Shot Trajectory Generation and Motion State Awareness for Human-like Parking](https://arxiv.org/abs/2602.21682)
*Jishu Miao,Han Chen,Jiankun Zhai,Qi Liu,Tsubasa Hirakawa,Takayoshi Yamashita,Hironobu Fujiyoshi*

Main category: cs.RO

TL;DR: SunnyParking是一个新颖的双分支端到端架构，通过联合预测空间轨迹和离散运动状态序列（如前向/倒车）实现运动状态感知，解决了传统方法在自主泊车中的维度缺陷问题。


<details>
  <summary>Details</summary>
Motivation: 现有端到端规划方法将泊车任务简化为几何路径回归问题，忽略了车辆运动状态的显式建模，导致物理不可行的轨迹和偏离真实人类驾驶行为，特别是在多段泊车场景的关键换挡点。

Method: 提出SunnyParking双分支架构：1）联合预测空间轨迹和离散运动状态序列；2）引入基于傅里叶特征的目标停车位表示，克服传统鸟瞰图方法的分辨率限制；3）开源新的CARLA模拟器泊车数据集。

Result: 在复杂多段泊车场景中生成更鲁棒和类人的轨迹，相比最先进方法显著提高了换挡点定位精度。

Conclusion: SunnyParking通过运动状态感知和创新的目标表示方法，有效解决了自主泊车中的维度缺陷问题，实现了更符合物理约束和人类驾驶行为的轨迹规划。

Abstract: Autonomous parking fundamentally differs from on-road driving due to its frequent direction changes and complex maneuvering requirements. However, existing End-to-End (E2E) planning methods often simplify the parking task into a geometric path regression problem, neglecting explicit modeling of the vehicle's kinematic state. This "dimensionality deficiency" easily leads to physically infeasible trajectories and deviates from real human driving behavior, particularly at critical gear-shift points in multi-shot parking scenarios. In this paper, we propose SunnyParking, a novel dual-branch E2E architecture that achieves motion state awareness by jointly predicting spatial trajectories and discrete motion state sequences (e.g., forward/reverse). Additionally, we introduce a Fourier feature-based representation of target parking slots to overcome the resolution limitations of traditional bird's-eye view (BEV) approaches, enabling high-precision target interactions. Experimental results demonstrate that our framework generates more robust and human-like trajectories in complex multi-shot parking scenarios, while significantly improving gear-shift point localization accuracy compared to state-of-the-art methods. We open-source a new parking dataset of the CARLA simulator, specifically designed to evaluate full prediction capabilities under complex maneuvers.

</details>


### [17] [Primary-Fine Decoupling for Action Generation in Robotic Imitation](https://arxiv.org/abs/2602.21684)
*Xiaohan Lei,Min Wang,Wengang Zhou,Xingyu Lu,Houqiang Li*

Main category: cs.RO

TL;DR: PF-DAG：两阶段动作生成框架，通过解耦粗粒度模式选择和细粒度动作生成，解决机器人模仿学习中多模态分布问题，在多个基准测试中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 机器人操作动作序列中的多模态分布给模仿学习带来挑战。现有方法要么将动作离散化为标记而丢失细粒度变化，要么在单阶段生成连续动作导致模式转换不稳定，需要更好的解决方案。

Method: 提出PF-DAG两阶段框架：1）将动作块压缩为少量离散模式，轻量级策略选择一致的粗粒度模式避免模式跳跃；2）学习模式条件MeanFlow策略生成高保真连续动作。

Result: 理论证明PF-DAG的两阶段设计比单阶段生成策略有更低的MSE边界。在Adroit、DexArt和MetaWorld基准的56个任务中优于最先进基线，并能泛化到真实世界触觉灵巧操作任务。

Conclusion: 显式的模式级解耦既能实现鲁棒的多模态建模，又能支持机器人操作的响应式闭环控制，为多模态动作生成提供了有效解决方案。

Abstract: Multi-modal distribution in robotic manipulation action sequences poses critical challenges for imitation learning. To this end, existing approaches often model the action space as either a discrete set of tokens or a continuous, latent-variable distribution. However, both approaches present trade-offs: some methods discretize actions into tokens and therefore lose fine-grained action variations, while others generate continuous actions in a single stage tend to produce unstable mode transitions. To address these limitations, we propose Primary-Fine Decoupling for Action Generation (PF-DAG), a two-stage framework that decouples coarse action consistency from fine-grained variations. First, we compress action chunks into a small set of discrete modes, enabling a lightweight policy to select consistent coarse modes and avoid mode bouncing. Second, a mode conditioned MeanFlow policy is learned to generate high-fidelity continuous actions. Theoretically, we prove PF-DAG's two-stage design achieves a strictly lower MSE bound than single-stage generative policies. Empirically, PF-DAG outperforms state-of-the-art baselines across 56 tasks from Adroit, DexArt, and MetaWorld benchmarks. It further generalizes to real-world tactile dexterous manipulation tasks. Our work demonstrates that explicit mode-level decoupling enables both robust multi-modal modeling and reactive closed-loop control for robotic manipulation.

</details>


### [18] [Trajectory Generation with Endpoint Regulation and Momentum-Aware Dynamics for Visually Impaired Scenarios](https://arxiv.org/abs/2602.21691)
*Yuting Zeng,Manping Fan,You Zhou,Yongbin Yu,Zhiwen Zheng,Jingtao Zhang,Liyong Ren,Zhenglin Yang*

Main category: cs.RO

TL;DR: 提出了一种用于视障场景的轨迹生成方法，通过端点调节和动量感知动力学来改善轨迹平滑性和一致性


<details>
  <summary>Details</summary>
Motivation: 传统基于加加速度的启发式轨迹采样方法在结构化、低速动态环境中，由于独立分段生成和传统平滑性惩罚，容易导致终端行为不稳定和状态不连续

Method: 提出了一种轨迹生成方法，集成了端点调节来稳定每个分段内的终端状态，以及动量感知动力学来规范速度和加速度的演化以确保分段一致性

Result: 实验结果显示：加速度峰值和加加速度水平降低且分散度减小，速度和加速度曲线更平滑，端点分布更稳定，不可行轨迹候选数量减少

Conclusion: 该方法通过端点调节和动量感知动力学有效改善了轨迹生成的平滑性、一致性和稳定性，优于基线规划器

Abstract: Trajectory generation for visually impaired scenarios requires smooth and temporally consistent state in structured, low-speed dynamic environments. However, traditional jerk-based heuristic trajectory sampling with independent segment generation and conventional smoothness penalties often lead to unstable terminal behavior and state discontinuities under frequent regenerating. This paper proposes a trajectory generation approach that integrates endpoint regulation to stabilize terminal states within each segment and momentum-aware dynamics to regularize the evolution of velocity and acceleration for segment consistency. Endpoint regulation is incorporated into trajectory sampling to stabilize terminal behavior, while a momentum-aware dynamics enforces consistent velocity and acceleration evolution across consecutive trajectory segments. Experimental results demonstrate reduced acceleration peaks and lower jerk levels with decreased dispersion, smoother velocity and acceleration profiles, more stable endpoint distributions, and fewer infeasible trajectory candidates compared with a baseline planner.

</details>


### [19] [LessMimic: Long-Horizon Humanoid Interaction with Unified Distance Field Representations](https://arxiv.org/abs/2602.21723)
*Yutang Lin,Jieming Cui,Yixuan Li,Baoxiong Jia,Yixin Zhu,Siyuan Huang*

Main category: cs.RO

TL;DR: LessMimic提出了一种基于距离场(DF)的统一交互表示方法，使人形机器人能够在无需参考运动的情况下实现几何泛化、长时程技能组合和视觉部署。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖参考运动或任务特定奖励，导致策略与特定物体几何紧密耦合，无法在单一框架内实现多技能泛化。需要一种统一的交互表示来实现无参考推理、几何泛化和长时程技能组合。

Method: 基于距离场(DF)的几何线索（表面距离、梯度和速度分解）来调节单一全身策略，使用变分自编码器(VAE)编码交互潜在变量，并通过对抗交互先验(AIP)在强化学习(RL)中进行后训练。通过DAgger式蒸馏将DF潜在变量与自我中心深度特征对齐，实现纯视觉部署。

Result: 单个LessMimic策略在物体尺度0.4x到1.6x范围内实现80-100%成功率（PickUp和SitStand任务），在5个任务实例轨迹上达到62.1%成功率，并能处理最多40个顺序组合任务，显著优于基线方法。

Conclusion: 通过将交互建立在局部几何而非演示基础上，LessMimic为人形机器人在非结构化环境中实现泛化、技能组合和故障恢复提供了可扩展的路径。

Abstract: Humanoid robots that autonomously interact with physical environments over extended horizons represent a central goal of embodied intelligence. Existing approaches rely on reference motions or task-specific rewards, tightly coupling policies to particular object geometries and precluding multi-skill generalization within a single framework. A unified interaction representation enabling reference-free inference, geometric generalization, and long-horizon skill composition within one policy remains an open challenge. Here we show that Distance Field (DF) provides such a representation: LessMimic conditions a single whole-body policy on DF-derived geometric cues--surface distances, gradients, and velocity decompositions--removing the need for motion references, with interaction latents encoded via a Variational Auto-Encoder (VAE) and post-trained using Adversarial Interaction Priors (AIP) under Reinforcement Learning (RL). Through DAgger-style distillation that aligns DF latents with egocentric depth features, LessMimic further transfers seamlessly to vision-only deployment without motion capture (MoCap) infrastructure. A single LessMimic policy achieves 80--100% success across object scales from 0.4x to 1.6x on PickUp and SitStand where baselines degrade sharply, attains 62.1% success on 5 task instances trajectories, and remains viable up to 40 sequentially composed tasks. By grounding interaction in local geometry rather than demonstrations, LessMimic offers a scalable path toward humanoid robots that generalize, compose skills, and recover from failures in unstructured environments.

</details>


### [20] [Joint-Aligned Latent Action: Towards Scalable VLA Pretraining in the Wild](https://arxiv.org/abs/2602.21736)
*Hao Luo,Ye Wang,Wanpeng Zhang,Haoqi Yuan,Yicheng Feng,Haiweng Xu,Sipeng Zheng,Zongqing Lu*

Main category: cs.RO

TL;DR: JALA框架通过联合对齐潜在动作，利用混合人类视频数据（实验室+野外）进行VLA预训练，无需完整视觉动态重建，显著提升机器人操作性能。


<details>
  <summary>Details</summary>
Motivation: 当前视觉-语言-动作模型受限于大规模多样化机器人数据的稀缺性。人类操作视频提供了丰富替代资源，但现有方法必须在小型精确标注数据集和大量野外视频（手部跟踪标签不可靠）之间做出选择。

Method: 提出JALA预训练框架，学习联合对齐潜在动作：1) 绕过完整视觉动态重建；2) 学习与逆动力学和真实动作对齐的预测性动作嵌入；3) 创建过渡感知、行为中心的潜在空间；4) 使用UniHand-Mix数据集（750万视频，2000+小时混合实验室和野外素材）。

Result: JALA在受控和非约束场景中生成更真实的手部动作，在仿真和真实世界任务中显著提升下游机器人操作性能。联合对齐潜在动作为从人类数据进行VLA预训练提供了可扩展路径。

Conclusion: 联合对齐潜在动作框架能够有效利用混合人类视频数据进行视觉-语言-动作模型预训练，为机器人操作学习提供了可扩展且性能优越的解决方案。

Abstract: Despite progress, Vision-Language-Action models (VLAs) are limited by a scarcity of large-scale, diverse robot data. While human manipulation videos offer a rich alternative, existing methods are forced to choose between small, precisely-labeled datasets and vast in-the-wild footage with unreliable hand tracking labels. We present JALA, a pretraining framework that learns Jointly-Aligned Latent Actions. JALA bypasses full visual dynamic reconstruction, instead learns a predictive action embedding aligned with both inverse dynamics and real actions. This yields a transition-aware, behavior-centric latent space for learning from heterogeneous human data. We scale this approach with UniHand-Mix, a 7.5M video corpus (>2,000 hours) blending laboratory and in-the-wild footage. Experiments demonstrate that JALA generates more realistic hand motions in both controlled and unconstrained scenarios, significantly improving downstream robot manipulation performance in both simulation and real-world tasks. These results indicate that jointly-aligned latent actions offer a scalable pathway for VLA pretraining from human data.

</details>


### [21] [Therapist-Robot-Patient Physical Interaction is Worth a Thousand Words: Enabling Intuitive Therapist Guidance via Remote Haptic Control](https://arxiv.org/abs/2602.21783)
*Beatrice Luciani,Alex van den Berg,Matti Lang,Alexandre L. Ratschat,Laura Marchal-Crespo*

Main category: cs.RO

TL;DR: 研究人员开发了一种触觉远程操作系统，让训练师可以通过手持触觉设备远程引导佩戴臂部外骨骼的学员，相比传统视觉演示，该系统显著减少了运动完成时间、提高了平滑度，并减少了口头指令需求。


<details>
  <summary>Details</summary>
Motivation: 当前机器人系统在物理引导运动训练中的应用受到限制，部分原因是训练师与学员之间的交互不够直观。为了填补这一空白，研究人员希望开发一种更直观的远程人机物理交互系统。

Method: 研究人员提出了一种触觉远程操作系统，训练师可以通过商用手持触觉设备与外骨骼的肘部和腕部虚拟接触点进行物理交互，从而直观地引导学员。32名参与者测试了该系统，比较了触觉演示与传统视觉演示在引导学员执行臂部姿势方面的效果。

Result: 定量分析显示，触觉演示显著减少了运动完成时间并提高了平滑度。使用大型语言模型进行语音分析发现口头指令更少。触觉演示没有导致训练师报告更高的心理和身体负担，同时训练师报告了更强的能力感，学员报告了更低的体力需求。

Conclusion: 研究结果支持所提出的远程人机物理交互接口的可行性。未来工作应评估其在临床人群中的可用性和有效性，以恢复临床医生在机器人辅助治疗中的能动感。

Abstract: Robotic systems can enhance the amount and repeatability of physically guided motor training. Yet their real-world adoption is limited, partly due to non-intuitive trainer/therapist-trainee/patient interactions. To address this gap, we present a haptic teleoperation system for trainers to remotely guide and monitor the movements of a trainee wearing an arm exoskeleton. The trainer can physically interact with the exoskeleton through a commercial handheld haptic device via virtual contact points at the exoskeleton's elbow and wrist, allowing intuitive guidance. Thirty-two participants tested the system in a trainer-trainee paradigm, comparing our haptic demonstration system with conventional visual demonstration in guiding trainees in executing arm poses. Quantitative analyses showed that haptic demonstration significantly reduced movement completion time and improved smoothness, while speech analysis using large language models for automated transcription and categorization of verbal commands revealed fewer verbal instructions. The haptic demonstration did not result in higher reported mental and physical effort by trainers compared to the visual demonstration, while trainers reported greater competence and trainees lower physical demand. These findings support the feasibility of our proposed interface for effective remote human-robot physical interaction. Future work should assess its usability and efficacy for clinical populations in restoring clinicians' sense of agency during robot-assisted therapy.

</details>


### [22] [DexRepNet++: Learning Dexterous Robotic Manipulation with Geometric and Spatial Hand-Object Representations](https://arxiv.org/abs/2602.21811)
*Qingtao Liu,Zhengnan Sun,Yu Cui,Haoming Li,Gaofeng Li,Lin Shao,Jiming Chen,Qi Ye*

Main category: cs.RO

TL;DR: DexRep是一种新颖的手-物交互表示方法，通过捕捉物体表面特征和手物空间关系来提升灵巧操作策略的泛化能力，在抓取、手内重定向和双手交接任务中显著超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有深度强化学习方法主要关注高维动作空间的样本效率，但忽视了表示学习在复杂手-物交互输入空间中对策略泛化能力的重要作用。

Method: 提出DexRep表示方法，专门设计用于捕捉物体表面特征和手物之间的空间关系，基于此表示学习灵巧操作策略。

Result: 在抓取任务中，仅用40个物体训练的策略在5000多个未见物体上达到87.9%成功率；在手内重定向和交接任务中，相比现有表示方法提升20-40%成功率；真实世界部署显示较小的仿真到现实差距。

Conclusion: DexRep表示方法能有效提升灵巧操作策略的泛化能力，在多种任务中显著超越现有方法，并具有良好的仿真到现实迁移性能。

Abstract: Robotic dexterous manipulation is a challenging problem due to high degrees of freedom (DoFs) and complex contacts of multi-fingered robotic hands. Many existing deep reinforcement learning (DRL) based methods aim at improving sample efficiency in high-dimensional output action spaces. However, existing works often overlook the role of representations in achieving generalization of a manipulation policy in the complex input space during the hand-object interaction. In this paper, we propose DexRep, a novel hand-object interaction representation to capture object surface features and spatial relations between hands and objects for dexterous manipulation skill learning. Based on DexRep, policies are learned for three dexterous manipulation tasks, i.e. grasping, in-hand reorientation, bimanual handover, and extensive experiments are conducted to verify the effectiveness. In simulation, for grasping, the policy learned with 40 objects achieves a success rate of 87.9% on more than 5000 unseen objects of diverse categories, significantly surpassing existing work trained with thousands of objects; for the in-hand reorientation and handover tasks, the policies also boost the success rates and other metrics of existing hand-object representations by 20% to 40%. The grasp policies with DexRep are deployed to the real world under multi-camera and single-camera setups and demonstrate a small sim-to-real gap.

</details>


### [23] [Self-Curriculum Model-based Reinforcement Learning for Shape Control of Deformable Linear Objects](https://arxiv.org/abs/2602.21816)
*Zhaowei Liang,Song Wang,Zhao Jin,Shirui Wu,Dan Wu*

Main category: cs.RO

TL;DR: 提出了一种结合强化学习和视觉伺服的DLO形状控制两阶段框架，通过模型强化学习提高样本效率，并设计自课程目标生成机制优化学习过程，实现了从仿真到真实环境的零样本迁移。


<details>
  <summary>Details</summary>
Motivation: 当前DLO形状控制方法在处理复杂大变形任务（特别是包含相反曲率的情况）时面临挑战，缺乏效率和精度，需要更有效的解决方案。

Method: 采用两阶段框架：1）大变形阶段使用基于模型的强化学习，采用动态模型集成提高样本效率，设计自课程目标生成机制动态选择中间难度目标；2）小变形阶段使用基于雅可比矩阵的视觉伺服控制器确保高精度收敛。

Result: 仿真结果显示该方法能高效学习策略，在形状控制成功率和精度上显著优于主流基线。框架成功将仿真训练的策略零样本迁移到真实世界任务，在30个不同初始和目标形状的案例中全部成功，适用于不同尺寸和材料的DLO。

Conclusion: 提出的两阶段框架有效解决了DLO复杂形状控制问题，结合强化学习和视觉伺服的优势，实现了高效学习和精确控制，并展示了良好的仿真到真实环境的迁移能力。

Abstract: Precise shape control of Deformable Linear Objects (DLOs) is crucial in robotic applications such as industrial and medical fields. However, existing methods face challenges in handling complex large deformation tasks, especially those involving opposite curvatures, and lack efficiency and precision. To address this, we propose a two-stage framework combining Reinforcement Learning (RL) and online visual servoing. In the large-deformation stage, a model-based reinforcement learning approach using an ensemble of dynamics models is introduced to significantly improve sample efficiency. Additionally, we design a self-curriculum goal generation mechanism that dynamically selects intermediate-difficulty goals with high diversity through imagined evaluations, thereby optimizing the policy learning process. In the small-deformation stage, a Jacobian-based visual servo controller is deployed to ensure high-precision convergence. Simulation results show that the proposed method enables efficient policy learning and significantly outperforms mainstream baselines in shape control success rate and precision. Furthermore, the framework effectively transfers the policy trained in simulation to real-world tasks with zero-shot adaptation. It successfully completes all 30 cases with diverse initial and target shapes across DLOs of different sizes and materials. The project website is available at: https://anonymous.4open.science/w/sc-mbrl-dlo-EB48/

</details>


### [24] [Dream-SLAM: Dreaming the Unseen for Active SLAM in Dynamic Environments](https://arxiv.org/abs/2602.21967)
*Xiangqi Meng,Pengxu Hou,Zhenjun Zhao,Javier Civera,Daniel Cremers,Hesheng Wang,Haoang Li*

Main category: cs.RO

TL;DR: Dream-SLAM：一种基于跨时空图像生成和语义结构预测的单目主动SLAM方法，通过融合生成图像与真实观测提升定位和建图精度，并支持长时程规划


<details>
  <summary>Details</summary>
Motivation: 现有主动SLAM方法存在三个主要限制：1) 受限于底层SLAM模块的限制；2) 运动规划策略短视，缺乏长期视野；3) 难以处理动态场景。需要一种能克服这些限制的新方法。

Method: 提出Dream-SLAM方法，基于对部分观测动态环境的跨时空图像生成和语义合理结构预测。将生成的跨时空图像与真实观测融合以减少噪声和数据不完整性，从而提高相机姿态估计精度和3D场景表示一致性。同时整合生成和观测的场景结构以实现长时程规划。

Result: 在公开数据集和自采集数据集上的大量实验表明，Dream-SLAM在定位精度、建图质量和探索效率方面均优于现有最先进方法。

Conclusion: Dream-SLAM通过跨时空图像生成和语义结构预测，有效解决了传统主动SLAM的局限性，实现了更准确的定位、更高质量的建图和更高效的探索。

Abstract: In addition to the core tasks of simultaneous localization and mapping (SLAM), active SLAM additionally in- volves generating robot actions that enable effective and efficient exploration of unknown environments. However, existing active SLAM pipelines are limited by three main factors. First, they inherit the restrictions of the underlying SLAM modules that they may be using. Second, their motion planning strategies are typically shortsighted and lack long-term vision. Third, most approaches struggle to handle dynamic scenes. To address these limitations, we propose a novel monocular active SLAM method, Dream-SLAM, which is based on dreaming cross-spatio-temporal images and semantically plausible structures of partially observed dynamic environments. The generated cross-spatio-temporal im- ages are fused with real observations to mitigate noise and data incompleteness, leading to more accurate camera pose estimation and a more coherent 3D scene representation. Furthermore, we integrate dreamed and observed scene structures to enable long- horizon planning, producing farsighted trajectories that promote efficient and thorough exploration. Extensive experiments on both public and self-collected datasets demonstrate that Dream-SLAM outperforms state-of-the-art methods in localization accuracy, mapping quality, and exploration efficiency. Source code will be publicly available upon paper acceptance.

</details>


### [25] [Humanizing Robot Gaze Shifts: A Framework for Natural Gaze Shifts in Humanoid Robots](https://arxiv.org/abs/2602.21983)
*Jingchao Wei,Jingkai Qin,Yuxiao Cao,Jingcheng Huang,Xiangrui Zeng,Min Li,Zhouping Yin*

Main category: cs.RO

TL;DR: RGS框架整合认知注意力机制与仿生运动生成，通过VLM推理上下文合适的注视目标，并使用条件VQ-VAE生成眼头协调的注视转移动作，实现人机交互中自然多样的注视行为。


<details>
  <summary>Details</summary>
Motivation: 在无约束的人机交互中，使人形机器人能够执行自然且上下文适当的注视转移仍然具有挑战性，因为这需要将认知注意力机制与仿生运动生成相结合。

Method: 提出Robot Gaze-Shift (RGS)框架，包含两个组件：1) 基于视觉-语言模型(VLM)的注视推理管道，从多模态交互线索推断上下文合适的注视目标；2) 条件向量量化变分自编码器(VQ-VAE)模型，用于眼头协调的注视转移运动生成。

Result: 实验验证RGS能够有效复制人类目标选择行为，并生成真实、多样的注视转移动作。

Conclusion: RGS框架成功整合了认知注意力推理和仿生运动生成，为人机交互中自然、上下文适当的注视转移提供了有效解决方案。

Abstract: Leveraging auditory and visual feedback for attention reorientation is essential for natural gaze shifts in social interaction. However, enabling humanoid robots to perform natural and context-appropriate gaze shifts in unconstrained human--robot interaction (HRI) remains challenging, as it requires the coupling of cognitive attention mechanisms and biomimetic motion generation. In this work, we propose the Robot Gaze-Shift (RGS) framework, which integrates these two components into a unified pipeline. First, RGS employs a vision--language model (VLM)-based gaze reasoning pipeline to infer context-appropriate gaze targets from multimodal interaction cues, ensuring consistency with human gaze-orienting regularities. Second, RGS introduces a conditional Vector Quantized-Variational Autoencoder (VQ-VAE) model for eye--head coordinated gaze-shift motion generation, producing diverse and human-like gaze-shift behaviors. Experiments validate that RGS effectively replicates human-like target selection and generates realistic, diverse gaze-shift motions.

</details>


### [26] [Parallel Continuous-Time Relative Localization with Augmented Clamped Non-Uniform B-Splines](https://arxiv.org/abs/2602.22006)
*Jiadong Lu,Zhehan Li,Tao Han,Miao Xu,Chao Xu,Yanjun Cao*

Main category: cs.RO

TL;DR: CT-RIO：一种新颖的连续时间相对惯性里程计框架，采用夹紧非均匀B样条消除查询延迟，通过并行优化实现机器人群体中的高精度、低延迟相对定位


<details>
  <summary>Details</summary>
Motivation: 多机器人协作需要精确的相对定位，但现有连续时间方法在处理异步测量时存在查询延迟和高计算成本的问题，难以满足机器人群体对高精度、低延迟和高频率性能的需求

Method: 首次采用夹紧非均匀B样条表示机器人状态以消除查询延迟，引入闭式扩展和收缩操作实现灵活的节点管理，提出节点-关键节点策略支持高频扩展，并设计滑动窗口相对定位问题，通过增量异步块坐标下降法并行求解机器人级子问题

Result: CT-RIO能够从高达263毫秒的时间偏移在3秒内收敛到亚毫秒级别，实现0.046米和1.8度的RMSE，在高速度运动下相比现有方法提升达60%

Conclusion: CT-RIO通过创新的B样条表示和并行优化架构，成功解决了多机器人系统中连续时间相对定位的挑战，为机器人群体协作提供了高精度、低延迟的解决方案

Abstract: Accurate relative localization is critical for multi-robot cooperation. In robot swarms, measurements from different robots arrive asynchronously and with clock time-offsets. Although Continuous-Time (CT) formulations have proved effective for handling asynchronous measurements in single-robot SLAM and calibration, extending CT methods to multi-robot settings faces great challenges to achieve high-accuracy, low-latency, and high-frequency performance. Especially, existing CT methods suffer from the inherent query-time delay of unclamped B-splines and high computational cost. This paper proposes CT-RIO, a novel Continuous-Time Relative-Inertial Odometry framework. We employ Clamped Non-Uniform B-splines (C-NUBS) to represent robot states for the first time, eliminating the query-time delay. We further augment C-NUBS with closed-form extension and shrinkage operations that preserve the spline shape, making it suitable for online estimation and enabling flexible knot management. This flexibility leads to the concept of knot-keyknot strategy, which supports spline extension at high-frequency while retaining sparse keyknots for adaptive relative-motion modeling. We then formulate a sliding-window relative localization problem that operates purely on relative kinematics and inter-robot constraints. To meet the demanding computation required at swarm scale, we decompose the tightly-coupled optimization into robot-wise sub-problems and solve them in parallel using incremental asynchronous block coordinate descent. Extensive experiments show that CT-RIO converges from time-offsets as large as 263 ms to sub-millisecond within 3 s, and achieves RMSEs of 0.046 m and 1.8 °. It consistently outperforms state-of-the-art methods, with improvements of up to 60% under high-speed motion.

</details>


### [27] [World Guidance: World Modeling in Condition Space for Action Generation](https://arxiv.org/abs/2602.22010)
*Yue Su,Sijin Chen,Haixin Shi,Mingyu Liu,Zhengshen Zhang,Ningyuan Huang,Weiheng Zhong,Zhengbang Zhu,Yuxiao Liu,Xihui Liu*

Main category: cs.RO

TL;DR: 提出WoG框架，通过将未来观测映射为紧凑条件并注入动作推理流程，在条件空间中实现有效的世界建模，从而提升VLA模型的细粒度动作生成能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以在保持高效、可预测的未来表示与保留足够细粒度信息以指导精确动作生成之间取得平衡，这限制了VLA模型的动作生成能力。

Method: 提出WoG框架，将未来观测映射为紧凑条件并注入动作推理流程，训练VLA模型同时预测这些压缩条件和未来动作，在条件空间中实现世界建模。

Result: 该方法不仅促进了细粒度动作生成，还展现出优越的泛化能力，并能从大量人类操作视频中有效学习。在仿真和真实环境中的实验验证了其显著优于基于未来预测的现有方法。

Conclusion: WoG框架通过在条件空间中建模未来观测，成功平衡了表示效率与信息保留，为VLA模型的动作生成提供了有效解决方案。

Abstract: Leveraging future observation modeling to facilitate action generation presents a promising avenue for enhancing the capabilities of Vision-Language-Action (VLA) models. However, existing approaches struggle to strike a balance between maintaining efficient, predictable future representations and preserving sufficient fine-grained information to guide precise action generation. To address this limitation, we propose WoG (World Guidance), a framework that maps future observations into compact conditions by injecting them into the action inference pipeline. The VLA is then trained to simultaneously predict these compressed conditions alongside future actions, thereby achieving effective world modeling within the condition space for action inference. We demonstrate that modeling and predicting this condition space not only facilitates fine-grained action generation but also exhibits superior generalization capabilities. Moreover, it learns effectively from substantial human manipulation videos. Extensive experiments across both simulation and real-world environments validate that our method significantly outperforms existing methods based on future prediction. Project page is available at: https://selen-suyue.github.io/WoGNet/

</details>


### [28] [System Design of the Ultra Mobility Vehicle: A Driving, Balancing, and Jumping Bicycle Robot](https://arxiv.org/abs/2602.22118)
*Benjamin Bokser,Daniel Gonzalez,Surya Singh,Aaron Preston,Alex Bahner,Annika Wollschläger,Arianna Ilvonen,Asa Eckert-Erdheim,Ashwin Khadke,Bilal Hammoud,Dean Molinaro,Fabian Jenelten,Henry Mayne,Howie Choset,Igor Bogoslavskyi,Itic Tinman,James Tigue,Jan Preisig,Kaiyu Zheng,Kenny Sharma,Kim Ang,Laura Lee,Liana Margolese,Nicole Lin,Oscar Frias,Paul Drews,Ravi Boggavarapu,Rick Burnham,Samuel Zapolsky,Sangbae Kim,Scott Biddlestone,Sean Mayorga,Shamel Fahmi,Tyler McCollum,Velin Dimitrov,William Moyne,Yu-Ming Chen,Farbod Farshidian,Marco Hutter,David Perry,Al Rizzi,Gabe Nelson*

Main category: cs.RO

TL;DR: 研究人员受自行车运动员启发，开发了一种结合自行车和反作用质量的机器人平台UMV，能够实现动态运动、跳跃和单轮平衡等多种运动技能。


<details>
  <summary>Details</summary>
Motivation: 受自行车和山地车运动员能够跳跃、平衡、单轮行驶等多样化运动能力的启发，研究人员希望开发一个具有类似运动能力的机器人平台，能够在平滑地形上实现高速高效运动，同时在崎岖地形上保持敏捷性。

Method: 采用仿真驱动的设计优化过程，合成空间连杆拓扑结构，重点优化垂直跳跃高度和基于动量的单轮接触平衡。使用约束强化学习框架，实现多种运动行为的零样本迁移。

Result: 开发出23.5公斤的UMV机器人，能够实现多种运动技能：定点平衡、跳跃、翘头行驶、后轮跳跃和前空翻。该机器人能够达到8米/秒的高速，并能跳跃1米高的障碍物（相当于机器人标称高度的130%）。

Conclusion: 通过结合自行车设计和反作用质量，配合仿真优化和强化学习控制，成功开发出具有高度动态运动能力的机器人平台，展示了在最小驱动自由度下实现多样化运动行为的可行性。

Abstract: Trials cyclists and mountain bike riders can hop, jump, balance, and drive on one or both wheels. This versatility allows them to achieve speed and energy-efficiency on smooth terrain and agility over rough terrain. Inspired by these athletes, we present the design and control of a robotic platform, Ultra Mobility Vehicle (UMV), which combines a bicycle and a reaction mass to move dynamically with minimal actuated degrees of freedom. We employ a simulation-driven design optimization process to synthesize a spatial linkage topology with a focus on vertical jump height and momentum-based balancing on a single wheel contact. Using a constrained Reinforcement Learning (RL) framework, we demonstrate zero-shot transfer of diverse athletic behaviors, including track-stands, jumps, wheelies, rear wheel hopping, and front flips. This 23.5 kg robot is capable of high speeds (8 m/s) and jumping on and over large obstacles (1 m tall, or 130% of the robot's nominal height).

</details>


### [29] [Position-Based Flocking for Persistent Alignment without Velocity Sensing](https://arxiv.org/abs/2602.22154)
*Hossein B. Jond,Veli Bakırcıoğlu,Logan E. Beaver,Nejat Tükenmez,Adel Akbarimajd,Martin Saska*

Main category: cs.RO

TL;DR: 提出一种基于位置的集群模型，无需速度传感即可实现持续速度对齐，适用于速度测量不可靠的真实机器人集群


<details>
  <summary>Details</summary>
Motivation: 鸟类和鱼群的协调集体运动启发了机器人集群算法，但真实机器人集群中速度测量往往不可靠、有噪声或不可用，需要不依赖速度传感的集群模型

Method: 通过当前与初始相对位置的变化近似相对速度差，结合时间和密度相关的对齐增益（具有非零最小阈值）来维持持续对齐，实现基于位置的集群

Result: 50个智能体的仿真显示，该模型比基于速度对齐的基线方法获得更快、更持续的方向对齐，并形成更紧凑的编队；9个真实轮式移动机器人的实验也验证了有效性

Conclusion: 基于位置的集群模型能够在没有速度传感的情况下实现持续的速度对齐和连贯的集体运动，特别适合速度测量不可靠的真实世界机器人集群应用

Abstract: Coordinated collective motion in bird flocks and fish schools inspires algorithms for cohesive swarm robotics. This paper presents a position-based flocking model that achieves persistent velocity alignment without velocity sensing. By approximating relative velocity differences from changes between current and initial relative positions and incorporating a time- and density-dependent alignment gain with a non-zero minimum threshold to maintain persistent alignment, the model sustains coherent collective motion over extended periods. Simulations with a collective of 50 agents demonstrate that the position-based flocking model attains faster and more sustained directional alignment and results in more compact formations than a velocity-alignment-based baseline. This position-based flocking model is particularly well-suited for real-world robotic swarms, where velocity measurements are unreliable, noisy, or unavailable. Experimental results using a team of nine real wheeled mobile robots are also presented.

</details>
