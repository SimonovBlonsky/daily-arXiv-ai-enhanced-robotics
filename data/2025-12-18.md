<div id=toc></div>

# Table of Contents

- [cs.RO](#cs.RO) [Total: 15]


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [1] [Breathe with Me: Synchronizing Biosignals for User Embodiment in Robots](https://arxiv.org/abs/2512.14952)
*Iddo Yehoshua Wald,Amber Maimon,Shiyao Zhang,Dennis Küster,Robert Porzel,Tanja Schultz,Rainer Malaka*

Main category: cs.RO

TL;DR: 通过将用户呼吸实时映射到机器人动作（呼吸同步），增强用户在机器人系统中的具身体验，显著提高身体所有权感


<details>
  <summary>Details</summary>
Motivation: 探索如何通过生理信号（特别是呼吸）作为新的内感受通路，增强人机交互中的具身体验，超越传统的视觉运动反馈

Method: 采用被试内实验设计，参与者控制机械臂，其运动与用户呼吸同步或不同步，测量身体所有权感和用户偏好

Result: 呼吸同步显著提高了身体所有权感，大多数参与者更喜欢同步条件

Conclusion: 生理信号（如呼吸）可作为增强人机交互中具身体验的新途径，对远程呈现、假肢、机器人协作和共享自主性有重要应用价值

Abstract: Embodiment of users within robotic systems has been explored in human-robot interaction, most often in telepresence and teleoperation. In these applications, synchronized visuomotor feedback can evoke a sense of body ownership and agency, contributing to the experience of embodiment. We extend this work by employing embreathment, the representation of the user's own breath in real time, as a means for enhancing user embodiment experience in robots. In a within-subjects experiment, participants controlled a robotic arm, while its movements were either synchronized or non-synchronized with their own breath. Synchrony was shown to significantly increase body ownership, and was preferred by most participants. We propose the representation of physiological signals as a novel interoceptive pathway for human-robot interaction, and discuss implications for telepresence, prosthetics, collaboration with robots, and shared autonomy.

</details>


### [2] [ISS Policy : Scalable Diffusion Policy with Implicit Scene Supervision](https://arxiv.org/abs/2512.15020)
*Wenlong Xia,Jinhao Zhang,Ce Zhang,Yaojia Wang,Youmin Gong,Jie Mei*

Main category: cs.RO

TL;DR: ISS Policy是一种基于3D点云观察的扩散策略，通过隐式场景监督模块提升视觉模仿学习的训练效率和泛化能力，在单臂和灵巧手操作任务上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 传统基于视觉的模仿学习过度依赖物体外观而忽略底层3D场景结构，导致训练效率低、泛化能力差。需要开发能够利用3D几何信息的方法来提升策略的性能和鲁棒性。

Method: 提出ISS Policy：基于DiT的3D视觉运动扩散策略，从点云观察预测连续动作序列。核心创新是隐式场景监督模块，鼓励模型输出与场景几何演化一致，从而提升策略性能。

Result: 在单臂操作任务（MetaWorld）和灵巧手操作（Adroit）上达到最先进性能。真实世界实验显示强大的泛化能力和鲁棒性。消融研究表明方法在数据和参数规模上都能有效扩展。

Conclusion: ISS Policy通过隐式场景监督有效解决了传统视觉模仿学习的局限性，在3D场景理解、训练效率和泛化能力方面取得显著改进，为机器人操作提供了更强大的解决方案。

Abstract: Vision-based imitation learning has enabled impressive robotic manipulation skills, but its reliance on object appearance while ignoring the underlying 3D scene structure leads to low training efficiency and poor generalization. To address these challenges, we introduce \emph{Implicit Scene Supervision (ISS) Policy}, a 3D visuomotor DiT-based diffusion policy that predicts sequences of continuous actions from point cloud observations. We extend DiT with a novel implicit scene supervision module that encourages the model to produce outputs consistent with the scene's geometric evolution, thereby improving the performance and robustness of the policy. Notably, ISS Policy achieves state-of-the-art performance on both single-arm manipulation tasks (MetaWorld) and dexterous hand manipulation (Adroit). In real-world experiments, it also demonstrates strong generalization and robustness. Additional ablation studies show that our method scales effectively with both data and parameters. Code and videos will be released.

</details>


### [3] [HERO: Hierarchical Traversable 3D Scene Graphs for Embodied Navigation Among Movable Obstacles](https://arxiv.org/abs/2512.15047)
*Yunheng Wang,Yixiao Feng,Yuetong Fang,Shuning Zhang,Tan Jing,Jian Li,Xiangrui Jiang,Renjing Xu*

Main category: cs.RO

TL;DR: HERO框架通过构建层次化可遍历3D场景图，将可操作障碍物重新定义为通路，显著提升了智能体在复杂环境中的导航效率和可达性。


<details>
  <summary>Details</summary>
Motivation: 现有3D场景图导航方法基于静态世界假设，将可交互障碍物视为不可穿越，导致在真实场景中可达性有限、效率低下、扩展性差。需要一种能建模障碍物物理交互性和功能语义的新方法。

Method: 提出HERO框架，构建层次化可遍历3D场景图，重新定义可遍历性：将可操作障碍物建模为通路，捕捉其物理交互性、功能语义和场景关系层次结构。

Result: 相比基线方法，HERO在部分阻塞环境中减少路径长度35.1%，在完全阻塞环境中提高成功率79.4%，显著提升了导航效率和可达性。

Conclusion: HERO通过建模障碍物的交互性和功能语义，克服了传统静态世界假设的限制，为智能体在复杂真实环境中的导航提供了更有效的表示和规划能力。

Abstract: 3D Scene Graphs (3DSGs) constitute a powerful representation of the physical world, distinguished by their abilities to explicitly model the complex spatial, semantic, and functional relationships between entities, rendering a foundational understanding that enables agents to interact intelligently with their environment and execute versatile behaviors. Embodied navigation, as a crucial component of such capabilities, leverages the compact and expressive nature of 3DSGs to enable long-horizon reasoning and planning in complex, large-scale environments. However, prior works rely on a static-world assumption, defining traversable space solely based on static spatial layouts and thereby treating interactable obstacles as non-traversable. This fundamental limitation severely undermines their effectiveness in real-world scenarios, leading to limited reachability, low efficiency, and inferior extensibility. To address these issues, we propose HERO, a novel framework for constructing Hierarchical Traversable 3DSGs, that redefines traversability by modeling operable obstacles as pathways, capturing their physical interactivity, functional semantics, and the scene's relational hierarchy. The results show that, relative to its baseline, HERO reduces PL by 35.1% in partially obstructed environments and increases SR by 79.4% in fully obstructed ones, demonstrating substantially higher efficiency and reachability.

</details>


### [4] [NAP3D: NeRF Assisted 3D-3D Pose Alignment for Autonomous Vehicles](https://arxiv.org/abs/2512.15080)
*Gaurav Bansal*

Main category: cs.RO

TL;DR: NAP3D是一种基于NeRF的3D-3D位姿对齐方法，通过将当前深度图像与预训练NeRF生成的3D点云对齐来修正位姿估计误差，无需回环检测，在长距离环境中表现鲁棒。


<details>
  <summary>Details</summary>
Motivation: 自主车辆定位中，传感器噪声和时间漂移会导致显著的位姿估计误差，特别是在长距离环境中。传统的视觉回环检测方法依赖于识别当前视图与先前观察场景之间的视觉映射，通常需要融合多个传感器数据，且需要在已访问位置进行回环检测。

Method: 提出NeRF-Assisted 3D-3D Pose Alignment (NAP3D)方法，利用智能体当前深度图像与预训练神经辐射场(NeRF)之间的3D-3D对应关系。通过直接将观察场景的3D点与NeRF合成的3D点对齐，即使在新的视角下也能细化估计位姿，无需依赖回访先前观察位置。

Result: 在自定义数据集上，NAP3D实现了5厘米内的相机位姿校正，鲁棒地优于2D-3D Perspective-N-Point基线方法。在TUM RGB-D数据集上，NAP3D在不同噪声条件下持续将3D对齐RMSE提高约6厘米，尽管PnP在某些情况下获得更低的原始旋转和平移参数误差，但NAP3D在3D空间中表现出更好的几何一致性。

Conclusion: NAP3D提供了一种轻量级、数据集无关的工具，当传统回环检测不可用时，可以补充现有的SLAM和定位流程。其3D-3D公式相比传统2D-3D定位方法具有优势，同时在准确性和适用性方面保持可比性。

Abstract: Accurate localization is essential for autonomous vehicles, yet sensor noise and drift over time can lead to significant pose estimation errors, particularly in long-horizon environments. A common strategy for correcting accumulated error is visual loop closure in SLAM, which adjusts the pose graph when the agent revisits previously mapped locations. These techniques typically rely on identifying visual mappings between the current view and previously observed scenes and often require fusing data from multiple sensors.
  In contrast, this work introduces NeRF-Assisted 3D-3D Pose Alignment (NAP3D), a complementary approach that leverages 3D-3D correspondences between the agent's current depth image and a pre-trained Neural Radiance Field (NeRF). By directly aligning 3D points from the observed scene with synthesized points from the NeRF, NAP3D refines the estimated pose even from novel viewpoints, without relying on revisiting previously observed locations.
  This robust 3D-3D formulation provides advantages over conventional 2D-3D localization methods while remaining comparable in accuracy and applicability. Experiments demonstrate that NAP3D achieves camera pose correction within 5 cm on a custom dataset, robustly outperforming a 2D-3D Perspective-N-Point baseline. On TUM RGB-D, NAP3D consistently improves 3D alignment RMSE by approximately 6 cm compared to this baseline given varying noise, despite PnP achieving lower raw rotation and translation parameter error in some regimes, highlighting NAP3D's improved geometric consistency in 3D space. By providing a lightweight, dataset-agnostic tool, NAP3D complements existing SLAM and localization pipelines when traditional loop closure is unavailable.

</details>


### [5] [BEV-Patch-PF: Particle Filtering with BEV-Aerial Feature Matching for Off-Road Geo-Localization](https://arxiv.org/abs/2512.15111)
*Dongmyeong Lee,Jesse Quattrociocchi,Christian Ellis,Rwik Rana,Amanda Adkins,Adam Uccello,Garrett Warnell,Joydeep Biswas*

Main category: cs.RO

TL;DR: BEV-Patch-PF：一种无需GPS的序列化地理定位系统，通过粒子滤波器结合鸟瞰视图特征图和航空特征图，在越野环境中实现高精度实时定位


<details>
  <summary>Details</summary>
Motivation: 解决在无GPS环境（如茂密树冠、阴影区域）下机器人的精确定位问题，传统基于检索的方法精度有限，需要更鲁棒的序列化定位方案

Method: 1) 从机载RGB和深度图像构建BEV特征图；2) 为每个3自由度粒子位姿假设，从局部航空图像查询的航空特征图中裁剪对应补丁；3) 通过匹配BEV特征与航空补丁特征计算每个粒子的对数似然；4) 使用粒子滤波器进行序列化定位

Result: 在两个真实世界越野数据集上：1) 在已见路线上绝对轨迹误差（ATE）比基于检索的基线低7.5倍；2) 在未见路线上ATE低7.0倍；3) 在茂密树冠和阴影下保持精度；4) 在NVIDIA Tesla T4上实时运行（10Hz）

Conclusion: BEV-Patch-PF系统在无GPS环境中实现了高精度、鲁棒的实时定位，显著优于传统方法，适用于实际机器人部署

Abstract: We propose BEV-Patch-PF, a GPS-free sequential geo-localization system that integrates a particle filter with learned bird's-eye-view (BEV) and aerial feature maps. From onboard RGB and depth images, we construct a BEV feature map. For each 3-DoF particle pose hypothesis, we crop the corresponding patch from an aerial feature map computed from a local aerial image queried around the approximate location. BEV-Patch-PF computes a per-particle log-likelihood by matching the BEV feature to the aerial patch feature. On two real-world off-road datasets, our method achieves 7.5x lower absolute trajectory error (ATE) on seen routes and 7.0x lower ATE on unseen routes than a retrieval-based baseline, while maintaining accuracy under dense canopy and shadow. The system runs in real time at 10 Hz on an NVIDIA Tesla T4, enabling practical robot deployment.

</details>


### [6] [EPSM: A Novel Metric to Evaluate the Safety of Environmental Perception in Autonomous Driving](https://arxiv.org/abs/2512.15195)
*Jörg Gamerdinger,Sven Teufel,Stephan Amann,Lukas Marc Listl,Oliver Bringmann*

Main category: cs.RO

TL;DR: 提出一种新的安全度量框架，用于评估自动驾驶感知系统（物体和车道检测）的安全性，弥补传统精度指标的不足


<details>
  <summary>Details</summary>
Motivation: 传统感知评估指标（如精确率、召回率、F1分数）只关注整体检测精度，未考虑安全相关因素。即使在这些指标上得分很高的感知系统，仍可能产生导致严重事故的误检测。因此需要专门的安全评估方法。

Method: 提出一个新颖的安全度量框架，包含：1）轻量级物体安全度量，量化物体检测错误相关的潜在风险；2）车道安全度量，考虑物体和车道检测任务间的相互依赖关系；3）将两者结合形成统一的、可解释的感知安全性能综合评分。

Result: 在DeepAccident数据集上的实验表明，该方法能够识别传统性能指标无法捕捉的安全关键感知错误，验证了安全中心评估方法的重要性。

Conclusion: 感知系统的安全评估至关重要，提出的安全度量框架能够有效评估自动驾驶中物体和车道检测的安全性，为感知系统的安全性能提供了更全面的评估方法。

Abstract: Extensive evaluation of perception systems is crucial for ensuring the safety of intelligent vehicles in complex driving scenarios. Conventional performance metrics such as precision, recall and the F1-score assess the overall detection accuracy, but they do not consider the safety-relevant aspects of perception. Consequently, perception systems that achieve high scores in these metrics may still cause misdetections that could lead to severe accidents. Therefore, it is important to evaluate not only the overall performance of perception systems, but also their safety. We therefore introduce a novel safety metric for jointly evaluating the most critical perception tasks, object and lane detection. Our proposed framework integrates a new, lightweight object safety metric that quantifies the potential risk associated with object detection errors, as well as an lane safety metric including the interdependence between both tasks that can occur in safety evaluation. The resulting combined safety score provides a unified, interpretable measure of perception safety performance. Using the DeepAccident dataset, we demonstrate that our approach identifies safety critical perception errors that conventional performance metrics fail to capture. Our findings emphasize the importance of safety-centric evaluation methods for perception systems in autonomous driving.

</details>


### [7] [Infrastructure-based Autonomous Mobile Robots for Internal Logistics -- Challenges and Future Perspectives](https://arxiv.org/abs/2512.15215)
*Erik Brorsson,Kristian Ceder,Ze Zhang,Sabino Francesco Roselli,Endre Erős,Martin Dahl,Beatrice Alenljung,Jessica Lindblom,Thanh Bui,Emmanuel Dean,Lennart Svensson,Kristofer Bengtsson,Per-Lage Götvall,Knut Åkesson*

Main category: cs.RO

TL;DR: 该论文提出了基于基础设施的自主移动机器人系统参考架构，结合基础设施传感、本地云计算和车载自主性，并在重型车辆制造环境中进行了实际部署和用户体验评估。


<details>
  <summary>Details</summary>
Motivation: 目前大多数自主移动机器人解决方案强调分散式、车载智能，而基于基础设施（外部传感器和计算资源）的系统在文献中研究不足，特别是在复杂工业环境中需要可扩展、鲁棒且人机兼容的AMR系统。

Method: 提出了一个结合基础设施传感、本地云计算和车载自主性的参考架构，并基于该架构回顾了定位、感知和规划等核心技术。在重型车辆制造环境中进行了实际部署，并进行了用户体验评估。

Result: 在真实世界的重型车辆制造环境中成功部署了基于基础设施的AMR系统，并通过用户体验评估总结了相关发现，为未来开发提供了实践基础。

Conclusion: 该研究为未来在复杂工业环境中开发可扩展、鲁棒且人机兼容的AMR系统提供了全面的基础，强调了基础设施支持的重要性。

Abstract: The adoption of Autonomous Mobile Robots (AMRs) for internal logistics is accelerating, with most solutions emphasizing decentralized, onboard intelligence. While AMRs in indoor environments like factories can be supported by infrastructure, involving external sensors and computational resources, such systems remain underexplored in the literature. This paper presents a comprehensive overview of infrastructure-based AMR systems, outlining key opportunities and challenges. To support this, we introduce a reference architecture combining infrastructure-based sensing, on-premise cloud computing, and onboard autonomy. Based on the architecture, we review core technologies for localization, perception, and planning. We demonstrate the approach in a real-world deployment in a heavy-vehicle manufacturing environment and summarize findings from a user experience (UX) evaluation. Our aim is to provide a holistic foundation for future development of scalable, robust, and human-compatible AMR systems in complex industrial environments.

</details>


### [8] [VLA-AN: An Efficient and Onboard Vision-Language-Action Framework for Aerial Navigation in Complex Environments](https://arxiv.org/abs/2512.15258)
*Yuze Wu,Mo Zhu,Xingxing Li,Yuheng Du,Yuxin Fan,Wenjun Li,Xin Zhou,Fei Gao*

Main category: cs.RO

TL;DR: VLA-AN是一个高效的机载视觉-语言-动作框架，用于无人机在复杂环境中的自主导航，解决了现有大型空中导航模型的四个主要限制：数据领域差距、时间导航推理不足、生成动作策略的安全问题以及机载部署约束。


<details>
  <summary>Details</summary>
Motivation: 现有的大型空中导航模型存在四个主要问题：1）数据领域差距导致模型泛化能力差；2）缺乏足够的时间导航推理能力；3）生成动作策略存在安全隐患；4）难以在资源受限的无人机上部署。这些问题限制了无人机在复杂环境中的自主导航能力。

Method: 1）使用3D高斯溅射构建高保真数据集以弥合领域差距；2）采用渐进式三阶段训练框架，依次强化场景理解、核心飞行技能和复杂导航能力；3）设计轻量级实时动作模块，结合几何安全校正，确保快速、无碰撞、稳定的指令生成；4）通过深度优化机载部署流程，在资源受限的无人机上实现高效推理。

Result: VLA-AN在机载推理吞吐量上实现了8.3倍的实时改进，显著提升了空间定位、场景推理和长时程导航能力，单任务最高成功率达到了98.1%，为轻量级空中机器人实现全链闭环自主性提供了高效实用的解决方案。

Conclusion: VLA-AN通过创新的数据集构建、渐进式训练框架、安全动作模块和优化部署流程，成功解决了无人机自主导航的关键挑战，为资源受限的无人机在复杂环境中实现高效、安全的自主导航提供了可行的技术方案。

Abstract: This paper proposes VLA-AN, an efficient and onboard Vision-Language-Action (VLA) framework dedicated to autonomous drone navigation in complex environments. VLA-AN addresses four major limitations of existing large aerial navigation models: the data domain gap, insufficient temporal navigation with reasoning, safety issues with generative action policies, and onboard deployment constraints. First, we construct a high-fidelity dataset utilizing 3D Gaussian Splatting (3D-GS) to effectively bridge the domain gap. Second, we introduce a progressive three-stage training framework that sequentially reinforces scene comprehension, core flight skills, and complex navigation capabilities. Third, we design a lightweight, real-time action module coupled with geometric safety correction. This module ensures fast, collision-free, and stable command generation, mitigating the safety risks inherent in stochastic generative policies. Finally, through deep optimization of the onboard deployment pipeline, VLA-AN achieves a robust real-time 8.3x improvement in inference throughput on resource-constrained UAVs. Extensive experiments demonstrate that VLA-AN significantly improves spatial grounding, scene reasoning, and long-horizon navigation, achieving a maximum single-task success rate of 98.1%, and providing an efficient, practical solution for realizing full-chain closed-loop autonomy in lightweight aerial robots.

</details>


### [9] [A Network-Based Framework for Modeling and Analyzing Human-Robot Coordination Strategies](https://arxiv.org/abs/2512.15282)
*Martijn IJtsma,Salvatore Hargis*

Main category: cs.RO

TL;DR: 提出用于人机系统联合工作策略分析的计算框架，整合功能建模与图论表示，支持概念设计阶段的协调需求分析


<details>
  <summary>Details</summary>
Motivation: 随着机器人能力提升，人机协作需求增加，但现有框架要么关注实时执行计算支持，要么依赖静态设计表示，缺乏对早期概念设计阶段协调动态的推理支持

Method: 整合功能建模技术与图论表示，构建分析人机系统联合工作策略的计算框架，通过系统功能关系和工作环境的物理信息结构来表征集体工作，并明确捕捉协调需求随时间演变

Result: 通过灾难机器人案例研究展示了框架在概念设计阶段的应用，支持早期人机协调策略的权衡探索，识别支持协调开销灵活管理的协作能力

Conclusion: 该框架使协调需求及其时间演变变得明确，支持在实施前对协作能力需求和工作需求进行设计时推理

Abstract: Studies of human-robot interaction in dynamic and unstructured environments show that as more advanced robotic capabilities are deployed, the need for cooperative competencies to support collaboration with human problem-holders increases. Designing human-robot systems to meet these demands requires an explicit understanding of the work functions and constraints that shape the feasibility of alternative joint work strategies. Yet existing human-robot interaction frameworks either emphasize computational support for real-time execution or rely on static representations for design, offering limited support for reasoning about coordination dynamics during early-stage conceptual design. To address this gap, this article presents a novel computational framework for analyzing joint work strategies in human-robot systems by integrating techniques from functional modeling with graph-theoretic representations. The framework characterizes collective work in terms of the relationships among system functions and the physical and informational structure of the work environment, while explicitly capturing how coordination demands evolve over time. Its use during conceptual design is demonstrated through a case study in disaster robotics, which shows how the framework can be used to support early trade-space exploration of human-robot coordination strategies and to identify cooperative competencies that support flexible management of coordination overhead. These results show how the framework makes coordination demands and their temporal evolution explicit, supporting design-time reasoning about cooperative competency requirements and work demands prior to implementation.

</details>


### [10] [GuangMing-Explorer: A Four-Legged Robot Platform for Autonomous Exploration in General Environments](https://arxiv.org/abs/2512.15309)
*Kai Zhang,Shoubin Chen,Dong Li,Baiyang Zhang,Tao Huang,Zehao Wu,Jiasheng Chen,Bo Zhang*

Main category: cs.RO

TL;DR: 介绍了一个名为GuangMing-Explorer的完全集成自主探索平台，该系统包含硬件和软件，能在多样化环境中进行鲁棒操作，并通过真实世界实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 尽管自主探索的各个组件已有显著进展，但缺乏一个包含硬件和软件的完整自主探索系统的全面实用描述。自主探索在室内目标搜索、极端环境测绘、资源勘探等应用中具有关键作用。

Method: 提出了GuangMing-Explorer平台，这是一个完全集成的自主探索系统，包括硬件设计、软件栈、算法部署和实验配置。系统架构涵盖了感知、规划、控制和运动执行的紧密集成。

Result: 通过大量真实世界实验证明了该平台在执行自主探索任务时的有效性和效率，展示了其在复杂和非结构化环境中实际部署的潜力。

Conclusion: GuangMing-Explorer是一个实用的完全集成自主探索平台，能够应对多样化环境，为复杂非结构化环境中的实际部署提供了有前景的解决方案。

Abstract: Autonomous exploration is a fundamental capability that tightly integrates perception, planning, control, and motion execution. It plays a critical role in a wide range of applications, including indoor target search, mapping of extreme environments, resource exploration, etc. Despite significant progress in individual components, a holistic and practical description of a completely autonomous exploration system, encompassing both hardware and software, remains scarce. In this paper, we present GuangMing-Explorer, a fully integrated autonomous exploration platform designed for robust operation across diverse environments. We provide a comprehensive overview of the system architecture, including hardware design, software stack, algorithm deployment, and experimental configuration. Extensive real-world experiments demonstrate the platform's effectiveness and efficiency in executing autonomous exploration tasks, highlighting its potential for practical deployment in complex and unstructured environments.

</details>


### [11] [Remotely Detectable Robot Policy Watermarking](https://arxiv.org/abs/2512.15379)
*Michael Amir,Manon Flageat,Amanda Prorok*

Main category: cs.RO

TL;DR: 该论文提出了首个用于机器人策略远程检测的水印方法CoNoCo，通过利用策略的固有随机性在机器人动作中嵌入频谱信号，解决了物理观察差距问题。


<details>
  <summary>Details</summary>
Motivation: 随着机器学习在机器人系统中的成功应用，训练好的策略成为一种新的知识产权形式，需要验证所有权并检测未经授权的使用。现有水印方法假设可以访问机器人内部状态，但审计者通常只能通过外部观察（如视频）进行检测，存在"物理观察差距"。

Method: 提出了Colored Noise Coherency (CoNoCo)水印策略，利用策略的固有随机性在机器人动作中嵌入频谱信号。该方法基于"glimpse sequence"概念，通过设计确保不降低性能，并证明了CoNoCo保持了边缘动作分布。

Result: 实验表明CoNoCo在各种远程模态下都能实现强大且鲁棒的检测，包括运动捕捉和侧向/俯视视频，在模拟和真实机器人实验中均表现良好。

Conclusion: 该工作为保护机器人知识产权提供了必要步骤，首次提出了使用纯远程观察非侵入性地验证物理策略来源的方法，解决了物理观察差距问题。

Abstract: The success of machine learning for real-world robotic systems has created a new form of intellectual property: the trained policy. This raises a critical need for novel methods that verify ownership and detect unauthorized, possibly unsafe misuse. While watermarking is established in other domains, physical policies present a unique challenge: remote detection. Existing methods assume access to the robot's internal state, but auditors are often limited to external observations (e.g., video footage). This ``Physical Observation Gap'' means the watermark must be detected from signals that are noisy, asynchronous, and filtered by unknown system dynamics. We formalize this challenge using the concept of a \textit{glimpse sequence}, and introduce Colored Noise Coherency (CoNoCo), the first watermarking strategy designed for remote detection. CoNoCo embeds a spectral signal into the robot's motions by leveraging the policy's inherent stochasticity. To show it does not degrade performance, we prove CoNoCo preserves the marginal action distribution. Our experiments demonstrate strong, robust detection across various remote modalities, including motion capture and side-way/top-down video footage, in both simulated and real-world robot experiments. This work provides a necessary step toward protecting intellectual property in robotics, offering the first method for validating the provenance of physical policies non-invasively, using purely remote observations.

</details>


### [12] [MiVLA: Towards Generalizable Vision-Language-Action Model with Human-Robot Mutual Imitation Pre-training](https://arxiv.org/abs/2512.15411)
*Zhenhan Yin,Xuanhan Wang,Jiahao Jiang,Kaiyuan Deng,Pengqi Chen,Shuangle Li,Chong Liu,Xing Xu,ingkuan Song,Lianli Gao,Heng Tao Shen*

Main category: cs.RO

TL;DR: MiVLA通过人类-机器人相互模仿预训练，利用手部与机械臂的行为相似性建立行为先验，提升视觉-语言-动作模型在跨视角、外观和形态差异下的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言-动作模型（VLA）在利用丰富的人类视频和模拟机器人数据时，由于相机视角、视觉外观和形态差异导致的泛化能力有限，需要解决跨域泛化问题。

Method: 提出MiVLA模型，基于人类-机器人相互模仿预训练，利用左右手坐标系运动学规则进行双向对齐，让模型既能预测一种形态的行为轨迹，又能模仿另一种未见形态的行为。

Result: 在三个机器人平台（ARX、PiPer和LocoMan）的仿真和真实世界实验中，MiVLA相比现有最佳VLA模型（π₀、π₀.₅和H-RDT）在仿真中提升25%，在真实机器人控制任务中提升14%。

Conclusion: 通过人类-机器人相互模仿预训练，MiVLA成功整合了真实世界人类数据的行为保真度和模拟机器人数据的操作多样性，显著提升了跨域泛化能力。

Abstract: While leveraging abundant human videos and simulated robot data poses a scalable solution to the scarcity of real-world robot data, the generalization capability of existing vision-language-action models (VLAs) remains limited by mismatches in camera views, visual appearance, and embodiment morphologies. To overcome this limitation, we propose MiVLA, a generalizable VLA empowered by human-robot mutual imitation pre-training, which leverages inherent behavioral similarity between human hands and robotic arms to build a foundation of strong behavioral priors for both human actions and robotic control. Specifically, our method utilizes kinematic rules with left/right hand coordinate systems for bidirectional alignment between human and robot action spaces. Given human or simulated robot demonstrations, MiVLA is trained to forecast behavior trajectories for one embodiment, and imitate behaviors for another one unseen in the demonstration. Based on this mutual imitation, it integrates the behavioral fidelity of real-world human data with the manipulative diversity of simulated robot data into a unified model, thereby enhancing the generalization capability for downstream tasks. Extensive experiments conducted on both simulation and real-world platforms with three robots (ARX, PiPer and LocoMan), demonstrate that MiVLA achieves strong improved generalization capability, outperforming state-of-the-art VLAs (e.g., $\boldsymbolπ_{0}$, $\boldsymbolπ_{0.5}$ and H-RDT) by 25% in simulation, and 14% in real-world robot control tasks.

</details>


### [13] [Load-Based Variable Transmission Mechanism for Robotic Applications](https://arxiv.org/abs/2512.15448)
*Sinan Emre,Victor Barasuol,Matteo Villa,Claudio Semini*

Main category: cs.RO

TL;DR: 提出一种基于负载的可变传动机制，通过预紧弹簧和四连杆机构被动调节传动比，无需额外执行器即可在达到扭矩阈值时实现40%的传动比提升。


<details>
  <summary>Details</summary>
Motivation: 现有可变传动系统需要额外的执行器进行主动控制，增加了机器人关节驱动系统的复杂性。研究旨在开发轻量、高效、自适应的传动系统，特别是在需要动态扭矩适应的腿式机器人中。

Method: 设计基于负载的可变传动机制，利用预紧弹簧和四连杆机构被动修改传动比。通过仿真分析评估机制有效性，系统在达到预定扭矩阈值时自动调整传动比。

Result: 系统在达到扭矩阈值时实现高达40%的传动比增加，当施加力超过18N时触发扭矩放大效应，能够自主响应变化的负载条件。

Conclusion: 该研究为机器人应用特别是腿式机器人开发了轻量、高效、自适应的传动系统，通过被动机制实现动态扭矩适应，减少了系统复杂性。

Abstract: This paper presents a Load-Based Variable Transmission (LBVT) mechanism designed to enhance robotic actuation by dynamically adjusting the transmission ratio in response to external torque demands. Unlike existing variable transmission systems that require additional actuators for active control, the proposed LBVT mechanism leverages a pre-tensioned spring and a four-bar linkage to passively modify the transmission ratio, thereby reducing the complexity of robot joint actuation systems. The effectiveness of the LBVT mechanism is evaluated through simulation-based analyses. The results confirm that the system achieves up to a 40 percent increase in transmission ratio upon reaching a predefined torque threshold, effectively amplifying joint torque when required without additional actuation. Furthermore, the simulations demonstrate a torque amplification effect triggered when the applied force exceeds 18 N, highlighting the system ability to autonomously respond to varying load conditions. This research contributes to the development of lightweight, efficient, and adaptive transmission systems for robotic applications, particularly in legged robots where dynamic torque adaptation is critical.

</details>


### [14] [OMCL: Open-vocabulary Monte Carlo Localization](https://arxiv.org/abs/2512.15557)
*Evgenii Kruzhkov,Raphael Memmesheimer,Sven Behnke*

Main category: cs.RO

TL;DR: 本文提出了一种基于视觉-语言特征的蒙特卡洛定位方法，通过开放词汇特征实现不同模态观测与地图元素的鲁棒关联，支持自然语言描述初始化全局定位。


<details>
  <summary>Details</summary>
Motivation: 机器人定位是导航规划的重要前提。当环境地图由不同传感器创建时，需要鲁棒地将机器人测量与地图特征关联。传统方法在处理不同模态数据时存在局限性。

Method: 扩展蒙特卡洛定位方法，引入视觉-语言特征。利用开放词汇特征计算给定相机位姿和3D地图（来自RGB-D图像或对齐点云）下视觉观测的似然度。通过抽象视觉-语言特征关联不同模态的观测和地图元素，支持自然语言描述初始化全局定位。

Result: 在室内场景（Matterport3D和Replica）和室外场景（SemanticKITTI）上评估了该方法，展示了良好的泛化能力。

Conclusion: 提出的基于视觉-语言特征的蒙特卡洛定位方法能够鲁棒地处理不同传感器创建的地图，通过开放词汇特征实现跨模态关联，支持自然语言初始化，在室内外场景均表现出良好性能。

Abstract: Robust robot localization is an important prerequisite for navigation planning. If the environment map was created from different sensors, robot measurements must be robustly associated with map features. In this work, we extend Monte Carlo Localization using vision-language features. These open-vocabulary features enable to robustly compute the likelihood of visual observations, given a camera pose and a 3D map created from posed RGB-D images or aligned point clouds. The abstract vision-language features enable to associate observations and map elements from different modalities. Global localization can be initialized by natural language descriptions of the objects present in the vicinity of locations. We evaluate our approach using Matterport3D and Replica for indoor scenes and demonstrate generalization on SemanticKITTI for outdoor scenes.

</details>


### [15] [An Open Toolkit for Underwater Field Robotics](https://arxiv.org/abs/2512.15597)
*Giacomo Picardi,Saverio Iacoponi,Matias Carandell,Jorge Aguirregomezcorta,Mrudul Chellapurath,Joaquin del Rio,Marcello Calisti,Iacopo Aguzzi*

Main category: cs.RO

TL;DR: 本文介绍了一个开源、低成本的水下机器人关节硬件软件工具包，包括深度额定水下机器人关节、紧凑控制电子设备和ROS2软件栈，旨在降低水下操作研究的门槛。


<details>
  <summary>Details</summary>
Motivation: 水下机器人对海洋科学、环境监测和海底工业操作日益重要，但水下操作和驱动系统的开发受到高成本、专有设计和缺乏模块化研究硬件的限制。现有开源项目主要集中在车辆构建和控制软件，而关节驱动系统特别是需要防水、带反馈的驱动系统存在明显空白。

Method: 开发了一个开源硬件软件工具包，包括：1) 深度额定水下机器人关节（URJ），具有早期泄漏检测功能；2) 紧凑的控制和电源管理电子设备；3) 基于ROS2的软件栈，用于传感和多模式驱动。所有CAD模型、制造文件、PCB源文件、固件和ROS2包都开源发布。

Result: 工具包经过广泛的实验室测试和多次现场部署，在40米深度下可靠运行，成功应用于3自由度水下机械臂、肌腱驱动软夹爪和欠驱动沉积物采样器等多样化应用，验证了其在真实海洋环境中的鲁棒性、多功能性和可重用性。

Conclusion: 通过提供完全开源、经过现场测试的平台，这项工作旨在降低水下操作研究的入门门槛，提高可重复性，并加速水下现场机器人技术的创新。

Abstract: Underwater robotics is becoming increasingly important for marine science, environmental monitoring, and subsea industrial operations, yet the development of underwater manipulation and actuation systems remains restricted by high costs, proprietary designs, and limited access to modular, research-oriented hardware. While open-source initiatives have democratized vehicle construction and control software, a substantial gap persists for joint-actuated systems-particularly those requiring waterproof, feedback-enabled actuation suitable for manipulators, grippers, and bioinspired devices. As a result, many research groups face lengthy development cycles, limited reproducibility, and difficulty transitioning laboratory prototypes to field-ready platforms.
  To address this gap, we introduce an open, cost-effective hardware and software toolkit for underwater manipulation research. The toolkit includes a depth-rated Underwater Robotic Joint (URJ) with early leakage detection, compact control and power management electronics, and a ROS2-based software stack for sensing and multi-mode actuation. All CAD models, fabrication files, PCB sources, firmware, and ROS2 packages are openly released, enabling local manufacturing, modification, and community-driven improvement.
  The toolkit has undergone extensive laboratory testing and multiple field deployments, demonstrating reliable operation up to 40 m depth across diverse applications, including a 3-DoF underwater manipulator, a tendon-driven soft gripper, and an underactuated sediment sampler. These results validate the robustness, versatility, and reusability of the toolkit for real marine environments.
  By providing a fully open, field-tested platform, this work aims to lower the barrier to entry for underwater manipulation research, improve reproducibility, and accelerate innovation in underwater field robotics.

</details>
