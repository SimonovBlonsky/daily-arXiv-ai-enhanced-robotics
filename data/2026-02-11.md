<div id=toc></div>

# Table of Contents

- [cs.RO](#cs.RO) [Total: 38]


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [1] [Feasible Static Workspace Optimization of Tendon Driven Continuum Robot based on Euclidean norm](https://arxiv.org/abs/2602.09046)
*Mohammad Jabari,Carmen Visconte,Giuseppe Quaglia,Med Amine Laribi*

Main category: cs.RO

TL;DR: 本文提出了一种基于可行静态工作空间（FSW）的肌腱驱动连续体机器人（TDCR）优化设计方法，通过遗传算法优化肌腱力来最大化机器人的工作空间，即使在外部载荷作用下也能有效工作。


<details>
  <summary>Details</summary>
Motivation: 肌腱驱动连续体机器人（TDCR）在医疗和工业应用中具有重要价值，但其性能受肌腱力配置影响。现有设计方法未充分考虑在外部载荷（力和扭矩）作用下的可行静态工作空间优化问题，需要一种系统化的优化方法来提升机器人在实际工作环境中的性能。

Method: 研究采用两段式、每段由四根肌腱驱动的TDCR结构。将肌腱力作为设计变量，可行静态工作空间（FSW）作为优化目标。使用遗传算法优化方法，最大化机器人末端位置的欧几里得范数在工作空间上的积分。在仿真中考虑了外部载荷（包括扭矩和力）的影响。

Result: 仿真结果表明，所提出的方法能够有效识别最优的肌腱力配置，即使在外部力和扭矩的影响下，也能最大化肌腱驱动连续体机器人的可行静态工作空间。该方法为TDCR的设计优化提供了有效工具。

Conclusion: 基于可行静态工作空间的优化方法为肌腱驱动连续体机器人的设计提供了有效的框架，能够在考虑外部载荷的情况下优化肌腱力配置，从而提升机器人的工作性能。该方法对医疗和工业应用中TDCR的实际部署具有重要意义。

Abstract: This paper focuses on the optimal design of a tendon-driven continuum robot (TDCR) based on its feasible static workspace (FSW). The TDCR under consideration is a two-segment robot driven by eight tendons, with four tendon actuators per segment. Tendon forces are treated as design variables, while the feasible static workspace (FSW) serves as the optimization objective. To determine the robot's feasible static workspace, a genetic algorithm optimization approach is employed to maximize a Euclidian norm of the TDCR's tip position over the workspace. During the simulations, the robot is subjected to external loads, including torques and forces. The results demonstrate the effectiveness of the proposed method in identifying optimal tendon forces to maximize the feasible static workspace, even under the influence of external forces and torques.

</details>


### [2] [Legs Over Arms: On the Predictive Value of Lower-Body Pose for Human Trajectory Prediction from Egocentric Robot Perception](https://arxiv.org/abs/2602.09076)
*Nhat Le,Daeun Song,Xuesu Xiao*

Main category: cs.RO

TL;DR: 该研究探索了利用人体骨骼特征（特别是下肢3D关键点）来提升多智能体轨迹预测精度，在JRDB和新的全景视频数据集上验证了方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 在拥挤环境中预测人类轨迹对社交机器人导航至关重要。现有方法大多将人视为质点，忽略了人体骨骼特征可能提供的额外信息，因此需要系统评估不同骨骼特征对轨迹预测的效用。

Method: 系统评估了2D和3D骨骼关键点以及衍生的生物力学线索作为额外输入的预测效用。特别关注下肢3D关键点，并在JRDB数据集和新的360度全景视频社交导航数据集上进行综合研究。

Result: 聚焦于下肢3D关键点可使平均位移误差降低13%；将3D关键点输入与相应的生物力学线索结合可进一步改善1-4%。即使使用从等距柱面全景图像提取的2D关键点输入，性能提升仍然存在。

Conclusion: 机器人通过观察人类腿部可以高效预测人类运动，这为设计社交机器人导航的感知能力提供了可行的见解。单目环绕视觉能够捕捉到用于运动预测的信息线索。

Abstract: Predicting human trajectory is crucial for social robot navigation in crowded environments. While most existing approaches treat human as point mass, we present a study on multi-agent trajectory prediction that leverages different human skeletal features for improved forecast accuracy. In particular, we systematically evaluate the predictive utility of 2D and 3D skeletal keypoints and derived biomechanical cues as additional inputs. Through a comprehensive study on the JRDB dataset and another new dataset for social navigation with 360-degree panoramic videos, we find that focusing on lower-body 3D keypoints yields a 13% reduction in Average Displacement Error and augmenting 3D keypoint inputs with corresponding biomechanical cues provides a further 1-4% improvement. Notably, the performance gain persists when using 2D keypoint inputs extracted from equirectangular panoramic images, indicating that monocular surround vision can capture informative cues for motion forecasting. Our finding that robots can forecast human movement efficiently by watching their legs provides actionable insights for designing sensing capabilities for social robot navigation.

</details>


### [3] [Agile asymmetric multi-legged locomotion: contact planning via geometric mechanics and spin model duality](https://arxiv.org/abs/2602.09123)
*Jackson Habala,Gabriel B. Margolis,Tianyu Wang,Pratyush Bhatt,Juntao He,Naheel Naeem,Zhaochen Xu,Pulkit Agrawal,Daniel I. Goldman,Di Luo,Baxi Chong*

Main category: cs.RO

TL;DR: 本文提出了一种多足机器人控制新框架，利用几何力学和统计力学中的自旋模型对偶性，通过对称性破缺优化步态规划，为六足机器人发现了不对称步态策略，速度比传统步态提升50%。


<details>
  <summary>Details</summary>
Motivation: 目前多足机器人研究主要集中在双足和四足机器人，而更多腿的机器人潜力未被充分挖掘。这并非硬件限制，而是缺乏能够解释何时以及如何通过增加腿数提升运动性能的控制框架。多足系统的协调控制面临维度灾难挑战，现有方法无法利用高维系统的新对称性和控制机会。

Method: 采用几何力学将接触丰富的运动规划简化为图优化问题，并提出统计力学中的自旋模型对偶框架来利用对称性破缺指导最优步态重组。通过该框架为六足机器人识别不对称运动策略。

Result: 发现了一种六足机器人的不对称运动策略，前向速度达到0.61体长/周期，比传统步态提升50%。这种不对称性体现在控制和硬件两个层面：控制层面，身体方向在快速顺时针和慢速逆时针转向阶段之间不对称振荡；硬件层面，同一侧的两条腿可以保持非驱动状态，用刚性部件替代而不影响性能。

Conclusion: 该框架为多足机器人控制提供了原则性方法，通过对称性重组在高维具身系统中发现了新颖的运动行为。数值模拟和机器人物理实验验证了框架的有效性，展示了对称性破缺在多足运动控制中的重要作用。

Abstract: Legged robot research is presently focused on bipedal or quadrupedal robots, despite capabilities to build robots with many more legs to potentially improve locomotion performance. This imbalance is not necessarily due to hardware limitations, but rather to the absence of principled control frameworks that explain when and how additional legs improve locomotion performance. In multi-legged systems, coordinating many simultaneous contacts introduces a severe curse of dimensionality that challenges existing modeling and control approaches. As an alternative, multi-legged robots are typically controlled using low-dimensional gaits originally developed for bipeds or quadrupeds. These strategies fail to exploit the new symmetries and control opportunities that emerge in higher-dimensional systems. In this work, we develop a principled framework for discovering new control structures in multi-legged locomotion. We use geometric mechanics to reduce contact-rich locomotion planning to a graph optimization problem, and propose a spin model duality framework from statistical mechanics to exploit symmetry breaking and guide optimal gait reorganization. Using this approach, we identify an asymmetric locomotion strategy for a hexapod robot that achieves a forward speed of 0.61 body lengths per cycle (a 50% improvement over conventional gaits). The resulting asymmetry appears at both the control and hardware levels. At the control level, the body orientation oscillates asymmetrically between fast clockwise and slow counterclockwise turning phases for forward locomotion. At the hardware level, two legs on the same side remain unactuated and can be replaced with rigid parts without degrading performance. Numerical simulations and robophysical experiments validate the framework and reveal novel locomotion behaviors that emerge from symmetry reforming in high-dimensional embodied systems.

</details>


### [4] [Elements of Robot Morphology: Supporting Designers in Robot Form Exploration](https://arxiv.org/abs/2602.09203)
*Amy Koike,Ge,Guo,Xinning He,Callie Y. Kim,Dakota Sullivan,Bilge Mutlu*

Main category: cs.RO

TL;DR: 提出了机器人形态学框架和实体化工具包，支持系统化的机器人形态设计和探索


<details>
  <summary>Details</summary>
Motivation: 机器人形态设计在HRI中至关重要，但缺乏系统化的设计框架来指导形态探索

Method: 提出了机器人形态学五要素框架，并开发了实体化探索积木工具包，通过案例研究和设计工作坊进行评估

Result: 框架和工具包支持分析、构思、反思和协作式机器人设计，促进多样化的机器人形态探索

Conclusion: 机器人形态学框架和实体化工具包为系统化的机器人形态设计提供了有效方法，填补了HRI领域的设计框架空白

Abstract: Robot morphology, the form, shape, and structure of robots, is a key design space in human-robot interaction (HRI), shaping how robots function, express themselves, and interact with people. Yet, despite its importance, little is known about how design frameworks can guide systematic form exploration. To address this gap, we introduce Elements of Robot Morphology, a framework that identifies five fundamental elements: perception, articulation, end effectors, locomotion, and structure. Derived from an analysis of existing robots, the framework supports structured exploration of diverse robot forms. To operationalize the framework, we developed Morphology Exploration Blocks (MEB), a set of tangible blocks that enable hands-on, collaborative experimentation with robot morphologies. We evaluate the framework and toolkit through a case study and design workshops, showing how they support analysis, ideation, reflection, and collaborative robot design.

</details>


### [5] [Risk-Aware Obstacle Avoidance Algorithm for Real-Time Applications](https://arxiv.org/abs/2602.09204)
*Ozan Kaya,Emir Cem Gezer,Roger Skjetne,Ingrid Bouwer Utne*

Main category: cs.RO

TL;DR: 提出了一种混合风险感知导航架构，将障碍物概率建模与平滑轨迹优化相结合，用于自主水面舰艇的导航


<details>
  <summary>Details</summary>
Motivation: 在变化的海洋环境中实现鲁棒导航需要能够感知、推理和在不确定性下行动的自主系统

Method: 构建概率风险地图捕捉障碍物接近度和动态对象行为；使用风险偏置RRT*规划器生成无碰撞路径；通过B样条算法优化轨迹连续性；实现三种RRT*重连模式

Result: 系统能够在包含静态和动态障碍物的实验场景中安全导航，保持平滑轨迹，并动态适应环境风险变化

Conclusion: 相比传统LIDAR或视觉导航方法，该方法在操作安全性和自主性方面有所改进，是风险感知自主车辆在不确定动态环境中的有前景解决方案

Abstract: Robust navigation in changing marine environments requires autonomous systems capable of perceiving, reasoning, and acting under uncertainty. This study introduces a hybrid risk-aware navigation architecture that integrates probabilistic modeling of obstacles along the vehicle path with smooth trajectory optimization for autonomous surface vessels. The system constructs probabilistic risk maps that capture both obstacle proximity and the behavior of dynamic objects. A risk-biased Rapidly Exploring Random Tree (RRT) planner leverages these maps to generate collision-free paths, which are subsequently refined using B-spline algorithms to ensure trajectory continuity. Three distinct RRT* rewiring modes are implemented based on the cost function: minimizing the path length, minimizing risk, and optimizing a combination of the path length and total risk. The framework is evaluated in experimental scenarios containing both static and dynamic obstacles. The results demonstrate the system's ability to navigate safely, maintain smooth trajectories, and dynamically adapt to changing environmental risks. Compared with conventional LIDAR or vision-only navigation approaches, the proposed method shows improvements in operational safety and autonomy, establishing it as a promising solution for risk-aware autonomous vehicle missions in uncertain and dynamic environments.

</details>


### [6] [From Legible to Inscrutable Trajectories: (Il)legible Motion Planning Accounting for Multiple Observers](https://arxiv.org/abs/2602.09227)
*Ananya Yammanuru,Maria Lusardi,Nancy M. Amato,Katherine Driggs-Campbell*

Main category: cs.RO

TL;DR: 提出MMLO-LMP问题，要求运动规划器生成对积极观察者可读、对消极观察者不可读的轨迹，同时考虑各观察者的视野限制，并开发DUBIOUS轨迹优化器解决该问题。


<details>
  <summary>Details</summary>
Motivation: 在现实环境中，机器人需要面对多个观察者，这些观察者可能具有不同的动机（合作或对抗），且每个观察者只能看到环境的一部分。现有研究主要关注单一观察者情况下的可读性或不可读性轨迹规划，缺乏对混合动机和有限观察能力的综合处理。

Method: 提出混合动机有限观察可读运动规划（MMLO-LMP）问题，开发DUBIOUS轨迹优化器来解决该问题。该方法考虑不同观察者的动机（积极/消极）和视野限制，生成既能向合作观察者清晰传达意图，又能对对抗观察者隐藏意图的轨迹。

Result: DUBIOUS能够生成平衡可读性与观察者动机和视野限制的轨迹。结果表明该方法能有效解决MMLO-LMP问题，为不同观察者提供差异化的信息传达策略。

Conclusion: MMLO-LMP问题为复杂环境中的运动规划提供了新视角，DUBIOUS方法展示了在混合动机和有限观察条件下的可行解决方案。未来工作包括扩展问题变体，如移动观察者和观察者团队协作等场景。

Abstract: In cooperative environments, such as in factories or assistive scenarios, it is important for a robot to communicate its intentions to observers, who could be either other humans or robots. A legible trajectory allows an observer to quickly and accurately predict an agent's intention. In adversarial environments, such as in military operations or games, it is important for a robot to not communicate its intentions to observers. An illegible trajectory leads an observer to incorrectly predict the agent's intention or delays when an observer is able to make a correct prediction about the agent's intention. However, in some environments there are multiple observers, each of whom may be able to see only part of the environment, and each of whom may have different motives. In this work, we introduce the Mixed-Motive Limited-Observability Legible Motion Planning (MMLO-LMP) problem, which requires a motion planner to generate a trajectory that is legible to observers with positive motives and illegible to observers with negative motives while also considering the visibility limitations of each observer. We highlight multiple strategies an agent can take while still achieving the problem objective. We also present DUBIOUS, a trajectory optimizer that solves MMLO-LMP. Our results show that DUBIOUS can generate trajectories that balance legibility with the motives and limited visibility regions of the observers. Future work includes many variations of MMLO-LMP, including moving observers and observer teaming.

</details>


### [7] [STaR: Scalable Task-Conditioned Retrieval for Long-Horizon Multimodal Robot Memory](https://arxiv.org/abs/2602.09255)
*Mingfeng Yuan,Hao Zhang,Mahan Mohammadi,Runhao Li,Jinjun Shan,Steven L. Waslander*

Main category: cs.RO

TL;DR: STaR是一个用于移动机器人的智能推理框架，通过构建任务无关的多模态长期记忆和基于信息瓶颈原理的可扩展检索算法，支持在开放动态场景中进行长期规划和导航推理。


<details>
  <summary>Details</summary>
Motivation: 移动机器人在开放动态场景中长期部署时面临核心挑战：需要构建可扩展的长期记忆系统，支持基于开放指令的规划、检索和推理，同时为导航提供精确可操作的结果。

Method: 提出STaR框架：(1)构建任务无关的多模态长期记忆，保留细粒度环境语义；(2)基于信息瓶颈原理开发可扩展的任务条件检索算法，从长期记忆中提取紧凑、非冗余、信息丰富的候选记忆进行上下文推理。

Result: 在NaVQA（室内外校园场景）和WH-VQA（仓库基准）两个数据集上，STaR始终优于强基线，实现更高的成功率和显著更低的空间误差。在真实Husky轮式机器人上的部署验证了其鲁棒的长期推理能力和实用性。

Conclusion: STaR框架通过创新的记忆构建和检索机制，有效解决了移动机器人在开放动态场景中长期部署时的记忆和推理挑战，展示了在实际机器人系统中的可扩展性和实用价值。

Abstract: Mobile robots are often deployed over long durations in diverse open, dynamic scenes, including indoor setting such as warehouses and manufacturing facilities, and outdoor settings such as agricultural and roadway operations. A core challenge is to build a scalable long-horizon memory that supports an agentic workflow for planning, retrieval, and reasoning over open-ended instructions at variable granularity, while producing precise, actionable answers for navigation. We present STaR, an agentic reasoning framework that (i) constructs a task-agnostic, multimodal long-term memory that generalizes to unseen queries while preserving fine-grained environmental semantics (object attributes, spatial relations, and dynamic events), and (ii) introduces a Scalable TaskConditioned Retrieval algorithm based on the Information Bottleneck principle to extract from long-term memory a compact, non-redundant, information-rich set of candidate memories for contextual reasoning. We evaluate STaR on NaVQA (mixed indoor/outdoor campus scenes) and WH-VQA, a customized warehouse benchmark with many visually similar objects built with Isaac Sim, emphasizing contextual reasoning. Across the two datasets, STaR consistently outperforms strong baselines, achieving higher success rates and markedly lower spatial error. We further deploy STaR on a real Husky wheeled robot in both indoor and outdoor environments, demonstrating robust longhorizon reasoning, scalability, and practical utility.

</details>


### [8] [Data-centric Design of Learning-based Surgical Gaze Perception Models in Multi-Task Simulation](https://arxiv.org/abs/2602.09259)
*Yizhou Li,Shuyuan Yang,Jiaji Su,Zonghe Chua*

Main category: cs.RO

TL;DR: 研究探讨手术机器人系统中专家注视监督的来源（专业水平与感知模式）如何影响注意力模型学习，通过收集主动-被动多任务手术注视数据集，分析不同监督源的可替代性。


<details>
  <summary>Details</summary>
Motivation: 在机器人辅助微创手术中，触觉反馈和深度线索减少，增加了对专家视觉感知的依赖，需要注视引导训练和学习型手术感知模型。但专家注视数据收集成本高，且不清楚注视监督的来源（专业水平和感知模式）如何影响注意力模型的学习。

Method: 在达芬奇SimNow模拟器上收集了配对主动-被动多任务手术注视数据集，涵盖四种训练任务。主动注视通过VR头显眼动追踪在执行任务时记录，相同视频用作刺激材料收集被动观察者的注视，实现受控的同视频比较。使用注视密度重叠分析和单帧显著性建模评估被动注视对操作监督的可替代性。

Result: MSI-Net产生稳定可解释的预测，而SalGAN不稳定且与人类注视对齐差。被动注视训练的模型能恢复大部分中级主动注意力，但有可预测的退化，主动与被动目标间的转移是不对称的。新手被动标签能以有限损失近似中级被动目标，为可扩展的众包注视监督提供了实用路径。

Conclusion: 被动注视监督在手术指导和感知建模中具有实用价值，特别是新手被动标签能有效近似中级被动目标，为可扩展的众包注视监督提供了可行方案，但需要注意主动与被动注视监督之间的不对称性。

Abstract: In robot-assisted minimally invasive surgery (RMIS), reduced haptic feedback and depth cues increase reliance on expert visual perception, motivating gaze-guided training and learning-based surgical perception models. However, operative expert gaze is costly to collect, and it remains unclear how the source of gaze supervision, both expertise level (intermediate vs. novice) and perceptual modality (active execution vs. passive viewing), shapes what attention models learn. We introduce a paired active-passive, multi-task surgical gaze dataset collected on the da Vinci SimNow simulator across four drills. Active gaze was recorded during task execution using a VR headset with eye tracking, and the corresponding videos were reused as stimuli to collect passive gaze from observers, enabling controlled same-video comparisons. We quantify skill- and modality-dependent differences in gaze organization and evaluate the substitutability of passive gaze for operative supervision using fixation density overlap analyses and single-frame saliency modeling. Across settings, MSI-Net produced stable, interpretable predictions, whereas SalGAN was unstable and often poorly aligned with human fixations. Models trained on passive gaze recovered a substantial portion of intermediate active attention, but with predictable degradation, and transfer was asymmetric between active and passive targets. Notably, novice passive labels approximated intermediate-passive targets with limited loss on higher-quality demonstrations, suggesting a practical path for scalable, crowd-sourced gaze supervision in surgical coaching and perception modeling.

</details>


### [9] [Disambiguating Anthropomorphism and Anthropomimesis in Human-Robot Interaction](https://arxiv.org/abs/2602.09287)
*Minja Axelsson,Henry Shevlin*

Main category: cs.RO

TL;DR: 该论文初步区分了人机交互中的两个理论概念：拟人化（用户感知机器人的人类特质）和拟人模仿（开发者设计机器人的人类特征），明确了责任主体差异。


<details>
  <summary>Details</summary>
Motivation: 人机交互和社会机器人学中，anthropomorphism和anthropomimesis这两个概念经常混淆使用，缺乏明确区分。研究者需要澄清这两个理论概念，明确人类特质在机器人中的来源和责任主体，为未来的机器人设计和评估提供理论基础。

Method: 通过概念分析和理论辨析的方法，对anthropomorphism和anthropomimesis进行初步区分和定义。将anthropomorphism定义为用户对机器人的人类特质感知，anthropomimesis定义为开发者对机器人的人类特征设计。

Result: 成功区分了两个关键概念：拟人化（用户视角的感知过程）和拟人模仿（设计者视角的构建过程）。明确了人类特质在机器人中的责任主体差异——拟人化的责任主体是机器人感知者，拟人模仿的责任主体是机器人设计者。

Conclusion: 该初步工作为人机交互研究提供了清晰的理论概念区分，有助于未来研究者在机器人设计和评估中更精确地使用这些概念。区分用户感知和设计构建的视角，为理解人类与机器人互动中的"人类特质"来源提供了理论框架。

Abstract: In this preliminary work, we offer an initial disambiguation of the theoretical concepts anthropomorphism and anthropomimesis in Human-Robot Interaction (HRI) and social robotics. We define anthropomorphism as users perceiving human-like qualities in robots, and anthropomimesis as robot developers designing human-like features into robots. This contribution aims to provide a clarification and exploration of these concepts for future HRI scholarship, particularly regarding the party responsible for human-like qualities - robot perceiver for anthropomorphism, and robot designer for anthropomimesis. We provide this contribution so that researchers can build on these disambiguated theoretical concepts for future robot design and evaluation.

</details>


### [10] [CAPER: Constrained and Procedural Reasoning for Robotic Scientific Experiments](https://arxiv.org/abs/2602.09367)
*Jinghan Yang,Jingyi Hou,Xinbo Yu,Wei He,Yifan Wu*

Main category: cs.RO

TL;DR: CAPER框架通过责任分离结构解决机器人科学实验中的长时程操作问题，在规划与控制管道中明确限制学习和推理的位置，提高程序正确性、鲁棒性和数据效率。


<details>
  <summary>Details</summary>
Motivation: 科学实验室中的机器人辅助需要程序正确的长时程操作、有限监督下的可靠执行以及低演示数据下的鲁棒性。现有的端到端视觉-语言-动作模型在这些协议敏感的实验环境中假设可恢复错误和数据驱动策略学习，但这些假设经常失效。

Method: CAPER采用责任分离结构：任务级推理在明确约束下生成程序有效的动作序列；中层多模态基础实现子任务而不将空间决策委托给大语言模型；底层控制通过最少演示的强化学习适应物理不确定性。通过可解释的中间表示编码程序承诺。

Result: 在科学工作流基准测试和公共长时程操作数据集上的实验表明，CAPER在成功率、程序正确性方面取得一致改进，特别是在低数据和长时程设置中表现优异。

Conclusion: CAPER通过明确限制学习和推理位置的责任分离结构，防止执行时违反实验逻辑，提高了机器人科学实验的可控性、鲁棒性和数据效率，为协议敏感的实验环境提供了有效解决方案。

Abstract: Robotic assistance in scientific laboratories requires procedurally correct long-horizon manipulation, reliable execution under limited supervision, and robustness in low-demonstration regimes. Such conditions greatly challenge end-to-end vision-language-action (VLA) models, whose assumptions of recoverable errors and data-driven policy learning often break down in protocol-sensitive experiments. We propose CAPER, a framework for Constrained And ProcEdural Reasoning for robotic scientific experiments, which explicitly restricts where learning and reasoning occur in the planning and control pipeline. Rather than strengthening end-to-end policies, CAPER enforces a responsibility-separated structure: task-level reasoning generates procedurally valid action sequences under explicit constraints, mid-level multimodal grounding realizes subtasks without delegating spatial decision-making to large language models, and low-level control adapts to physical uncertainty via reinforcement learning with minimal demonstrations. By encoding procedural commitments through interpretable intermediate representations, CAPER prevents execution-time violations of experimental logic, improving controllability, robustness, and data efficiency. Experiments on a scientific workflow benchmark and a public long-horizon manipulation dataset demonstrate consistent improvements in success rate and procedural correctness, particularly in low-data and long-horizon settings.

</details>


### [11] [Certified Gradient-Based Contact-Rich Manipulation via Smoothing-Error Reachable Tubes](https://arxiv.org/abs/2602.09368)
*Wei-Chen Li,Glen Chou*

Main category: cs.RO

TL;DR: 该论文提出了一种结合可微物理与鲁棒控制的方法，通过平滑接触动力学并量化模型误差，为接触丰富的操作任务提供可证明的约束满足保证。


<details>
  <summary>Details</summary>
Motivation: 基于梯度的控制器优化方法在处理接触丰富的操作任务时面临挑战，因为混合接触动力学会导致梯度不连续或消失。虽然平滑动力学可以提供连续梯度，但模型不匹配会导致在真实系统上执行时控制器失败。

Method: 提出了一种基于凸优化的新型可微模拟器，平滑接触动力学和几何形状，并将真实动力学与平滑动力学之间的差异量化为集合值偏差。通过时变仿射反馈策略的优化，利用系统可达集的分析边界来约束这种偏差，从而为真实闭环混合动力学提供鲁棒的约束满足保证。

Result: 在平面推动、物体旋转和手内灵巧操作等多个接触丰富的任务上进行评估，相比基线方法，该方法实现了有保证的约束满足，具有更低的安全违规率和目标误差。

Conclusion: 通过桥接可微物理与集合值鲁棒控制，该方法成为首个用于接触丰富操作的可证明梯度策略合成方法，能够在依赖平滑动力学信息梯度的同时，为真实混合动力学提供形式化保证。

Abstract: Gradient-based methods can efficiently optimize controllers using physical priors and differentiable simulators, but contact-rich manipulation remains challenging due to discontinuous or vanishing gradients from hybrid contact dynamics. Smoothing the dynamics yields continuous gradients, but the resulting model mismatch can cause controller failures when executed on real systems. We address this trade-off by planning with smoothed dynamics while explicitly quantifying and compensating for the induced errors, providing formal guarantees of constraint satisfaction and goal reachability on the true hybrid dynamics. Our method smooths both contact dynamics and geometry via a novel differentiable simulator based on convex optimization, which enables us to characterize the discrepancy from the true dynamics as a set-valued deviation. This deviation constrains the optimization of time-varying affine feedback policies through analytical bounds on the system's reachable set, enabling robust constraint satisfaction guarantees for the true closed-loop hybrid dynamics, while relying solely on informative gradients from the smoothed dynamics. We evaluate our method on several contact-rich tasks, including planar pushing, object rotation, and in-hand dexterous manipulation, achieving guaranteed constraint satisfaction with lower safety violation and goal error than baselines. By bridging differentiable physics with set-valued robust control, our method is the first certifiable gradient-based policy synthesis method for contact-rich manipulation.

</details>


### [12] [LLM-Grounded Dynamic Task Planning with Hierarchical Temporal Logic for Human-Aware Multi-Robot Collaboration](https://arxiv.org/abs/2602.09472)
*Shuyuan Hu,Tao Lin,Kai Ye,Yang Yang,Tianwei Zhang*

Main category: cs.RO

TL;DR: 提出神经符号框架，将LLM推理转化为层次化LTL规范，解决多机器人任务分配与规划问题，通过滚动时域规划处理动态环境变化。


<details>
  <summary>Details</summary>
Motivation: LLM生成的多机器人任务计划缺乏运动学可行性且效率低，而形式化方法如LTL虽然提供正确性和最优性保证，但局限于静态离线设置且计算可扩展性差。

Method: 提出神经符号框架，将LLM推理转化为层次化LTL规范，解决同时任务分配与规划问题，通过滚动时域规划循环处理随机环境变化，在层次状态空间中动态优化计划。

Result: 大量真实世界实验表明，该方法在成功率和交互流畅度上显著优于基线方法，同时最小化规划延迟。

Conclusion: 该框架成功桥接了LLM的开放世界任务规范能力和LTL的形式化保证，实现了动态环境下的高效多机器人规划。

Abstract: While Large Language Models (LLM) enable non-experts to specify open-world multi-robot tasks, the generated plans often lack kinematic feasibility and are not efficient, especially in long-horizon scenarios. Formal methods like Linear Temporal Logic (LTL) offer correctness and optimal guarantees, but are typically confined to static, offline settings and struggle with computational scalability. To bridge this gap, we propose a neuro-symbolic framework that grounds LLM reasoning into hierarchical LTL specifications and solves the corresponding Simultaneous Task Allocation and Planning (STAP) problem. Unlike static approaches, our system resolves stochastic environmental changes, such as moving users or updated instructions via a receding horizon planning (RHP) loop with real-time perception, which dynamically refines plans through a hierarchical state space. Extensive real-world experiments demonstrate that our approach significantly outperforms baseline methods in success rate and interaction fluency while minimizing planning latency.

</details>


### [13] [Optimal Control of Microswimmers for Trajectory Tracking Using Bayesian Optimization](https://arxiv.org/abs/2602.09563)
*Lucas Palazzolo,Mickaël Binois,Laëtitia Giraldi*

Main category: cs.RO

TL;DR: 使用贝叶斯优化结合B样条参数化解决微泳体轨迹跟踪的最优控制问题，适用于不同精度模型，能处理壁面诱导的流体效应


<details>
  <summary>Details</summary>
Motivation: 微泳体轨迹跟踪在微机器人领域面临挑战，低雷诺数动力学使控制设计复杂，需要一种能处理高计算成本且无需复杂梯度计算的方法

Method: 将轨迹跟踪问题表述为最优控制问题，采用B样条参数化结合贝叶斯优化求解，避免复杂梯度计算，适用于从低维ODE模型到高保真PDE模拟的不同精度模型

Result: 方法成功再现了多种目标轨迹，包括实验研究中观察到的生物启发路径；在三球泳体模型上能适应并部分补偿壁面诱导的流体效应；在不同精度模型间表现一致

Conclusion: 贝叶斯优化作为多功能工具在复杂流体-结构相互作用下的微尺度运动最优控制策略中具有潜力，方法具有鲁棒性和通用性

Abstract: Trajectory tracking for microswimmers remains a key challenge in microrobotics, where low-Reynolds-number dynamics make control design particularly complex. In this work, we formulate the trajectory tracking problem as an optimal control problem and solve it using a combination of B-spline parametrization with Bayesian optimization, allowing the treatment of high computational costs without requiring complex gradient computations. Applied to a flagellated magnetic swimmer, the proposed method reproduces a variety of target trajectories, including biologically inspired paths observed in experimental studies. We further evaluate the approach on a three-sphere swimmer model, demonstrating that it can adapt to and partially compensate for wall-induced hydrodynamic effects. The proposed optimization strategy can be applied consistently across models of different fidelity, from low-dimensional ODE-based models to high-fidelity PDE-based simulations, showing its robustness and generality. These results highlight the potential of Bayesian optimization as a versatile tool for optimal control strategies in microscale locomotion under complex fluid-structure interactions.

</details>


### [14] [Sample-Efficient Real-World Dexterous Policy Fine-Tuning via Action-Chunked Critics and Normalizing Flows](https://arxiv.org/abs/2602.09580)
*Chenyu Yang,Denis Tarasov,Davide Liconti,Hehui Zheng,Robert K. Katzschmann*

Main category: cs.RO

TL;DR: SOFT-FLOW：一种基于归一化流的样本高效离线策略微调框架，用于解决灵巧操作中的多模态动作分布和长时程信用分配问题


<details>
  <summary>Details</summary>
Motivation: 现实世界灵巧操作策略微调面临两大挑战：1）现实交互预算有限，需要样本高效；2）动作分布高度多模态，传统高斯策略会坍缩，扩散策略无法进行基于似然的保守更新。此外，分块执行动作时，传统逐步critic无法有效进行信用分配。

Method: 提出SOFT-FLOW框架：1）使用归一化流（NF）策略，为多模态动作块提供精确似然，通过似然正则化实现保守稳定的策略更新；2）设计动作分块critic，评估整个动作序列，使价值估计与策略的时间结构对齐，改善长时程信用分配。

Result: 在两项真实机器人灵巧操作任务中验证：1）从盒子中取出剪刀剪胶带；2）掌心向下抓握下的魔方旋转。SOFT-FLOW在这些需要精确长时程控制的任务上实现了稳定、样本高效的适应，而标准方法则表现不佳。

Conclusion: SOFT-FLOW首次在真实机器人硬件上展示了基于似然的多模态生成策略与分块级价值学习的结合，为现实世界灵巧操作策略的样本高效微调提供了有效解决方案。

Abstract: Real-world fine-tuning of dexterous manipulation policies remains challenging due to limited real-world interaction budgets and highly multimodal action distributions. Diffusion-based policies, while expressive, do not permit conservative likelihood-based updates during fine-tuning because action probabilities are intractable. In contrast, conventional Gaussian policies collapse under multimodality, particularly when actions are executed in chunks, and standard per-step critics fail to align with chunked execution, leading to poor credit assignment. We present SOFT-FLOW, a sample-efficient off-policy fine-tuning framework with normalizing flow (NF) to address these challenges. The normalizing flow policy yields exact likelihoods for multimodal action chunks, allowing conservative, stable policy updates through likelihood regularization and thereby improving sample efficiency. An action-chunked critic evaluates entire action sequences, aligning value estimation with the policy's temporal structure and improving long-horizon credit assignment. To our knowledge, this is the first demonstration of a likelihood-based, multimodal generative policy combined with chunk-level value learning on real robotic hardware. We evaluate SOFT-FLOW on two challenging dexterous manipulation tasks in the real world: cutting tape with scissors retrieved from a case, and in-hand cube rotation with a palm-down grasp -- both of which require precise, dexterous control over long horizons. On these tasks, SOFT-FLOW achieves stable, sample-efficient adaptation where standard methods struggle.

</details>


### [15] [Preference Aligned Visuomotor Diffusion Policies for Deformable Object Manipulation](https://arxiv.org/abs/2602.09583)
*Marco Moletta,Michael C. Welle,Danica Kragic*

Main category: cs.RO

TL;DR: RKO：一种结合RPO和KTO优势的新型偏好对齐方法，用于适应预训练视觉运动扩散策略以反映有限演示中的偏好行为，在真实世界布料折叠任务中表现出优越性能


<details>
  <summary>Details</summary>
Motivation: 人类对操作任务有微妙、个性化且难以表达的偏好，机器人需要考虑这些偏好以提高个性化和用户满意度，但在可变形物体（如衣物和织物）操作中这一领域尚未充分探索

Method: 提出RKO方法，结合RPO和KTO两种框架的优势，通过有限演示来适应预训练的视觉运动扩散策略以反映偏好行为

Result: 在多种衣物和偏好设置的真实世界布料折叠任务中，偏好对齐策略（特别是RKO）相比标准扩散策略微调实现了更优越的性能和样本效率

Conclusion: 研究强调了结构化偏好学习对于在复杂可变形物体操作任务中扩展个性化机器人行为的重要性和可行性

Abstract: Humans naturally develop preferences for how manipulation tasks should be performed, which are often subtle, personal, and difficult to articulate. Although it is important for robots to account for these preferences to increase personalization and user satisfaction, they remain largely underexplored in robotic manipulation, particularly in the context of deformable objects like garments and fabrics. In this work, we study how to adapt pretrained visuomotor diffusion policies to reflect preferred behaviors using limited demonstrations. We introduce RKO, a novel preference-alignment method that combines the benefits of two recent frameworks: RPO and KTO. We evaluate RKO against common preference learning frameworks, including these two, as well as a baseline vanilla diffusion policy, on real-world cloth-folding tasks spanning multiple garments and preference settings. We show that preference-aligned policies (particularly RKO) achieve superior performance and sample efficiency compared to standard diffusion policy fine-tuning. These results highlight the importance and feasibility of structured preference learning for scaling personalized robot behavior in complex deformable object manipulation tasks.

</details>


### [16] [AnyTouch 2: General Optical Tactile Representation Learning For Dynamic Tactile Perception](https://arxiv.org/abs/2602.09617)
*Ruoxuan Feng,Yuxuan Zhou,Siyu Mei,Dongzhan Zhou,Pengwei Wang,Shaowei Cui,Bin Fang,Guocai Yao,Di Hu*

Main category: cs.RO

TL;DR: ToucHD大规模触觉数据集与AnyTouch 2统一触觉表征学习框架，通过层次化动态感知能力提升光学触觉传感器在真实世界接触式操作中的表现。


<details>
  <summary>Details</summary>
Motivation: 真实世界的接触式操作需要机器人感知时序触觉反馈、捕捉细微表面变形并推理物体属性和力动力学。现有触觉数据集和模型主要关注物体级属性（如材质），而忽视了物理交互中的细粒度触觉时序动态。

Method: 1) 提出ToucHD大规模层次化触觉数据集，涵盖触觉原子动作、真实世界操作和触觉-力配对数据；2) 提出AnyTouch 2通用触觉表征学习框架，统一物体级理解和细粒度、力感知的动态感知，捕捉像素级和动作特定的跨帧变形，并显式建模物理力动力学。

Result: 实验结果表明，该模型在涵盖静态物体属性和动态物理属性的基准测试中，以及在跨越多个动态感知能力层级的真实世界操作任务中（从基本物体级理解到力感知灵巧操作），在不同传感器和任务上都表现出一致且强大的性能。

Conclusion: 通过建立层次化触觉动态数据生态系统和统一触觉表征学习框架，成功解决了光学触觉传感器在动态触觉感知方面的局限性，为真实世界接触式操作提供了系统化的解决方案。

Abstract: Real-world contact-rich manipulation demands robots to perceive temporal tactile feedback, capture subtle surface deformations, and reason about object properties as well as force dynamics. Although optical tactile sensors are uniquely capable of providing such rich information, existing tactile datasets and models remain limited. These resources primarily focus on object-level attributes (e.g., material) while largely overlooking fine-grained tactile temporal dynamics during physical interactions. We consider that advancing dynamic tactile perception requires a systematic hierarchy of dynamic perception capabilities to guide both data collection and model design. To address the lack of tactile data with rich dynamic information, we present ToucHD, a large-scale hierarchical tactile dataset spanning tactile atomic actions, real-world manipulations, and touch-force paired data. Beyond scale, ToucHD establishes a comprehensive tactile dynamic data ecosystem that explicitly supports hierarchical perception capabilities from the data perspective. Building on it, we propose AnyTouch 2, a general tactile representation learning framework for diverse optical tactile sensors that unifies object-level understanding with fine-grained, force-aware dynamic perception. The framework captures both pixel-level and action-specific deformations across frames, while explicitly modeling physical force dynamics, thereby learning multi-level dynamic perception capabilities from the model perspective. We evaluate our model on benchmarks that covers static object properties and dynamic physical attributes, as well as real-world manipulation tasks spanning multiple tiers of dynamic perception capabilities-from basic object-level understanding to force-aware dexterous manipulation. Experimental results demonstrate consistent and strong performance across sensors and tasks.

</details>


### [17] [AutoFly: Vision-Language-Action Model for UAV Autonomous Navigation in the Wild](https://arxiv.org/abs/2602.09657)
*Xiaolou Sun,Wufei Si,Wenhui Ni,Yuntian Li,Dongming Wu,Fei Xie,Runwei Guan,He-Yang Xu,Henghui Ding,Yuan Wu,Yutao Yue,Yongming Huang,Hui Xiong*

Main category: cs.RO

TL;DR: AutoFly：用于无人机自主导航的端到端视觉-语言-动作模型，通过伪深度编码器和两阶段训练策略，在缺乏详细指令的真实户外环境中实现连续规划和避障。


<details>
  <summary>Details</summary>
Motivation: 当前无人机视觉语言导航研究依赖预定的详细指令和固定路线，但真实户外探索通常在未知环境中进行，只能提供粗略的位置或方向指导，需要无人机自主进行连续规划和障碍物避让。

Method: 提出AutoFly端到端VLA模型，包含伪深度编码器从RGB输入提取深度感知特征以增强空间推理，采用渐进式两阶段训练策略对齐视觉、深度和语言表示与动作策略。同时构建新的自主导航数据集，从指令跟随转向自主行为建模。

Result: AutoFly相比最先进的VLA基线实现了3.9%的成功率提升，在模拟和真实环境中表现一致。新数据集解决了现有VLN数据集对显式指令跟随依赖过重和真实数据不足的问题。

Conclusion: AutoFly通过深度感知特征和两阶段训练策略，有效解决了无人机在真实户外环境中的自主导航问题，将VLN范式从指令跟随扩展到自主行为建模，为具身AI的实际应用提供了新思路。

Abstract: Vision-language navigation (VLN) requires intelligent agents to navigate environments by interpreting linguistic instructions alongside visual observations, serving as a cornerstone task in Embodied AI. Current VLN research for unmanned aerial vehicles (UAVs) relies on detailed, pre-specified instructions to guide the UAV along predetermined routes. However, real-world outdoor exploration typically occurs in unknown environments where detailed navigation instructions are unavailable. Instead, only coarse-grained positional or directional guidance can be provided, requiring UAVs to autonomously navigate through continuous planning and obstacle avoidance. To bridge this gap, we propose AutoFly, an end-to-end Vision-Language-Action (VLA) model for autonomous UAV navigation. AutoFly incorporates a pseudo-depth encoder that derives depth-aware features from RGB inputs to enhance spatial reasoning, coupled with a progressive two-stage training strategy that effectively aligns visual, depth, and linguistic representations with action policies. Moreover, existing VLN datasets have fundamental limitations for real-world autonomous navigation, stemming from their heavy reliance on explicit instruction-following over autonomous decision-making and insufficient real-world data. To address these issues, we construct a novel autonomous navigation dataset that shifts the paradigm from instruction-following to autonomous behavior modeling through: (1) trajectory collection emphasizing continuous obstacle avoidance, autonomous planning, and recognition workflows; (2) comprehensive real-world data integration. Experimental results demonstrate that AutoFly achieves a 3.9% higher success rate compared to state-of-the-art VLA baselines, with consistent performance across simulated and real environments.

</details>


### [18] [RANT: Ant-Inspired Multi-Robot Rainforest Exploration Using Particle Filter Localisation and Virtual Pheromone Coordination](https://arxiv.org/abs/2602.09661)
*Ameer Alhashemi,Layan Abdulhadi,Karam Abuodeh,Tala Baghdadi,Suryanarayana Datla*

Main category: cs.RO

TL;DR: RANT是一个受蚂蚁启发的多机器人探索框架，用于噪声不确定环境，结合粒子滤波定位、行为控制器和虚拟信息素协调机制，在10x10米地形中探索隐藏的丰富度场。


<details>
  <summary>Details</summary>
Motivation: 在噪声和不确定环境中，多机器人系统需要有效的探索策略来覆盖未知区域、检测热点区域并减少冗余探索。传统方法在定位精度、协调效率和热点利用方面存在挑战。

Method: RANT框架包含三个核心组件：1)粒子滤波定位处理噪声环境下的定位问题；2)基于行为控制器结合梯度驱动的热点利用策略；3)轻量级无重复访问协调机制，基于虚拟信息素阻塞来避免重复探索。

Result: 实验分析表明：粒子滤波对可靠的热点参与至关重要；协调机制显著减少重叠探索；增加团队规模能提高覆盖率但存在收益递减现象，因为干扰增加。

Conclusion: RANT框架成功展示了在噪声不确定环境中多机器人探索的有效性，粒子滤波定位、协调机制和适当团队规模是实现高效探索的关键因素。

Abstract: This paper presents RANT, an ant-inspired multi-robot exploration framework for noisy, uncertain environments. A team of differential-drive robots navigates a 10 x 10 m terrain, collects noisy probe measurements of a hidden richness field, and builds local probabilistic maps while the supervisor maintains a global evaluation. RANT combines particle-filter localisation, a behaviour-based controller with gradient-driven hotspot exploitation, and a lightweight no-revisit coordination mechanism based on virtual pheromone blocking. We experimentally analyse how team size, localisation fidelity, and coordination influence coverage, hotspot recall, and redundancy. Results show that particle filtering is essential for reliable hotspot engagement, coordination substantially reduces overlap, and increasing team size improves coverage but yields diminishing returns due to interference.

</details>


### [19] [Fast Motion Planning for Non-Holonomic Mobile Robots via a Rectangular Corridor Representation of Structured Environments](https://arxiv.org/abs/2602.09714)
*Alejandro Gonzalez-Garcia,Sebastiaan Wyns,Sonia De Santis,Jan Swevers,Wilm Decré*

Main category: cs.RO

TL;DR: 提出一个用于非完整自主移动机器人在复杂结构化环境中快速运动规划的完整框架，通过确定性自由空间分解创建重叠矩形走廊的紧凑图，显著减少搜索空间，实现高效大规模导航。


<details>
  <summary>Details</summary>
Motivation: 传统基于网格的规划器在可扩展性方面存在困难，而许多运动学可行的规划器由于搜索空间复杂度高而带来显著计算负担。需要一种既能处理复杂结构化环境，又能保持高效计算性能的运动规划方法。

Method: 引入确定性自由空间分解方法，创建重叠矩形走廊的紧凑图，显著减少搜索空间而不牺牲路径分辨率。然后在线进行运动规划，找到矩形序列并使用解析规划器生成接近时间最优、运动学可行的轨迹。

Result: 该框架实现了高效的大规模导航解决方案，通过广泛的仿真和物理机器人实验验证了其有效性。实现已作为开源软件公开可用。

Conclusion: 提出的框架为非完整自主移动机器人在高度复杂但结构化环境中提供了快速运动规划的完整解决方案，通过创新的空间分解方法平衡了计算效率和路径质量。

Abstract: We present a complete framework for fast motion planning of non-holonomic autonomous mobile robots in highly complex but structured environments. Conventional grid-based planners struggle with scalability, while many kinematically-feasible planners impose a significant computational burden due to their search space complexity. To overcome these limitations, our approach introduces a deterministic free-space decomposition that creates a compact graph of overlapping rectangular corridors. This method enables a significant reduction in the search space, without sacrificing path resolution. The framework then performs online motion planning by finding a sequence of rectangles and generating a near-time-optimal, kinematically-feasible trajectory using an analytical planner. The result is a highly efficient solution for large-scale navigation. We validate our framework through extensive simulations and on a physical robot. The implementation is publicly available as open-source software.

</details>


### [20] [NavDreamer: Video Models as Zero-Shot 3D Navigators](https://arxiv.org/abs/2602.09765)
*Xijie Huang,Weiqi Gai,Tianyue Wu,Congyu Wang,Zhiyang Liu,Xin Zhou,Yuze Wu,Fei Gao*

Main category: cs.RO

TL;DR: NavDreamer是一个基于视频的3D导航框架，利用生成式视频模型作为语言指令和导航轨迹之间的通用接口，通过视频的时空信息编码和物理动态捕捉实现零样本泛化。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言-动作模型在导航任务中存在关键限制：数据稀缺且收集成本高，静态表示无法捕捉时间动态和物理规律。需要一种能利用互联网规模视频数据并编码时空信息的解决方案。

Method: 提出NavDreamer框架：1）使用生成式视频模型作为语言指令到导航轨迹的接口；2）引入基于采样的优化方法，利用VLM进行轨迹评分和选择以缓解生成预测的随机性；3）采用逆动力学模型从生成的视频计划解码可执行的导航路径点。

Result: 实验表明NavDreamer在多个视频模型骨干上实现了强大的零样本泛化能力，能够适应新物体和未见环境。消融研究揭示导航的高层决策特性特别适合基于视频的规划。

Conclusion: 视频的时空信息编码能力和互联网规模可用性使其成为导航任务的强大接口。NavDreamer框架通过生成式视频规划和优化方法，为3D导航提供了有效的零样本解决方案。

Abstract: Previous Vision-Language-Action models face critical limitations in navigation: scarce, diverse data from labor-intensive collection and static representations that fail to capture temporal dynamics and physical laws. We propose NavDreamer, a video-based framework for 3D navigation that leverages generative video models as a universal interface between language instructions and navigation trajectories. Our main hypothesis is that video's ability to encode spatiotemporal information and physical dynamics, combined with internet-scale availability, enables strong zero-shot generalization in navigation. To mitigate the stochasticity of generative predictions, we introduce a sampling-based optimization method that utilizes a VLM for trajectory scoring and selection. An inverse dynamics model is employed to decode executable waypoints from generated video plans for navigation. To systematically evaluate this paradigm in several video model backbones, we introduce a comprehensive benchmark covering object navigation, precise navigation, spatial grounding, language control, and scene reasoning. Extensive experiments demonstrate robust generalization across novel objects and unseen environments, with ablation studies revealing that navigation's high-level decision-making nature makes it particularly suited for video-based planning.

</details>


### [21] [Diverse Skill Discovery for Quadruped Robots via Unsupervised Learning](https://arxiv.org/abs/2602.09767)
*Ruopeng Cui,Yifei Bi,Haojie Luo,Wei Li*

Main category: cs.RO

TL;DR: 提出OMoE架构和多判别器框架，解决无监督技能发现中的表示重叠和奖励欺骗问题，在四足机器人上实现多样化的运动技能学习。


<details>
  <summary>Details</summary>
Motivation: 强化学习需要专家精心设计奖励函数，模仿学习需要昂贵的任务特定数据。现有无监督技能发现方法存在两个关键问题：1）使用单一策略学习多样行为，未建模行为间的共享结构和差异，导致学习效率低；2）容易发生奖励欺骗，奖励信号快速收敛但实际技能多样性不足。

Method: 提出正交专家混合（OMoE）架构，防止多样行为在表示空间中重叠；设计多判别器框架，不同判别器在不同观察空间上操作，有效缓解奖励欺骗问题。

Result: 在12-DOF Unitree A1四足机器人上评估，展示了多样化的运动技能。实验表明，所提框架提升了训练效率，状态空间覆盖率比基线方法提高了18.3%。

Conclusion: 通过OMoE架构和多判别器框架，解决了无监督技能发现中的表示重叠和奖励欺骗问题，实现了高效且多样化的运动技能学习。

Abstract: Reinforcement learning necessitates meticulous reward shaping by specialists to elicit target behaviors, while imitation learning relies on costly task-specific data. In contrast, unsupervised skill discovery can potentially reduce these burdens by learning a diverse repertoire of useful skills driven by intrinsic motivation. However, existing methods exhibit two key limitations: they typically rely on a single policy to master a versatile repertoire of behaviors without modeling the shared structure or distinctions among them, which results in low learning efficiency; moreover, they are susceptible to reward hacking, where the reward signal increases and converges rapidly while the learned skills display insufficient actual diversity. In this work, we introduce an Orthogonal Mixture-of-Experts (OMoE) architecture that prevents diverse behaviors from collapsing into overlapping representations, enabling a single policy to master a wide spectrum of locomotion skills. In addition, we design a multi-discriminator framework in which different discriminators operate on distinct observation spaces, effectively mitigating reward hacking. We evaluated our method on the 12-DOF Unitree A1 quadruped robot, demonstrating a diverse set of locomotion skills. Our experiments demonstrate that the proposed framework boosts training efficiency and yields an 18.3\% expansion in state-space coverage compared to the baseline.

</details>


### [22] [BagelVLA: Enhancing Long-Horizon Manipulation via Interleaved Vision-Language-Action Generation](https://arxiv.org/abs/2602.09849)
*Yucheng Hu,Jianke Zhang,Yuanfei Luo,Yanjiang Guo,Xiaoyu Chen,Xinshu Sun,Kun Feng,Qingzhou Lu,Sheng Chen,Yangang Zhang,Wei Li,Jianyu Chen*

Main category: cs.RO

TL;DR: BagelVLA是一个统一模型，集成了语言规划、视觉预测和动作生成，通过残差流引导技术实现多模态耦合，在复杂长时程操作任务中显著优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言-动作模型通常单独处理语言规划或视觉预测，缺乏两者协同指导动作生成的能力，导致在复杂长时程操作任务中表现不佳。

Method: 提出BagelVLA统一框架，从预训练的统一理解和生成模型初始化，训练时将文本推理和视觉预测交织到动作执行循环中，并引入残差流引导技术，从当前观察初始化，利用单步去噪提取预测性视觉特征来指导动作生成。

Result: 在多个模拟和真实世界基准测试中，BagelVLA显著优于现有基线方法，特别是在需要多阶段推理的任务中表现突出。

Conclusion: 通过统一语言规划、视觉预测和动作生成，并引入高效的残差流引导技术，BagelVLA能够更好地处理复杂长时程操作任务，为具身智能提供了更强大的推理和执行能力。

Abstract: Equipping embodied agents with the ability to reason about tasks, foresee physical outcomes, and generate precise actions is essential for general-purpose manipulation. While recent Vision-Language-Action (VLA) models have leveraged pre-trained foundation models, they typically focus on either linguistic planning or visual forecasting in isolation. These methods rarely integrate both capabilities simultaneously to guide action generation, leading to suboptimal performance in complex, long-horizon manipulation tasks. To bridge this gap, we propose BagelVLA, a unified model that integrates linguistic planning, visual forecasting, and action generation within a single framework. Initialized from a pretrained unified understanding and generative model, BagelVLA is trained to interleave textual reasoning and visual prediction directly into the action execution loop. To efficiently couple these modalities, we introduce Residual Flow Guidance (RFG), which initializes from current observation and leverages single-step denoising to extract predictive visual features, guiding action generation with minimal latency. Extensive experiments demonstrate that BagelVLA outperforms existing baselines by a significant margin on multiple simulated and real-world benchmarks, particularly in tasks requiring multi-stage reasoning.

</details>


### [23] [TriPilot-FF: Coordinated Whole-Body Teleoperation with Force Feedback](https://arxiv.org/abs/2602.09888)
*Zihao Li,Yanan Zhou,Ranpeng Qiu,Hangyu Wu,Guoqiang Ren,Weiming Zhi*

Main category: cs.RO

TL;DR: TriPilot-FF是一个用于双臂移动机械手的全身遥操作系统，通过脚控踏板配合激光雷达触觉反馈，实现更直观的避障和协调控制


<details>
  <summary>Details</summary>
Motivation: 移动机械手扩大了机器人操作范围，但全身遥操作仍然困难：操作者需要协调轮式底座和双臂，同时考虑障碍物和接触。现有接口主要是手控（如VR控制器和操纵杆），脚控通道未被充分探索用于连续底座控制。

Method: 开发了TriPilot-FF开源系统，包括脚控踏板配合激光雷达驱动的踏板触觉反馈，以及上半身双臂主从遥操作。仅使用低成本底座安装的激光雷达，根据指令方向上的障碍物接近度信号生成阻力踏板提示，引导操作者避免碰撞。系统还支持手臂侧力反馈用于接触感知，并提供实时力和视觉引导的双臂可操作性提示，以提示移动底座重新定位。

Result: TriPilot-FF能够有效"协同驾驶"人类操作者完成长时间任务，需要精确移动底座移动和协调的任务。将遥操作反馈信号整合到ACT策略中，当有额外信息可用时表现出改进的性能。

Conclusion: TriPilot-FF通过创新的脚控踏板触觉反馈系统，解决了移动机械手全身遥操作的协调问题，提高了操作效率和安全性，并为机器人学习提供了有价值的反馈信号。

Abstract: Mobile manipulators broaden the operational envelope for robot manipulation. However, the whole-body teleoperation of such robots remains a problem: operators must coordinate a wheeled base and two arms while reasoning about obstacles and contact. Existing interfaces are predominantly hand-centric (e.g., VR controllers and joysticks), leaving foot-operated channels underexplored for continuous base control. We present TriPilot-FF, an open-source whole-body teleoperation system for a custom bimanual mobile manipulator that introduces a foot-operated pedal with lidar-driven pedal haptics, coupled with upper-body bimanual leader-follower teleoperation. Using only a low-cost base-mounted lidar, TriPilot-FF renders a resistive pedal cue from proximity-to-obstacle signals in the commanded direction, shaping operator commands toward collision-averse behaviour without an explicit collision-avoidance controller. The system also supports arm-side force reflection for contact awareness and provides real-time force and visual guidance of bimanual manipulability to prompt mobile base repositioning, thereby improving reach. We demonstrate the capability of TriPilot-FF to effectively ``co-pilot'' the human operator over long time-horizons and tasks requiring precise mobile base movement and coordination. Finally, we incorporate teleoperation feedback signals into an Action Chunking with Transformers (ACT) policy and demonstrate improved performance when the additional information is available. We release the pedal device design, full software stack, and conduct extensive real-world evaluations on a bimanual wheeled platform. The project page of TriPilot-FF is http://bit.ly/46H3ZJT.

</details>


### [24] [TaCo: A Benchmark for Lossless and Lossy Codecs of Heterogeneous Tactile Data](https://arxiv.org/abs/2602.09893)
*Zhengxue Cheng,Yan Zhao,Keyu Wang,Hengdi Zhang,Li Song*

Main category: cs.RO

TL;DR: TaCo是首个触觉数据编解码器综合基准，评估了30种压缩方法在5个数据集上的性能，并开发了专门针对触觉数据训练的TaCo-LL（无损）和TaCo-L（有损）编解码器。


<details>
  <summary>Details</summary>
Motivation: 触觉传感对具身智能至关重要，但在严格带宽限制下，高效的触觉数据压缩对于实时机器人应用仍未被充分探索。触觉数据固有的异质性和时空复杂性进一步加剧了这一挑战。

Method: 引入TaCo基准，评估30种压缩方法（包括现成压缩算法和神经编解码器），在5个不同传感器类型的数据集上，系统评估无损和有损压缩方案在四个关键任务上的表现：无损存储、人类可视化、材料和物体分类、灵巧机器人抓取。特别开发了专门针对触觉数据训练的数据驱动编解码器TaCo-LL（无损）和TaCo-L（有损）。

Result: 结果验证了TaCo-LL和TaCo-L的优越性能。该基准为理解压缩效率与任务性能之间的关键权衡提供了基础框架。

Conclusion: TaCo基准为触觉数据压缩研究奠定了基础，推动了触觉感知领域的未来发展，特别强调了在机器人应用中平衡压缩效率与任务性能的重要性。

Abstract: Tactile sensing is crucial for embodied intelligence, providing fine-grained perception and control in complex environments. However, efficient tactile data compression, which is essential for real-time robotic applications under strict bandwidth constraints, remains underexplored. The inherent heterogeneity and spatiotemporal complexity of tactile data further complicate this challenge. To bridge this gap, we introduce TaCo, the first comprehensive benchmark for Tactile data Codecs. TaCo evaluates 30 compression methods, including off-the-shelf compression algorithms and neural codecs, across five diverse datasets from various sensor types. We systematically assess both lossless and lossy compression schemes on four key tasks: lossless storage, human visualization, material and object classification, and dexterous robotic grasping. Notably, we pioneer the development of data-driven codecs explicitly trained on tactile data, TaCo-LL (lossless) and TaCo-L (lossy). Results have validated the superior performance of our TaCo-LL and TaCo-L. This benchmark provides a foundational framework for understanding the critical trade-offs between compression efficiency and task performance, paving the way for future advances in tactile perception.

</details>


### [25] [Hydra-Nav: Object Navigation via Adaptive Dual-Process Reasoning](https://arxiv.org/abs/2602.09972)
*Zixuan Wang,Huang Fang,Shaoan Wang,Yuanfei Luo,Heng Dong,Wei Li,Yiming Gan*

Main category: cs.RO

TL;DR: Hydra-Nav是一个用于物体目标导航的统一视觉语言模型架构，通过自适应切换慢速推理系统和快速执行系统，在保持高效的同时提升导航成功率。


<details>
  <summary>Details</summary>
Motivation: 当前基于视觉语言模型的物体目标导航方法存在成功率低、未见物体定位效率差的问题，主要原因是时空推理能力弱。虽然已有尝试通过注入推理能力来改进，但带来了巨大的计算开销。

Method: 提出Hydra-Nav统一架构，包含：1）慢速系统用于分析探索历史并制定高层计划；2）快速系统用于高效执行。采用三阶段课程训练：空间-动作对齐、记忆-推理集成、迭代拒绝微调。

Result: 在HM3D、MP3D和OVON基准测试中达到最先进性能，分别比第二名方法高出11.1%、17.4%和21.2%。提出的SOT（操作时间加权成功率）指标显示自适应推理显著提升了搜索效率。

Conclusion: Hydra-Nav通过自适应切换慢速推理和快速执行系统，有效解决了物体目标导航中推理能力不足和计算效率低下的双重挑战，实现了性能与效率的平衡。

Abstract: While large vision-language models (VLMs) show promise for object goal navigation, current methods still struggle with low success rates and inefficient localization of unseen objects--failures primarily attributed to weak temporal-spatial reasoning. Meanwhile, recent attempts to inject reasoning into VLM-based agents improve success rates but incur substantial computational overhead. To address both the ineffectiveness and inefficiency of existing approaches, we introduce Hydra-Nav, a unified VLM architecture that adaptively switches between a deliberative slow system for analyzing exploration history and formulating high-level plans, and a reactive fast system for efficient execution. We train Hydra-Nav through a three-stage curriculum: (i) spatial-action alignment to strengthen trajectory planning, (ii) memory-reasoning integration to enhance temporal-spatial reasoning over long-horizon exploration, and (iii) iterative rejection fine-tuning to enable selective reasoning at critical decision points. Extensive experiments demonstrate that Hydra-Nav achieves state-of-the-art performance on the HM3D, MP3D, and OVON benchmarks, outperforming the second-best methods by 11.1%, 17.4%, and 21.2%, respectively. Furthermore, we introduce SOT (Success weighted by Operation Time), a new metric to measure search efficiency across VLMs with varying reasoning intensity. Results show that adaptive reasoning significantly enhances search efficiency over fixed-frequency baselines.

</details>


### [26] [RoboInter: A Holistic Intermediate Representation Suite Towards Robotic Manipulation](https://arxiv.org/abs/2602.09973)
*Hao Li,Ziqin Wang,Zi-han Ding,Shuai Yang,Yilun Chen,Yang Tian,Xiaolin Hu,Tai Wang,Dahua Lin,Feng Zhao,Si Liu,Jiangmiao Pang*

Main category: cs.RO

TL;DR: RoboInter Manipulation Suite是一个统一的机器人操作资源，包括数据、基准和中间表示模型，旨在解决现有VLA系统数据成本高、覆盖不足的问题。


<details>
  <summary>Details</summary>
Motivation: 现有机器人操作数据集成本高、特定于具体实现、覆盖范围和多样性不足，阻碍了VLA模型的泛化能力。当前"先规划后执行"范式需要额外的中间监督，但现有数据集缺乏这种监督。

Method: 引入RoboInter Manipulation Suite：包括RoboInter-Tool（轻量级GUI用于半自动标注）、RoboInter-Data（大规模数据集，23万+片段，571个场景，10+类中间表示密集标注）、RoboInter-VQA（29个空间和时间VQA类别基准）、RoboInter-VLA（集成规划-执行框架）。

Result: 创建了规模和质量远超先前工作的数据集，提供密集的中间表示标注，建立了系统化的VQA基准，支持模块化和端到端的VLA变体。

Conclusion: RoboInter通过精细多样的中间表示为推进稳健和可泛化的机器人学习奠定了实践基础，解决了现有VLA系统的数据瓶颈问题。

Abstract: Advances in large vision-language models (VLMs) have stimulated growing interest in vision-language-action (VLA) systems for robot manipulation. However, existing manipulation datasets remain costly to curate, highly embodiment-specific, and insufficient in coverage and diversity, thereby hindering the generalization of VLA models. Recent approaches attempt to mitigate these limitations via a plan-then-execute paradigm, where high-level plans (e.g., subtasks, trace) are first generated and subsequently translated into low-level actions, but they critically rely on extra intermediate supervision, which is largely absent from existing datasets. To bridge this gap, we introduce the RoboInter Manipulation Suite, a unified resource including data, benchmarks, and models of intermediate representations for manipulation. It comprises RoboInter-Tool, a lightweight GUI that enables semi-automatic annotation of diverse representations, and RoboInter-Data, a large-scale dataset containing over 230k episodes across 571 diverse scenes, which provides dense per-frame annotations over more than 10 categories of intermediate representations, substantially exceeding prior work in scale and annotation quality. Building upon this foundation, RoboInter-VQA introduces 9 spatial and 20 temporal embodied VQA categories to systematically benchmark and enhance the embodied reasoning capabilities of VLMs. Meanwhile, RoboInter-VLA offers an integrated plan-then-execute framework, supporting modular and end-to-end VLA variants that bridge high-level planning with low-level execution via intermediate supervision. In total, RoboInter establishes a practical foundation for advancing robust and generalizable robotic learning via fine-grained and diverse intermediate representations.

</details>


### [27] [A Collaborative Safety Shield for Safe and Efficient CAV Lane Changes in Congested On-Ramp Merging](https://arxiv.org/abs/2602.10007)
*Bharathkumar Hegde,Melanie Bouroche*

Main category: cs.RO

TL;DR: 提出MARL-MASS控制器，结合多智能体安全屏障(MASS)和强化学习，解决密集交通中换道时安全与效率的平衡问题


<details>
  <summary>Details</summary>
Motivation: 现有换道控制器要么只保证安全，要么只提升交通效率，未能同时考虑这两个冲突目标，特别是在密集交通场景下

Method: 使用控制屏障函数(CBFs)设计多智能体安全屏障(MASS)，通过简单算法构建交互拓扑图捕捉CAV间交互；将MASS集成到最先进的多智能体强化学习(MARL)换道控制器中，并定义定制奖励函数优先提升效率

Result: 在拥堵匝道合流仿真中，MASS能实现有安全保证的协作换道，严格尊重安全约束；定制奖励函数提高了带安全屏障的MARL策略稳定性；MARL-MASS有效平衡了安全保证与交通效率提升

Conclusion: 通过鼓励探索协作换道策略同时尊重安全约束，MARL-MASS在拥堵交通中有效平衡了安全与效率的权衡，代码已开源

Abstract: Lane changing in dense traffic is a significant challenge for Connected and Autonomous Vehicles (CAVs). Existing lane change controllers primarily either ensure safety or collaboratively improve traffic efficiency, but do not consider these conflicting objectives together. To address this, we propose the Multi-Agent Safety Shield (MASS), designed using Control Barrier Functions (CBFs) to enable safe and collaborative lane changes. The MASS enables collaboration by capturing multi-agent interactions among CAVs through interaction topologies constructed as a graph using a simple algorithm. Further, a state-of-the-art Multi-Agent Reinforcement Learning (MARL) lane change controller is extended by integrating MASS to ensure safety and defining a customised reward function to prioritise efficiency improvements. As a result, we propose a lane change controller, known as MARL-MASS, and evaluate it in a congested on-ramp merging simulation. The results demonstrate that MASS enables collaborative lane changes with safety guarantees by strictly respecting the safety constraints. Moreover, the proposed custom reward function improves the stability of MARL policies trained with a safety shield. Overall, by encouraging the exploration of a collaborative lane change policy while respecting safety constraints, MARL-MASS effectively balances the trade-off between ensuring safety and improving traffic efficiency in congested traffic. The code for MARL-MASS is available with an open-source licence at https://github.com/hkbharath/MARL-MASS

</details>


### [28] [Learning Force-Regulated Manipulation with a Low-Cost Tactile-Force-Controlled Gripper](https://arxiv.org/abs/2602.10013)
*Xuhui Kang,Tongxuan Tian,Sung-Wook Lee,Binghao Huang,Yunzhu Li,Yen-Ling Kuo*

Main category: cs.RO

TL;DR: TF-Gripper：低成本力控夹爪与RETAF框架，通过触觉反馈实现精确力调节，提升机器人对易碎物体的抓取性能


<details>
  <summary>Details</summary>
Motivation: 日常物体（如薯片）的抓取需要精确的力调节，但商用夹爪成本高或最小力过大，不适合研究力控策略学习。人类能通过触觉反馈快速适应力调节，希望赋予机器人这种能力。

Method: 1. 开发TF-Gripper：低成本（~150美元）力控平行夹爪，集成触觉反馈，力范围0.45-45N；2. 设计配套遥操作设备记录人力数据；3. 提出RETAF框架：将抓取力控制与手臂姿态预测解耦，高频使用手腕图像和触觉反馈调节力，基础策略预测末端姿态和夹爪开合。

Result: 在5个需要精确力调节的真实任务中评估：相比位置控制，直接力控制显著提升抓取稳定性和任务性能；触觉反馈对力调节至关重要；RETAF始终优于基线方法，并能与多种基础策略集成。

Conclusion: TF-Gripper和RETAF框架为机器人精确力控制提供了可行方案，触觉反馈是关键，有望推动力控策略学习的规模化发展。

Abstract: Successfully manipulating many everyday objects, such as potato chips, requires precise force regulation. Failure to modulate force can lead to task failure or irreversible damage to the objects. Humans can precisely achieve this by adapting force from tactile feedback, even within a short period of physical contact. We aim to give robots this capability. However, commercial grippers exhibit high cost or high minimum force, making them unsuitable for studying force-controlled policy learning with everyday force-sensitive objects. We introduce TF-Gripper, a low-cost (~$150) force-controlled parallel-jaw gripper that integrates tactile sensing as feedback. It has an effective force range of 0.45-45N and is compatible with different robot arms. Additionally, we designed a teleoperation device paired with TF-Gripper to record human-applied grasping forces. While standard low-frequency policies can be trained on this data, they struggle with the reactive, contact-dependent nature of force regulation. To overcome this, we propose RETAF (REactive Tactile Adaptation of Force), a framework that decouples grasping force control from arm pose prediction. RETAF regulates force at high frequency using wrist images and tactile feedback, while a base policy predicts end-effector pose and gripper open/close action. We evaluate TF-Gripper and RETAF across five real-world tasks requiring precise force regulation. Results show that compared to position control, direct force control significantly improves grasp stability and task performance. We further show that tactile feedback is essential for force regulation, and that RETAF consistently outperforms baselines and can be integrated with various base policies. We hope this work opens a path for scaling the learning of force-controlled policies in robotic manipulation. Project page: https://force-gripper.github.io .

</details>


### [29] [RoboSubtaskNet: Temporal Sub-task Segmentation for Human-to-Robot Skill Transfer in Real-World Environments](https://arxiv.org/abs/2602.10015)
*Dharmendra Sharma,Archit Sharma,John Reberio,Vaibhav Kesharwani,Peeyush Thakur,Narendra Kumar Dhar,Laxmidhar Behera*

Main category: cs.RO

TL;DR: RoboSubtaskNet：一种多阶段人机协作子任务分割框架，通过注意力增强的I3D特征和修改的MS-TCN结构，结合斐波那契扩张调度，在长视频中精确定位和分类机器人可执行的细粒度子任务。


<details>
  <summary>Details</summary>
Motivation: 在长未修剪视频中精确定位和分类细粒度子任务对于安全人机协作至关重要。与通用活动识别不同，协作操作需要可直接由机器人执行的子任务标签，以弥合视觉基准与控制之间的差距。

Method: 提出RoboSubtaskNet多阶段框架：1）使用注意力增强的I3D特征（RGB+光流）；2）采用修改的MS-TCN结构，使用斐波那契扩张调度以更好地捕捉短时程转换；3）使用复合损失函数（交叉熵+时间正则化器）减少过分割并鼓励有效的子任务进展。

Result: 在GTEA数据集上：F1@50=79.5%，Edit=88.6%，Acc=78.9%；在Breakfast数据集上：F1@50=30.4%，Edit=52.0%，Acc=53.5%；在RoboSubtask数据集上：F1@50=94.2%，Edit=95.6%，Acc=92.2%。在7-DoF Kinova Gen3机械臂上验证了端到端执行，总体任务成功率约91.25%。

Conclusion: RoboSubtaskNet在多个基准测试中优于MS-TCN和MS-TCN++，特别是在短时程转换任务上表现优异。通过RoboSubtask数据集和完整的感知到执行流程验证，展示了从子任务级视频理解到实际机器人操作的实用路径。

Abstract: Temporally locating and classifying fine-grained sub-task segments in long, untrimmed videos is crucial to safe human-robot collaboration. Unlike generic activity recognition, collaborative manipulation requires sub-task labels that are directly robot-executable. We present RoboSubtaskNet, a multi-stage human-to-robot sub-task segmentation framework that couples attention-enhanced I3D features (RGB plus optical flow) with a modified MS-TCN employing a Fibonacci dilation schedule to capture better short-horizon transitions such as reach-pick-place. The network is trained with a composite objective comprising cross-entropy and temporal regularizers (truncated MSE and a transition-aware term) to reduce over-segmentation and to encourage valid sub-task progressions. To close the gap between vision benchmarks and control, we introduce RoboSubtask, a dataset of healthcare and industrial demonstrations annotated at the sub-task level and designed for deterministic mapping to manipulator primitives. Empirically, RoboSubtaskNet outperforms MS-TCN and MS-TCN++ on GTEA and our RoboSubtask benchmark (boundary-sensitive and sequence metrics), while remaining competitive on the long-horizon Breakfast benchmark. Specifically, RoboSubtaskNet attains F1 @ 50 = 79.5%, Edit = 88.6%, Acc = 78.9% on GTEA; F1 @ 50 = 30.4%, Edit = 52.0%, Acc = 53.5% on Breakfast; and F1 @ 50 = 94.2%, Edit = 95.6%, Acc = 92.2% on RoboSubtask. We further validate the full perception-to-execution pipeline on a 7-DoF Kinova Gen3 manipulator, achieving reliable end-to-end behavior in physical trials (overall task success approx 91.25%). These results demonstrate a practical path from sub-task level video understanding to deployed robotic manipulation in real-world settings.

</details>


### [30] [A Collision-Free Sway Damping Model Predictive Controller for Safe and Reactive Forestry Crane Navigation](https://arxiv.org/abs/2602.10035)
*Marc-Philip Ecker,Christoph Fröhlich,Johannes Huemer,David Gruber,Bernhard Bischof,Tobias Glück,Wolfgang Kemmetmüller*

Main category: cs.RO

TL;DR: 首个将碰撞避免和载荷摆动控制统一在单一MPC框架中的林业起重机控制器，通过LiDAR环境映射和欧几里得距离场实现实时环境适应


<details>
  <summary>Details</summary>
Motivation: 林业起重机在动态、非结构化的户外环境中作业，需要同时处理碰撞避免和载荷摆动控制问题。现有方法要么专注于摆动阻尼但使用预定义的碰撞自由路径，要么只在全局规划层面进行碰撞避免，缺乏统一解决方案

Method: 提出首个碰撞自由、摆动阻尼模型预测控制器(MPC)，将LiDAR环境映射通过在线欧几里得距离场(EDF)直接集成到MPC中，实现实时环境适应。控制器同时强制执行碰撞约束和阻尼载荷摆动

Result: 在真实林业起重机上的实验验证表明，该控制器能有效阻尼摆动并成功避免障碍物。能够：(i)在准静态环境变化时重新规划，(ii)在干扰下保持碰撞自由操作，(iii)当无绕过路径时提供安全停止

Conclusion: 提出的统一MPC框架成功解决了林业起重机同时进行碰撞避免和载荷摆动控制的挑战，通过实时环境适应实现了安全导航

Abstract: Forestry cranes operate in dynamic, unstructured outdoor environments where simultaneous collision avoidance and payload sway control are critical for safe navigation. Existing approaches address these challenges separately, either focusing on sway damping with predefined collision-free paths or performing collision avoidance only at the global planning level. We present the first collision-free, sway-damping model predictive controller (MPC) for a forestry crane that unifies both objectives in a single control framework. Our approach integrates LiDAR-based environment mapping directly into the MPC using online Euclidean distance fields (EDF), enabling real-time environmental adaptation. The controller simultaneously enforces collision constraints while damping payload sway, allowing it to (i) replan upon quasi-static environmental changes, (ii) maintain collision-free operation under disturbances, and (iii) provide safe stopping when no bypass exists. Experimental validation on a real forestry crane demonstrates effective sway damping and successful obstacle avoidance. A video can be found at https://youtu.be/tEXDoeLLTxA.

</details>


### [31] [UniVTAC: A Unified Simulation Platform for Visuo-Tactile Manipulation Data Generation, Learning, and Benchmarking](https://arxiv.org/abs/2602.10093)
*Baijun Chen,Weijie Wan,Tianxing Chen,Xianda Guo,Congsheng Xu,Yuanyang Qi,Haojie Zhang,Longyan Wu,Tianling Xu,Zixuan Li,Yizhe Wu,Rui Li,Xiaokang Yang,Ping Luo,Wei Sui,Yao Mu*

Main category: cs.RO

TL;DR: UniVTAC是一个基于仿真的视触觉数据合成平台，支持三种常用传感器，通过大规模仿真数据训练视触觉编码器，提升接触丰富操作任务的性能。


<details>
  <summary>Details</summary>
Motivation: 视觉-语言-动作策略在机器人操作中进展迅速，但接触丰富的操作任务（如插入）仅靠视觉难以稳健完成。真实世界获取大规模可靠触觉数据成本高且困难，缺乏统一评估平台限制了策略学习和系统分析。

Method: 提出UniVTAC仿真平台支持三种常用视触觉传感器，可扩展生成可控的接触交互数据；基于此训练UniVTAC编码器，使用设计的监督信号从大规模仿真数据学习触觉中心的视触觉表示；建立包含8个代表性任务的UniVTAC基准用于评估触觉驱动策略。

Result: 集成UniVTAC编码器在UniVTAC基准上平均成功率提升17.1%；真实世界机器人实验显示任务成功率提升25%。

Conclusion: UniVTAC平台解决了触觉数据获取难题，通过仿真合成大规模数据训练的有效编码器能显著提升接触丰富操作任务的性能，为视触觉感知研究提供了统一评估框架。

Abstract: Robotic manipulation has seen rapid progress with vision-language-action (VLA) policies. However, visuo-tactile perception is critical for contact-rich manipulation, as tasks such as insertion are difficult to complete robustly using vision alone. At the same time, acquiring large-scale and reliable tactile data in the physical world remains costly and challenging, and the lack of a unified evaluation platform further limits policy learning and systematic analysis. To address these challenges, we propose UniVTAC, a simulation-based visuo-tactile data synthesis platform that supports three commonly used visuo-tactile sensors and enables scalable and controllable generation of informative contact interactions. Based on this platform, we introduce the UniVTAC Encoder, a visuo-tactile encoder trained on large-scale simulation-synthesized data with designed supervisory signals, providing tactile-centric visuo-tactile representations for downstream manipulation tasks. In addition, we present the UniVTAC Benchmark, which consists of eight representative visuo-tactile manipulation tasks for evaluating tactile-driven policies. Experimental results show that integrating the UniVTAC Encoder improves average success rates by 17.1% on the UniVTAC Benchmark, while real-world robotic experiments further demonstrate a 25% improvement in task success. Our webpage is available at https://univtac.github.io/.

</details>


### [32] [VLA-JEPA: Enhancing Vision-Language-Action Model with Latent World Model](https://arxiv.org/abs/2602.10098)
*Jingwen Sun,Wenyao Zhang,Zekun Qi,Shaojie Ren,Zezhi Liu,Hanxin Zhu,Guangzhong Sun,Xin Jin,Zhibo Chen*

Main category: cs.RO

TL;DR: VLA-JEPA：一种新的视觉-语言-动作预训练框架，通过无泄漏状态预测学习鲁棒的动作相关状态转移表示，避免像素变化干扰


<details>
  <summary>Details</summary>
Motivation: 当前VLA预训练方法存在像素变化锚定问题，容易受到外观偏差、干扰运动和信息泄漏的影响，需要更鲁棒的动作相关状态转移表示学习方法

Method: 采用JEPA风格的无泄漏状态预测框架：目标编码器从未来帧生成潜在表示，学生路径只看到当前观察，未来信息仅作为监督目标而非输入；在潜在空间而非像素空间进行预测

Result: 在LIBERO、LIBERO-Plus、SimplerEnv和真实世界操作任务上，VLA-JEPA相比现有方法在泛化性和鲁棒性方面取得一致提升

Conclusion: VLA-JEPA通过无泄漏状态预测学习鲁棒的动作相关状态转移表示，简化了预训练流程，提高了VLA策略的泛化能力和鲁棒性

Abstract: Pretraining Vision-Language-Action (VLA) policies on internet-scale video is appealing, yet current latent-action objectives often learn the wrong thing: they remain anchored to pixel variation rather than action-relevant state transitions, making them vulnerable to appearance bias, nuisance motion, and information leakage. We introduce VLA-JEPA, a JEPA-style pretraining framework that sidesteps these pitfalls by design. The key idea is \emph{leakage-free state prediction}: a target encoder produces latent representations from future frames, while the student pathway sees only the current observation -- future information is used solely as supervision targets, never as input. By predicting in latent space rather than pixel space, VLA-JEPA learns dynamics abstractions that are robust to camera motion and irrelevant background changes. This yields a simple two-stage recipe -- JEPA pretraining followed by action-head fine-tuning -- without the multi-stage complexity of prior latent-action pipelines. Experiments on LIBERO, LIBERO-Plus, SimplerEnv and real-world manipulation tasks show that VLA-JEPA achieves consistent gains in generalization and robustness over existing methods.

</details>


### [33] [Robo3R: Enhancing Robotic Manipulation with Accurate Feed-Forward 3D Reconstruction](https://arxiv.org/abs/2602.10101)
*Sizhe Yang,Linning Xu,Hao Li,Juncheng Mu,Jia Zeng,Dahua Lin,Jiangmiao Pang*

Main category: cs.RO

TL;DR: Robo3R：一种用于机器人操作的实时3D重建模型，直接从RGB图像和机器人状态预测精确的度量尺度场景几何


<details>
  <summary>Details</summary>
Motivation: 现有3D感知方法存在局限性：深度传感器有噪声和材料敏感性，现有重建模型缺乏物理交互所需的精度和度量一致性，需要一种适用于机器人操作的可靠高质量3D几何重建方法

Method: 提出Robo3R前馈模型，联合推断尺度不变的局部几何和相对相机姿态，通过学习的全局相似变换统一到规范机器人坐标系；使用掩码点头生成精细点云，基于关键点的PnP公式精炼相机外参和全局对齐；在400万帧合成数据集Robo3R-4M上训练

Result: Robo3R在重建精度上持续优于最先进的重建方法和深度传感器；在下游任务（模仿学习、仿真到真实迁移、抓取合成、无碰撞运动规划）中观察到一致的性能提升

Conclusion: Robo3R展示了作为机器人操作替代3D感知模块的潜力，能够提供精确、度量尺度的场景几何，满足物理交互的精度要求

Abstract: 3D spatial perception is fundamental to generalizable robotic manipulation, yet obtaining reliable, high-quality 3D geometry remains challenging. Depth sensors suffer from noise and material sensitivity, while existing reconstruction models lack the precision and metric consistency required for physical interaction. We introduce Robo3R, a feed-forward, manipulation-ready 3D reconstruction model that predicts accurate, metric-scale scene geometry directly from RGB images and robot states in real time. Robo3R jointly infers scale-invariant local geometry and relative camera poses, which are unified into the scene representation in the canonical robot frame via a learned global similarity transformation. To meet the precision demands of manipulation, Robo3R employs a masked point head for sharp, fine-grained point clouds, and a keypoint-based Perspective-n-Point (PnP) formulation to refine camera extrinsics and global alignment. Trained on Robo3R-4M, a curated large-scale synthetic dataset with four million high-fidelity annotated frames, Robo3R consistently outperforms state-of-the-art reconstruction methods and depth sensors. Across downstream tasks including imitation learning, sim-to-real transfer, grasp synthesis, and collision-free motion planning, we observe consistent gains in performance, suggesting the promise of this alternative 3D sensing module for robotic manipulation.

</details>


### [34] [DexImit: Learning Bimanual Dexterous Manipulation from Monocular Human Videos](https://arxiv.org/abs/2602.10105)
*Juncheng Mu,Sizhe Yang,Yiming Bao,Hojin Bae,Tianming Wei,Linning Xu,Boyi Li,Huazhe Xu,Jiangmiao Pang*

Main category: cs.RO

TL;DR: DexImit是一个自动化框架，能够将单目人类操作视频转换为物理上合理的机器人数据，用于解决双手灵巧操作中的数据稀缺问题。


<details>
  <summary>Details</summary>
Motivation: 双手灵巧操作的数据稀缺严重限制了泛化能力，因为真实世界的数据收集成本高昂。人类操作视频作为操作知识的直接载体，具有扩展机器人学习的巨大潜力，但人类手与机器人灵巧手之间的本体差距使得直接从人类视频进行预训练极具挑战性。

Method: 采用四阶段生成流程：1）从任意视角重建具有近度量尺度的手-物体交互；2）执行子任务分解和双手调度；3）合成与演示交互一致的机器人轨迹；4）为零样本真实世界部署进行全面的数据增强。

Result: DexImit能够基于人类视频（来自互联网或视频生成模型）生成大规模机器人数据，能够处理多样化的操作任务，包括工具使用（如切苹果）、长时程任务（如制作饮料）和精细操作（如堆叠杯子）。

Conclusion: DexImit通过弥合人类手与机器人手之间的本体差距，释放了大规模人类操作视频数据的潜力，为机器人灵巧操作提供了可扩展的数据生成解决方案。

Abstract: Data scarcity fundamentally limits the generalization of bimanual dexterous manipulation, as real-world data collection for dexterous hands is expensive and labor-intensive. Human manipulation videos, as a direct carrier of manipulation knowledge, offer significant potential for scaling up robot learning. However, the substantial embodiment gap between human hands and robotic dexterous hands makes direct pretraining from human videos extremely challenging. To bridge this gap and unleash the potential of large-scale human manipulation video data, we propose DexImit, an automated framework that converts monocular human manipulation videos into physically plausible robot data, without any additional information. DexImit employs a four-stage generation pipeline: (1) reconstructing hand-object interactions from arbitrary viewpoints with near-metric scale; (2) performing subtask decomposition and bimanual scheduling; (3) synthesizing robot trajectories consistent with the demonstrated interactions; (4) comprehensive data augmentation for zero-shot real-world deployment. Building on these designs, DexImit can generate large-scale robot data based on human videos, either from the Internet or video generation models. DexImit is capable of handling diverse manipulation tasks, including tool use (e.g., cutting an apple), long-horizon tasks (e.g., making a beverage), and fine-grained manipulations (e.g., stacking cups).

</details>


### [35] [EgoHumanoid: Unlocking In-the-Wild Loco-Manipulation with Robot-Free Egocentric Demonstration](https://arxiv.org/abs/2602.10106)
*Modi Shi,Shijia Peng,Jin Chen,Haoran Jiang,Yinghui Li,Di Huang,Ping Luo,Hongyang Li,Li Chen*

Main category: cs.RO

TL;DR: EgoHumanoid框架首次利用大量人类自我中心视角演示与少量机器人数据联合训练视觉-语言-动作策略，使仿人机器人能够在多样真实环境中完成移动操作任务，性能比纯机器人基线提升51%。


<details>
  <summary>Details</summary>
Motivation: 人类演示提供了丰富的环境多样性和自然扩展性，是机器人遥操作的理想替代方案。虽然这一范式在机械臂操作方面取得了进展，但对于更具挑战性、数据需求更大的仿人机器人移动操作问题，其潜力尚未充分探索。

Method: 开发了EgoHumanoid框架，通过系统性对齐管道弥合人类与机器人之间的具身差距，包括硬件设计到数据处理。核心包括：1) 视图对齐减少相机高度和视角变化引起的视觉域差异；2) 动作对齐将人类动作映射到统一的、运动学可行的仿人机器人控制动作空间。建立了便携式人类数据收集系统和实用收集协议。

Result: 广泛的真实世界实验表明，结合无机器人自我中心数据显著优于纯机器人基线51%，特别是在未见环境中。分析揭示了哪些行为能有效迁移以及人类数据扩展的潜力。

Conclusion: EgoHumanoid首次展示了利用丰富人类演示与有限机器人数据联合训练仿人机器人移动操作策略的可行性，通过系统性对齐管道成功弥合了人类与机器人之间的具身差距，为数据驱动的仿人机器人控制开辟了新途径。

Abstract: Human demonstrations offer rich environmental diversity and scale naturally, making them an appealing alternative to robot teleoperation. While this paradigm has advanced robot-arm manipulation, its potential for the more challenging, data-hungry problem of humanoid loco-manipulation remains largely unexplored. We present EgoHumanoid, the first framework to co-train a vision-language-action policy using abundant egocentric human demonstrations together with a limited amount of robot data, enabling humanoids to perform loco-manipulation across diverse real-world environments. To bridge the embodiment gap between humans and robots, including discrepancies in physical morphology and viewpoint, we introduce a systematic alignment pipeline spanning from hardware design to data processing. A portable system for scalable human data collection is developed, and we establish practical collection protocols to improve transferability. At the core of our human-to-humanoid alignment pipeline lies two key components. The view alignment reduces visual domain discrepancies caused by camera height and perspective variation. The action alignment maps human motions into a unified, kinematically feasible action space for humanoid control. Extensive real-world experiments demonstrate that incorporating robot-free egocentric data significantly outperforms robot-only baselines by 51\%, particularly in unseen environments. Our analysis further reveals which behaviors transfer effectively and the potential for scaling human data.

</details>


### [36] [ST4VLA: Spatially Guided Training for Vision-Language-Action Models](https://arxiv.org/abs/2602.10109)
*Jinhui Ye,Fangjing Wang,Ning Gao,Junqiu Yu,Yangkun Zhu,Bin Wang,Jinyu Zhang,Weiyang Jin,Yanwei Fu,Feng Zheng,Yilun Chen,Jiangmiao Pang*

Main category: cs.RO

TL;DR: ST4VLA是一个双系统视觉-语言-动作框架，通过空间引导训练将动作学习与视觉语言模型中的空间先验对齐，显著提升了机器人任务性能。


<details>
  <summary>Details</summary>
Motivation: 大型视觉语言模型在多模态理解方面表现出色，但在需要将指令转化为低级运动动作的具身任务中表现不足。需要一种方法将空间理解与动作生成有效结合。

Method: ST4VLA采用双阶段设计：1) 空间基础预训练，通过点、框和轨迹预测为VLM提供可迁移的空间先验；2) 空间引导动作后训练，通过空间提示鼓励模型产生更丰富的空间先验来指导动作生成。

Result: 在Google Robot上性能从66.1提升到84.6，在WidowX Robot上从54.7提升到73.2，在SimplerEnv上达到新的SOTA。同时展现出对未见物体、改写指令的更强泛化能力，以及对长时域扰动的鲁棒性。

Conclusion: 空间引导训练是构建鲁棒、可泛化机器人学习的有前景方向，ST4VLA框架成功地将空间理解与动作生成相结合，显著提升了具身任务的性能。

Abstract: Large vision-language models (VLMs) excel at multimodal understanding but fall short when extended to embodied tasks, where instructions must be transformed into low-level motor actions. We introduce ST4VLA, a dual-system Vision-Language-Action framework that leverages Spatial Guided Training to align action learning with spatial priors in VLMs. ST4VLA includes two stages: (i) spatial grounding pre-training, which equips the VLM with transferable priors via scalable point, box, and trajectory prediction from both web-scale and robot-specific data, and (ii) spatially guided action post-training, which encourages the model to produce richer spatial priors to guide action generation via spatial prompting. This design preserves spatial grounding during policy learning and promotes consistent optimization across spatial and action objectives. Empirically, ST4VLA achieves substantial improvements over vanilla VLA, with performance increasing from 66.1 -> 84.6 on Google Robot and from 54.7 -> 73.2 on WidowX Robot, establishing new state-of-the-art results on SimplerEnv. It also demonstrates stronger generalization to unseen objects and paraphrased instructions, as well as robustness to long-horizon perturbations in real-world settings. These results highlight scalable spatially guided training as a promising direction for robust, generalizable robot learning. Source code, data and models are released at https://internrobotics.github.io/internvla-m1.github.io/

</details>


### [37] [Learning Agile Quadrotor Flight in the Real World](https://arxiv.org/abs/2602.10111)
*Yunfan Ren,Zhiyuan Zhu,Jiaxu Xing,Davide Scaramuzza*

Main category: cs.RO

TL;DR: 提出了一种自适应的四旋翼飞行控制框架，无需精确系统辨识或离线Sim2Real迁移，通过在线探索物理极限和残差学习实现高性能敏捷飞行


<details>
  <summary>Details</summary>
Motivation: 基于学习的控制器通常需要大量仿真训练和精确系统辨识，但固定策略难以应对分布外场景（外部气动扰动、硬件退化等），保守的安全边界限制了敏捷性，而在线适应面临数据稀缺和安全风险

Method: 提出自适应时间缩放(ATS)主动探索平台物理极限，在线残差学习增强简单标称模型，基于学习的混合模型进一步提出真实世界锚定短时域反向传播(RASH-BPTT)实现高效稳健的飞行中策略更新

Result: 四旋翼可靠执行接近执行器饱和极限的敏捷机动，保守基础策略峰值速度从1.9 m/s提升至7.3 m/s，仅需约100秒飞行时间，展示了真实世界适应不仅是补偿建模误差，更是持续性能提升的实用机制

Conclusion: 该自适应框架消除了对精确系统辨识和离线Sim2Real迁移的需求，通过在线探索和学习实现了在激进飞行状态下的持续性能改进，为安全高效的敏捷飞行控制提供了新途径

Abstract: Learning-based controllers have achieved impressive performance in agile quadrotor flight but typically rely on massive training in simulation, necessitating accurate system identification for effective Sim2Real transfer. However, even with precise modeling, fixed policies remain susceptible to out-of-distribution scenarios, ranging from external aerodynamic disturbances to internal hardware degradation. To ensure safety under these evolving uncertainties, such controllers are forced to operate with conservative safety margins, inherently constraining their agility outside of controlled settings. While online adaptation offers a potential remedy, safely exploring physical limits remains a critical bottleneck due to data scarcity and safety risks. To bridge this gap, we propose a self-adaptive framework that eliminates the need for precise system identification or offline Sim2Real transfer. We introduce Adaptive Temporal Scaling (ATS) to actively explore platform physical limits, and employ online residual learning to augment a simple nominal model. {Based on the learned hybrid model, we further propose Real-world Anchored Short-horizon Backpropagation Through Time (RASH-BPTT) to achieve efficient and robust in-flight policy updates. Extensive experiments demonstrate that our quadrotor reliably executes agile maneuvers near actuator saturation limits. The system evolves a conservative base policy with a peak speed of 1.9 m/s to 7.3 m/s within approximately 100 seconds of flight time. These findings underscore that real-world adaptation serves not merely to compensate for modeling errors, but as a practical mechanism for sustained performance improvement in aggressive flight regimes.

</details>


### [38] [Decoupled MPPI-Based Multi-Arm Motion Planning](https://arxiv.org/abs/2602.10114)
*Dan Evron,Elias Goldsztejn,Ronen I. Brafman*

Main category: cs.RO

TL;DR: MR-STORM：基于分布式采样的多机器人运动规划算法，扩展STORM算法处理多机器人协作，通过动态优先级和动态障碍物处理实现高效规划


<details>
  <summary>Details</summary>
Motivation: 现有基于采样的高自由度机械臂运动规划算法虽然可以利用GPU实现高性能，但在多机器人联合控制时扩展性差。需要开发能够有效处理多机器人协作的分布式规划算法。

Method: 1. 扩展STORM算法以处理动态障碍物；2. 让每个机械臂计算自己的运动规划前缀并与其他机械臂共享，其他机械臂将其视为动态障碍物；3. 添加动态优先级机制

Result: MR-STORM算法在静态和动态障碍物环境下相比现有最先进算法展现出明显的经验优势

Conclusion: MR-STORM成功解决了多机器人运动规划的扩展性问题，通过分布式处理和动态优先级机制实现了高效的多机器人协作规划

Abstract: Recent advances in sampling-based motion planning algorithms for high DOF arms leverage GPUs to provide SOTA performance. These algorithms can be used to control multiple arms jointly, but this approach scales poorly. To address this, we extend STORM, a sampling-based model-predictive-control (MPC) motion planning algorithm, to handle multiple robots in a distributed fashion. First, we modify STORM to handle dynamic obstacles. Then, we let each arm compute its own motion plan prefix, which it shares with the other arms, which treat it as a dynamic obstacle. Finally, we add a dynamic priority scheme. The new algorithm, MR-STORM, demonstrates clear empirical advantages over SOTA algorithms when operating with both static and dynamic obstacles.

</details>
