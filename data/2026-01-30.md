<div id=toc></div>

# Table of Contents

- [cs.RO](#cs.RO) [Total: 29]


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [1] [Quick Heuristic Validation of Edges in Dynamic Roadmap Graphs](https://arxiv.org/abs/2601.20968)
*Yulie Arad,Stav Ashur,Nancy M. Amato*

Main category: cs.RO

TL;DR: 本文提出"红-绿-灰"范式，改进SPITE方法，通过廉价启发式检查快速更新非静态环境中的机器人运动规划路线图。


<details>
  <summary>Details</summary>
Motivation: 解决机器人运动规划在非静态环境中路线图调整的问题，传统方法难以快速适应环境变化，需要高效更新路线图的有效性状态。

Method: 采用"红-绿-灰"分类范式，结合简单计算几何方法近似机器人扫掠体积，进行惰性碰撞检查，将边标记为无效(红)、有效(绿)或未知(灰)。

Result: 与Leven和Hutchinson的成熟技术相比，该方法提高了准确性，能够正确标记无效边，同时保持相当的更新运行时间。

Conclusion: "红-绿-灰"范式为机器人运动规划提供了一种有效的半惰性路线图更新方法，适用于动态变化的环境。

Abstract: In this paper we tackle the problem of adjusting roadmap graphs for robot motion planning to non-static environments. We introduce the "Red-Green-Gray" paradigm, a modification of the SPITE method, capable of classifying the validity status of nodes and edges using cheap heuristic checks, allowing fast semi-lazy roadmap updates. Given a roadmap, we use simple computational geometry methods to approximate the swept volumes of robots and perform lazy collision checks, and label a subset of the edges as invalid (red), valid (green), or unknown (gray). We present preliminary experimental results comparing our method to the well-established technique of Leven and Hutchinson, and showing increased accuracy as well as the ability to correctly label edges as invalid while maintaining comparable update runtimes.

</details>


### [2] [Meta-ROS: A Next-Generation Middleware Architecture for Adaptive and Scalable Robotic Systems](https://arxiv.org/abs/2601.21011)
*Anshul Ranjan,Anoosh Damodar,Neha Chougule,Dhruva S Nayak,Anantharaman P. N,Shylaja S S*

Main category: cs.RO

TL;DR: Meta-ROS是一个新型机器人中间件，旨在解决ROS2等现有框架的复杂性和互操作性问题，通过简化集成、提升性能和确保跨平台兼容性来优化机器人开发。


<details>
  <summary>Details</summary>
Motivation: 现有机器人中间件框架（如ROS2）存在复杂性和互操作性问题，对新开发者不友好，阻碍了机器人技术的普及和应用。

Method: Meta-ROS采用现代通信协议（如Zenoh和ZeroMQ），支持高效低延迟的跨硬件平台通信，并支持音频、图像、视频等多种数据类型。

Result: Meta-ROS在性能测试中优于ROS2，吞吐量提升高达30%，显著降低消息延迟，优化资源使用，并具备强大的硬件支持和开发者友好设计。

Conclusion: Meta-ROS通过简化集成、提升性能和确保跨平台兼容性，成为现代实时机器人AI应用的理想解决方案。

Abstract: The field of robotics faces significant challenges related to the complexity and interoperability of existing middleware frameworks, like ROS2, which can be difficult for new developers to adopt. To address these issues, we propose Meta-ROS, a novel middleware solution designed to streamline robotics development by simplifying integration, enhancing performance, and ensuring cross-platform compatibility. Meta-ROS leverages modern communication protocols, such as Zenoh and ZeroMQ, to enable efficient and low-latency communication across diverse hardware platforms, while also supporting various data types like audio, images, and video. We evaluated Meta-ROS's performance through comprehensive testing, comparing it with existing middleware frameworks like ROS1 and ROS2. The results demonstrated that Meta-ROS outperforms ROS2, achieving up to 30% higher throughput, significantly reducing message latency, and optimizing resource usage. Additionally, its robust hardware support and developer-centric design facilitate seamless integration and ease of use, positioning Meta-ROS as an ideal solution for modern, real-time robotics AI applications.

</details>


### [3] [Track-centric Iterative Learning for Global Trajectory Optimization in Autonomous Racing](https://arxiv.org/abs/2601.21027)
*Youngim Nam,Jungbin Kim,Kyungtae Kang,Cheolhyeon Kwon*

Main category: cs.RO

TL;DR: 提出一个用于自动驾驶赛车在不确定车辆动力学下最小化圈时的全局轨迹优化框架，通过贝叶斯优化在参数空间中探索全时域轨迹，并嵌入迭代学习循环来逐步优化轨迹。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要在跟踪层面学习动力学，而不更新轨迹本身来适应学习到的动力学。全局轨迹优化计算昂贵，且由于不确定的动力学，在现实世界中跟踪这样的轨迹难以保证全局最优性。

Method: 提出基于赛道中心的方法，直接学习和优化全时域轨迹。首先通过小波变换在赛道无关的参数空间中表示轨迹，然后使用贝叶斯优化高效探索该空间，通过模拟学习到的动力学来评估每个候选轨迹的圈时。将此优化嵌入迭代学习框架，部署优化轨迹收集真实数据来更新动力学模型，逐步迭代优化轨迹。

Result: 通过仿真和真实实验验证了所提框架的有效性，相比基准方法实现了高达20.7%的圈时提升，并持续优于最先进的方法。

Conclusion: 提出的全局轨迹优化框架能够有效处理不确定车辆动力学，通过迭代学习和贝叶斯优化逐步优化全时域轨迹，显著提升自动驾驶赛车的圈时性能。

Abstract: This paper presents a global trajectory optimization framework for minimizing lap time in autonomous racing under uncertain vehicle dynamics. Optimizing the trajectory over the full racing horizon is computationally expensive, and tracking such a trajectory in the real world hardly assures global optimality due to uncertain dynamics. Yet, existing work mostly focuses on dynamics learning at the tracking level, without updating the trajectory itself to account for the learned dynamics. To address these challenges, we propose a track-centric approach that directly learns and optimizes the full-horizon trajectory. We first represent trajectories through a track-agnostic parametric space in light of the wavelet transform. This space is then efficiently explored using Bayesian optimization, where the lap time of each candidate is evaluated by running simulations with the learned dynamics. This optimization is embedded in an iterative learning framework, where the optimized trajectory is deployed to collect real-world data for updating the dynamics, progressively refining the trajectory over the iterations. The effectiveness of the proposed framework is validated through simulations and real-world experiments, demonstrating lap time improvement of up to 20.7% over a nominal baseline and consistently outperforming state-of-the-art methods.

</details>


### [4] [WheelArm-Sim: A Manipulation and Navigation Combined Multimodal Synthetic Data Generation Simulator for Unified Control in Assistive Robotics](https://arxiv.org/abs/2601.21129)
*Guangping Liu,Tipu Sultan,Vittorio Di Giorgio,Nick Hawkins,Flavio Esposito,Madi Babaiasl*

Main category: cs.RO

TL;DR: 本文提出了WheelArm-Sim仿真框架，用于收集轮椅与机械臂集成控制的多模态数据集，为开发WheelArm集成控制系统提供数据基础。


<details>
  <summary>Details</summary>
Motivation: 虽然轮椅和机械臂分别能帮助上肢和行动受限者完成日常活动，但现有的辅助机器人研究大多独立关注轮椅或机械臂，缺乏对两者集成统一控制的机器学习模型研究。

Method: 开发了WheelArm-Sim仿真框架（基于Isaac Sim），用于合成数据收集。收集了包含13个任务、232条轨迹、67,783个样本的多模态数据集，涵盖操作和导航任务。

Result: 通过实现一个基础模型在芥末拾取任务中进行动作预测，验证了从WheelArm-Sim收集的数据可用于数据驱动的机器学习模型进行集成控制。

Conclusion: WheelArm-Sim仿真框架能够为轮椅与机械臂集成控制系统提供可行的数据收集方案，为开发数据驱动的WheelArm模型奠定了基础。

Abstract: Wheelchairs and robotic arms enhance independent living by assisting individuals with upper-body and mobility limitations in their activities of daily living (ADLs). Although recent advancements in assistive robotics have focused on Wheelchair-Mounted Robotic Arms (WMRAs) and wheelchairs separately, integrated and unified control of the combination using machine learning models remains largely underexplored. To fill this gap, we introduce the concept of WheelArm, an integrated cyber-physical system (CPS) that combines wheelchair and robotic arm controls. Data collection is the first step toward developing WheelArm models. In this paper, we present WheelArm-Sim, a simulation framework developed in Isaac Sim for synthetic data collection. We evaluate its capability by collecting a manipulation and navigation combined multimodal dataset, comprising 13 tasks, 232 trajectories, and 67,783 samples. To demonstrate the potential of the WheelArm dataset, we implement a baseline model for action prediction in the mustard-picking task. The results illustrate that data collected from WheelArm-Sim is feasible for a data-driven machine learning model for integrated control.

</details>


### [5] [InspecSafe-V1: A Multimodal Benchmark for Safety Assessment in Industrial Inspection Scenarios](https://arxiv.org/abs/2601.21173)
*Zeyi Liu,Shuang Liu,Jihai Min,Zhaoheng Zhang,Jun Cen,Pengyu Han,Songqiao Hu,Zihan Meng,Xiao He,Donghua Zhou*

Main category: cs.RO

TL;DR: InspecSafe-V1是首个用于工业检测安全评估的多模态基准数据集，包含真实工业环境中机器人采集的5,013个检测实例，覆盖5种工业场景，提供7种同步传感模态和像素级分割标注。


<details>
  <summary>Details</summary>
Motivation: 工业智能化和无人检测快速发展，但现有数据集多为模拟数据、单模态感知或缺乏细粒度标注，限制了工业基础模型的场景理解和多模态安全推理能力。

Method: 从真实工业环境中41个轮式和轨道式检测机器人在2,239个有效检测点采集数据，构建包含5,013个检测实例的数据集。提供可见光图像的像素级分割标注、语义场景描述和安全等级标签，并包含红外视频、音频、深度点云、雷达点云、气体测量、温度和湿度等7种同步传感模态。

Result: 创建了首个用于工业检测安全评估的多模态基准数据集InspecSafe-V1，覆盖隧道、电力设施、烧结设备、石油化工和煤炭输送栈桥等5种代表性工业场景，支持多模态异常识别、跨模态融合和综合安全评估。

Conclusion: InspecSafe-V1填补了工业检测安全评估领域真实多模态数据集的空白，为工业基础模型的发展提供了重要资源，能够支持更可靠的感知和安全评估，促进预测性维护和自主检测的部署。

Abstract: With the rapid development of industrial intelligence and unmanned inspection, reliable perception and safety assessment for AI systems in complex and dynamic industrial sites has become a key bottleneck for deploying predictive maintenance and autonomous inspection. Most public datasets remain limited by simulated data sources, single-modality sensing, or the absence of fine-grained object-level annotations, which prevents robust scene understanding and multimodal safety reasoning for industrial foundation models. To address these limitations, InspecSafe-V1 is released as the first multimodal benchmark dataset for industrial inspection safety assessment that is collected from routine operations of real inspection robots in real-world environments. InspecSafe-V1 covers five representative industrial scenarios, including tunnels, power facilities, sintering equipment, oil and gas petrochemical plants, and coal conveyor trestles. The dataset is constructed from 41 wheeled and rail-mounted inspection robots operating at 2,239 valid inspection sites, yielding 5,013 inspection instances. For each instance, pixel-level segmentation annotations are provided for key objects in visible-spectrum images. In addition, a semantic scene description and a corresponding safety level label are provided according to practical inspection tasks. Seven synchronized sensing modalities are further included, including infrared video, audio, depth point clouds, radar point clouds, gas measurements, temperature, and humidity, to support multimodal anomaly recognition, cross-modal fusion, and comprehensive safety assessment in industrial environments.

</details>


### [6] [Disturbance-Aware Flight Control of Robotic Gliding Blimp via Moving Mass Actuation](https://arxiv.org/abs/2601.21188)
*Hao Cheng,Feitian Zhang*

Main category: cs.RO

TL;DR: 本文提出了一种针对受风扰影响的机器人飞艇的扰动感知控制框架，结合移动水平估计器(MHE)和模型预测控制器(MPC)，通过2自由度移动质量机制实现姿态和航向控制，显著提升了在风扰环境下的飞行稳定性。


<details>
  <summary>Details</summary>
Motivation: 机器人飞艇作为轻于空气的航空系统，虽然具有长续航和固有安全性优势，但对风扰动高度敏感。现有研究缺乏针对LTA平台的扰动感知控制框架，需要开发能够明确建模和补偿风致效应的控制系统。

Method: 采用移动水平估计器(MHE)实时推断风扰动，并将估计结果提供给模型预测控制器(MPC)。利用2自由度移动质量机制产生惯性和气动力矩，实现姿态和航向控制。MHE-MPC集成框架能够对风扰进行建模和补偿。

Result: 在顶风和侧风条件下的广泛飞行实验表明，集成的MHE-MPC框架显著优于基准PID控制，证明了其在扰动感知LTA飞行中的有效性，增强了风扰环境下的飞行稳定性。

Conclusion: 提出的MHE-MPC集成框架成功解决了LTA平台对风扰敏感的问题，通过实时风扰动估计和预测控制，结合移动质量机制，实现了在变化风条件下的鲁棒轨迹和航向调节，为扰动环境下的机器人飞艇控制提供了有效解决方案。

Abstract: Robotic blimps, as lighter-than-air (LTA) aerial systems, offer long endurance and inherently safe operation but remain highly susceptible to wind disturbances. Building on recent advances in moving mass actuation, this paper addresses the lack of disturbance-aware control frameworks for LTA platforms by explicitly modeling and compensating for wind-induced effects. A moving horizon estimator (MHE) infers real-time wind perturbations and provides these estimates to a model predictive controller (MPC), enabling robust trajectory and heading regulation under varying wind conditions. The proposed approach leverages a two-degree-of-freedom (2-DoF) moving-mass mechanism to generate both inertial and aerodynamic moments for attitude and heading control, thereby enhancing flight stability in disturbance-prone environments. Extensive flight experiments under headwind and crosswind conditions show that the integrated MHE-MPC framework significantly outperforms baseline PID control, demonstrating its effectiveness for disturbance-aware LTA flight.

</details>


### [7] [HPTune: Hierarchical Proactive Tuning for Collision-Free Model Predictive Control](https://arxiv.org/abs/2601.21346)
*Wei Zuo,Chengyang Li,Yikun Wang,Bingyang Cheng,Zeyi Ren,Shuai Wang,Derrick Wing Kwan Ng,Yik-Chung Wu*

Main category: cs.RO

TL;DR: 提出HPTune分层主动调参框架，通过评估已执行和未执行动作来提升MPC运动规划器的参数调优效率，结合快速级和慢速级调参，并整合多普勒激光雷达增强运动预测。


<details>
  <summary>Details</summary>
Motivation: 现有MPC运动规划器参数调优方法通常只评估已执行动作，导致参数更新效率低下，因为失败事件（如障碍物接近或碰撞）稀疏。需要扩展评估范围以提高调优效率。

Method: 提出分层主动调参（HPTune）框架：1）快速级调参采用预测接近速度和预测接近距离的风险指标；2）慢速级调参利用扩展评估损失进行闭环反向传播；3）整合多普勒激光雷达提供障碍物速度和位置信息以增强运动预测。

Result: 在高保真模拟器上的大量实验表明，HPTune实现了高效的MPC调参，在复杂环境中优于各种基线方案，能够通过制定安全、敏捷的避碰策略实现情境定制的运动规划。

Conclusion: HPTune通过扩展评估范围到未执行动作，结合分层调参和增强感知，显著提高了MPC运动规划器的参数调优效率和适应性，实现了更安全、更敏捷的碰撞避免策略。

Abstract: Parameter tuning is a powerful approach to enhance adaptability in model predictive control (MPC) motion planners. However, existing methods typically operate in a myopic fashion that only evaluates executed actions, leading to inefficient parameter updates due to the sparsity of failure events (e.g., obstacle nearness or collision). To cope with this issue, we propose to extend evaluation from executed to non-executed actions, yielding a hierarchical proactive tuning (HPTune) framework that combines both a fast-level tuning and a slow-level tuning. The fast one adopts risk indicators of predictive closing speed and predictive proximity distance, and the slow one leverages an extended evaluation loss for closed-loop backpropagation. Additionally, we integrate HPTune with the Doppler LiDAR that provides obstacle velocities apart from position-only measurements for enhanced motion predictions, thus facilitating the implementation of HPTune. Extensive experiments on high-fidelity simulator demonstrate that HPTune achieves efficient MPC tuning and outperforms various baseline schemes in complex environments. It is found that HPTune enables situation-tailored motion planning by formulating a safe, agile collision avoidance strategy.

</details>


### [8] [Towards Bridging the Gap between Large-Scale Pretraining and Efficient Finetuning for Humanoid Control](https://arxiv.org/abs/2601.21363)
*Weidong Huang,Zhehan Li,Hangxin Liu,Biao Hou,Yao Su,Jingwen Zhang*

Main category: cs.RO

TL;DR: 本文提出了一种结合大规模预训练和高效微调的人形机器人控制方法，使用SAC进行大规模预训练实现零样本部署，然后通过基于模型的方法在新环境中进行高效微调。


<details>
  <summary>Details</summary>
Motivation: 当前人形机器人控制中，基于策略的方法（如PPO）虽然能通过大规模并行仿真实现鲁棒训练和零样本部署，但样本效率低限制了在新环境中的安全适应。虽然离策略RL和基于模型的RL具有更好的样本效率，但大规模预训练与高效微调之间仍存在差距。

Method: 1. 使用离策略的Soft Actor-Critic（SAC）算法，配合大批量更新和高更新数据比（UTD），进行大规模预训练人形机器人运动策略，实现零样本部署到真实机器人。2. 对于适应阶段，使用基于模型的方法对SAC预训练策略在新环境和分布外任务中进行微调。3. 在新环境中执行确定性策略收集数据，而随机探索则限制在物理信息世界模型中，降低适应过程中的随机探索风险。

Result: SAC预训练策略能够可靠地支持大规模预训练，实现零样本部署到真实机器人。这些预训练策略可以通过基于模型的方法在新环境中高效微调。该方法结合了预训练阶段的时钟效率和微调阶段的样本效率。

Conclusion: 该方法成功地将大规模仿真预训练的时钟效率与基于模型学习的样本效率相结合，为人形机器人控制提供了一种有效的预训练-微调框架，既能实现零样本部署，又能安全高效地适应新环境。

Abstract: Reinforcement learning (RL) is widely used for humanoid control, with on-policy methods such as Proximal Policy Optimization (PPO) enabling robust training via large-scale parallel simulation and, in some cases, zero-shot deployment to real robots. However, the low sample efficiency of on-policy algorithms limits safe adaptation to new environments. Although off-policy RL and model-based RL have shown improved sample efficiency, the gap between large-scale pretraining and efficient finetuning on humanoids still exists. In this paper, we find that off-policy Soft Actor-Critic (SAC), with large-batch update and a high Update-To-Data (UTD) ratio, reliably supports large-scale pretraining of humanoid locomotion policies, achieving zero-shot deployment on real robots. For adaptation, we demonstrate that these SAC-pretrained policies can be finetuned in new environments and out-of-distribution tasks using model-based methods. Data collection in the new environment executes a deterministic policy while stochastic exploration is instead confined to a physics-informed world model. This separation mitigates the risks of random exploration during adaptation while preserving exploratory coverage for improvement. Overall, the approach couples the wall-clock efficiency of large-scale simulation during pretraining with the sample efficiency of model-based learning during fine-tuning.

</details>


### [9] [Towards Space-Based Environmentally-Adaptive Grasping](https://arxiv.org/abs/2601.21394)
*Leonidas Askianakis,Aleksandr Artemov*

Main category: cs.RO

TL;DR: 该研究提出了一种在学习的潜在流形中直接学习控制策略的方法，用于空间环境中的抓取任务，通过融合多模态信息实现更高效的强化学习，在100万步内达到95%以上的任务成功率。


<details>
  <summary>Details</summary>
Motivation: 当前最先进的机器人操作系统在非结构化环境中面临高维动作空间、稀疏奖励以及难以泛化到精心策划的训练场景之外的问题，特别是在空间环境中的抓取任务中表现尤为明显。

Method: 研究采用GPU加速的物理仿真，构建单次操作任务集，在学习的潜在流形中直接学习控制策略。该潜在流形融合了多种模态信息，形成结构化表示用于策略决策。使用基于Soft Actor-Critic（SAC）的强化学习方法进行训练。

Result: 在连续变化的抓取条件下，在不到100万环境步数内实现了超过95%的任务成功率。与代表性最先进的视觉基线相比，在相同的开环单次条件下显示出更快的收敛速度。潜在空间推理方法相比标准基线具有更高的样本效率和更强的鲁棒性。

Conclusion: 在潜在空间中显式推理能够实现更高效的样本学习和更强的鲁棒性，能够应对新颖物体和夹爪几何形状、环境杂乱以及传感器配置的变化。研究指出了在极端空间条件下实现完全自适应和可泛化抓取的未来方向。

Abstract: Robotic manipulation in unstructured environments requires reliable execution under diverse conditions, yet many state-of-the-art systems still struggle with high-dimensional action spaces, sparse rewards, and slow generalization beyond carefully curated training scenarios. We study these limitations through the example of grasping in space environments. We learn control policies directly in a learned latent manifold that fuses (grammarizes) multiple modalities into a structured representation for policy decision-making. Building on GPU-accelerated physics simulation, we instantiate a set of single-shot manipulation tasks and achieve over 95% task success with Soft Actor-Critic (SAC)-based reinforcement learning in less than 1M environment steps, under continuously varying grasping conditions from step 1. This empirically shows faster convergence than representative state-of-the-art visual baselines under the same open-loop single-shot conditions. Our analysis indicates that explicitly reasoning in latent space yields more sample-efficient learning and improved robustness to novel object and gripper geometries, environmental clutter, and sensor configurations compared to standard baselines. We identify remaining limitations and outline directions toward fully adaptive and generalizable grasping in the extreme conditions of space.

</details>


### [10] [Singularity-Free Lie Group Integration and Geometrically Consistent Evaluation of Multibody System Models Described in Terms of Standard Absolute Coordinates](https://arxiv.org/abs/2601.21413)
*Andreas Mueller*

Main category: cs.RO

TL;DR: 本文提出了一种将李群积分器与标准多体系统方程对接的框架，以及一种在绝对坐标中保持刚体运动几何一致性的方法，通过局部-全局转换映射实现李群坐标与绝对坐标的更新。


<details>
  <summary>Details</summary>
Motivation: 传统多体系统建模使用绝对坐标，但时间积分面临无奇异性参数化问题。李群积分方法虽然能避免奇异性并保持运动几何，但与标准方程不兼容，难以在现有仿真代码中实现。

Method: 提出两个主要方法：1) 李群积分器与标准方程对接框架，允许在绝对坐标描述多体系统的同时使用李群积分方案；2) 在绝对坐标中一致地融入刚体运动几何的方法，使用局部-全局转换映射在SO(3)×R³和SE(3)李群上更新坐标。

Result: 建立了李群积分器与标准多体系统方程之间的接口框架，使得现有仿真代码能够利用李群积分的优势，同时保持了刚体运动几何在积分过程中的一致性。

Conclusion: 该研究成功解决了李群积分方法与标准多体系统方程的不兼容问题，通过局部-全局转换映射实现了两种方法的有效结合，为多体系统仿真提供了更优的数值积分方案。

Abstract: A classical approach to the multibody systems (MBS) modeling is to use absolute coordinates, i.e., a set of (possibly redundant) coordinates that describe the absolute position and orientation of the individual bodies with respect to an inertial frame (IFR). A well-known problem for the time integration of the equations of motion (EOM) is the lack of a singularity-free parameterization of spatial motions, which is usually tackled by using unit quaternions. Lie group integration methods were proposed as an alternative approach to the singularity-free time integration. At the same time, Lie group formulations of EOM naturally respect the geometry of spatial motions during integration. Lie group integration methods, operating directly on the configuration space Lie group, are incompatible with standard formulations of the EOM, and cannot be implemented in existing MBS simulation codes without a major restructuring. The contribution of this paper is twofold: (1) A framework for interfacing Lie group integrators to standard EOM formulations is presented. It allows describing MBS in terms of various absolute coordinates and at the same using Lie group integration schemes. (2) A method for consistently incorporating the geometry of rigid body motions into the evaluation of EOM in absolute coordinates integrated with standard vector space integration schemes. The direct product group and the semidirect product group SO(3)xR3 and the semidirect product group SE(3) are used for representing rigid body motions. The key element is the local-global transitions (LGT) transition map, which facilitates the update of (global) absolute coordinates in terms of the (local) coordinates on the Lie group. This LGT map is specific to the absolute coordinates, the local coordinates on the Lie group, and the Lie group used to represent rigid body configurations.

</details>


### [11] [Spotlighting Task-Relevant Features: Object-Centric Representations for Better Generalization in Robotic Manipulation](https://arxiv.org/abs/2601.21416)
*Alexandre Chapin,Bruno Machado,Emmanuel Dellandréa,Liming Chen*

Main category: cs.RO

TL;DR: SBOCR（基于槽的对象中心表示）在机器人操作任务中比全局和密集表示具有更好的泛化能力，特别是在光照、纹理变化和干扰物存在的情况下。


<details>
  <summary>Details</summary>
Motivation: 现有的机器人操作策略通常使用预训练编码器提取的视觉表示，包括全局特征（整个图像的汇总向量）和密集特征（保留补丁级嵌入）。这两种特征都混合了任务相关和无关信息，导致在分布偏移（如光照、纹理变化或干扰物存在）下泛化能力差。

Method: 探索了一种中间结构化替代方案：基于槽的对象中心表示（SBOCR），将密集特征分组为有限的对象类实体。这种表示能自然减少提供给机器人操作策略的噪声，同时保留足够信息来高效执行任务。在模拟和真实世界的一系列简单到复杂的操作任务中，对全局和密集表示与中间槽基表示进行了基准测试。

Result: SBOCR基策略在泛化设置中优于密集和全局表示基策略，即使没有任务特定的预训练。在光照、纹理变化和干扰物存在等多样化视觉条件下，SBOCR表现出更好的泛化能力。

Conclusion: SBOCR是设计在动态、真实世界机器人环境中有效泛化的视觉系统的一个有前景方向。这种对象中心表示能更好地分离任务相关信息，提高机器人操作策略的鲁棒性和适应性。

Abstract: The generalization capabilities of robotic manipulation policies are heavily influenced by the choice of visual representations. Existing approaches typically rely on representations extracted from pre-trained encoders, using two dominant types of features: global features, which summarize an entire image via a single pooled vector, and dense features, which preserve a patch-wise embedding from the final encoder layer. While widely used, both feature types mix task-relevant and irrelevant information, leading to poor generalization under distribution shifts, such as changes in lighting, textures, or the presence of distractors. In this work, we explore an intermediate structured alternative: Slot-Based Object-Centric Representations (SBOCR), which group dense features into a finite set of object-like entities. This representation permits to naturally reduce the noise provided to the robotic manipulation policy while keeping enough information to efficiently perform the task. We benchmark a range of global and dense representations against intermediate slot-based representations, across a suite of simulated and real-world manipulation tasks ranging from simple to complex. We evaluate their generalization under diverse visual conditions, including changes in lighting, texture, and the presence of distractors. Our findings reveal that SBOCR-based policies outperform dense and global representation-based policies in generalization settings, even without task-specific pretraining. These insights suggest that SBOCR is a promising direction for designing visual systems that generalize effectively in dynamic, real-world robotic environments.

</details>


### [12] [Nimbus: A Unified Embodied Synthetic Data Generation Framework](https://arxiv.org/abs/2601.21449)
*Zeyu He,Yuchang Zhang,Yuanzhen Zhou,Miao Tao,Hengjie Li,Yang Tian,Jia Zeng,Tai Wang,Wenzhe Cai,Yilun Chen,Ning Gao,Jiangmiao Pang*

Main category: cs.RO

TL;DR: Nimbus是一个统一的合成数据生成框架，通过模块化四层架构和异步执行模型，将异构导航和操作管道集成，实现大规模分布式环境下的高效数据生成。


<details>
  <summary>Details</summary>
Motivation: 现有合成数据生成管道分散且任务特定，导致工程效率低下和系统不稳定，无法支持基础模型训练所需的大规模、高吞吐量数据生成。

Method: 提出Nimbus框架，采用模块化四层架构，将轨迹规划、渲染和存储解耦为异步阶段，实现动态管道调度、全局负载均衡、分布式容错和特定后端渲染优化。

Result: Nimbus相比未优化基线实现了2-3倍的端到端吞吐量提升，确保在大型分布式环境中的稳健长期运行，作为InternData套件的生产骨干。

Conclusion: Nimbus框架解决了现有合成数据生成系统的碎片化问题，为具身智能的规模化数据生成提供了高效、统一的解决方案。

Abstract: Scaling data volume and diversity is critical for generalizing embodied intelligence. While synthetic data generation offers a scalable alternative to expensive physical data acquisition, existing pipelines remain fragmented and task-specific. This isolation leads to significant engineering inefficiency and system instability, failing to support the sustained, high-throughput data generation required for foundation model training. To address these challenges, we present Nimbus, a unified synthetic data generation framework designed to integrate heterogeneous navigation and manipulation pipelines. Nimbus introduces a modular four-layer architecture featuring a decoupled execution model that separates trajectory planning, rendering, and storage into asynchronous stages. By implementing dynamic pipeline scheduling, global load balancing, distributed fault tolerance, and backend-specific rendering optimizations, the system maximizes resource utilization across CPU, GPU, and I/O resources. Our evaluation demonstrates that Nimbus achieves a 2-3X improvement in end-to-end throughput compared to unoptimized baselines and ensuring robust, long-term operation in large-scale distributed environments. This framework serves as the production backbone for the InternData suite, enabling seamless cross-domain data synthesis.

</details>


### [13] [4D-CAAL: 4D Radar-Camera Calibration and Auto-Labeling for Autonomous Driving](https://arxiv.org/abs/2601.21454)
*Shanliang Yao,Zhuoxiao Li,Runwei Guan,Kebin Cao,Meng Xia,Fuping Hu,Sen Xu,Yong Yue,Xiaohui Zhu,Weiping Ding,Ryan Wen Liu*

Main category: cs.RO

TL;DR: 4D-CAAL是一个用于4D雷达-相机标定和自动标注的统一框架，通过双用途标定目标和几何投影优化，解决了多模态传感器标定和雷达数据标注的挑战。


<details>
  <summary>Details</summary>
Motivation: 4D雷达在自动驾驶中日益重要，但现有标定方法使用分离的目标，难以建立对应关系；同时手动标注稀疏雷达数据费时且不可靠，需要统一的解决方案。

Method: 提出双用途标定目标设计：正面棋盘格用于相机检测，背面中心角反射器用于雷达检测；开发鲁棒的对应匹配算法，将棋盘格中心与最强雷达反射点对齐；构建自动标注流程，通过几何投影和多特征优化将相机分割标注转移到雷达点云。

Result: 实验证明该方法实现了高精度的标定，同时显著减少了手动标注工作量，加速了自动驾驶多模态感知系统的开发。

Conclusion: 4D-CAAL框架有效解决了4D雷达-相机标定和雷达数据自动标注的双重挑战，为自动驾驶多模态感知系统提供了实用的解决方案。

Abstract: 4D radar has emerged as a critical sensor for autonomous driving, primarily due to its enhanced capabilities in elevation measurement and higher resolution compared to traditional 3D radar. Effective integration of 4D radar with cameras requires accurate extrinsic calibration, and the development of radar-based perception algorithms demands large-scale annotated datasets. However, existing calibration methods often employ separate targets optimized for either visual or radar modalities, complicating correspondence establishment. Furthermore, manually labeling sparse radar data is labor-intensive and unreliable. To address these challenges, we propose 4D-CAAL, a unified framework for 4D radar-camera calibration and auto-labeling. Our approach introduces a novel dual-purpose calibration target design, integrating a checkerboard pattern on the front surface for camera detection and a corner reflector at the center of the back surface for radar detection. We develop a robust correspondence matching algorithm that aligns the checkerboard center with the strongest radar reflection point, enabling accurate extrinsic calibration. Subsequently, we present an auto-labeling pipeline that leverages the calibrated sensor relationship to transfer annotations from camera-based segmentations to radar point clouds through geometric projection and multi-feature optimization. Extensive experiments demonstrate that our method achieves high calibration accuracy while significantly reducing manual annotation effort, thereby accelerating the development of robust multi-modal perception systems for autonomous driving.

</details>


### [14] [Don't double it: Efficient Agent Prediction in Occlusions](https://arxiv.org/abs/2601.21504)
*Anna Rothenhäusler,Markus Mazzola,Andreas Look,Raghu Rajan,Joschka Bödecker*

Main category: cs.RO

TL;DR: MatchInformer：基于Transformer的遮挡交通参与者预测方法，通过匈牙利匹配减少冗余预测，解耦航向与运动提高轨迹精度，使用MCC评估不平衡场景下的性能。


<details>
  <summary>Details</summary>
Motivation: 遮挡的交通参与者对自动驾驶构成重大挑战，现有学习方法虽然能推断隐藏参与者的存在，但常产生冗余的占用预测（单个参与者被多次识别），这增加了下游规划和计算负载。

Method: 1. 基于最先进的SceneInformer架构，提出MatchInformer方法；2. 将目标检测中的匈牙利匹配算法集成到训练过程中，强制预测与真实值之间的一一对应关系；3. 解耦参与者的航向与运动，提高轨迹预测的准确性和可解释性；4. 使用马修斯相关系数（MCC）评估占用预测，处理类别不平衡问题。

Result: 在Waymo Open Motion Dataset上的实验表明，该方法能更好地推理遮挡区域，并产生比现有方法更准确的轨迹预测。

Conclusion: MatchInformer通过集成匈牙利匹配减少冗余预测，解耦航向与运动提高轨迹精度，使用MCC评估不平衡场景，有效解决了遮挡交通参与者预测中的冗余和准确性挑战。

Abstract: Occluded traffic agents pose a significant challenge for autonomous vehicles, as hidden pedestrians or vehicles can appear unexpectedly, yet this problem remains understudied. Existing learning-based methods, while capable of inferring the presence of hidden agents, often produce redundant occupancy predictions where a single agent is identified multiple times. This issue complicates downstream planning and increases computational load. To address this, we introduce MatchInformer, a novel transformer-based approach that builds on the state-of-the-art SceneInformer architecture. Our method improves upon prior work by integrating Hungarian Matching, a state-of-the-art object matching algorithm from object detection, into the training process to enforce a one-to-one correspondence between predictions and ground truth, thereby reducing redundancy. We further refine trajectory forecasts by decoupling an agent's heading from its motion, a strategy that improves the accuracy and interpretability of predicted paths. To better handle class imbalances, we propose using the Matthews Correlation Coefficient (MCC) to evaluate occupancy predictions. By considering all entries in the confusion matrix, MCC provides a robust measure even in sparse or imbalanced scenarios. Experiments on the Waymo Open Motion Dataset demonstrate that our approach improves reasoning about occluded regions and produces more accurate trajectory forecasts than prior methods.

</details>


### [15] [IROS: A Dual-Process Architecture for Real-Time VLM-Based Indoor Navigation](https://arxiv.org/abs/2601.21506)
*Joonhee Lee,Hyunseung Shin,Jeonggil Ko*

Main category: cs.RO

TL;DR: IROS是一个实时室内导航框架，结合了VLM级别的上下文推理能力和轻量级感知模块的效率，在低成本设备硬件上实现快速响应和语义理解


<details>
  <summary>Details</summary>
Motivation: 现有室内移动机器人导航方法无法同时满足快速响应和鲁棒语义理解的需求。传统几何方法依赖详细地图且无法解释人类导向的语义线索；VLA模型仅基于可见帧做出反应性决策；VLMs计算延迟高，不适合嵌入式平台实时操作

Method: IROS采用双过程理论，将快速反射决策（系统一）与慢速审慎推理（系统二）分离，仅在必要时调用VLM。通过为紧凑型VLMs增强空间和文本线索，实现高效的人性化导航

Result: 在五个真实世界建筑中，IROS相比连续VLM导航提高了决策准确性，并将延迟降低了66%

Conclusion: IROS框架成功地将VLM级别的上下文推理能力与轻量级感知模块的效率相结合，在低成本设备硬件上实现了实时、鲁棒的室内导航，解决了现有方法在响应速度和语义理解之间的权衡问题

Abstract: Indoor mobile robot navigation requires fast responsiveness and robust semantic understanding, yet existing methods struggle to provide both. Classical geometric approaches such as SLAM offer reliable localization but depend on detailed maps and cannot interpret human-targeted cues (e.g., signs, room numbers) essential for indoor reasoning. Vision-Language-Action (VLA) models introduce semantic grounding but remain strictly reactive, basing decisions only on visible frames and failing to anticipate unseen intersections or reason about distant textual cues. Vision-Language Models (VLMs) provide richer contextual inference but suffer from high computational latency, making them unsuitable for real-time operation on embedded platforms. In this work, we present IROS, a real-time navigation framework that combines VLM-level contextual reasoning with the efficiency of lightweight perceptual modules on low-cost, on-device hardware. Inspired by Dual Process Theory, IROS separates fast reflexive decisions (System One) from slow deliberative reasoning (System Two), invoking the VLM only when necessary. Furthermore, by augmenting compact VLMs with spatial and textual cues, IROS delivers robust, human-like navigation with minimal latency. Across five real-world buildings, IROS improves decision accuracy and reduces latency by 66% compared to continuous VLM-based navigation.

</details>


### [16] [AIR-VLA: Vision-Language-Action Systems for Aerial Manipulation](https://arxiv.org/abs/2601.21602)
*Jianli Sun,Bin Tian,Qiyao Zhang,Chengxiang Li,Zihan Song,Zhiyong Cui,Yisheng Lv,Yonglin Tian*

Main category: cs.RO

TL;DR: AIR-VLA是首个专为空中操作设计的视觉-语言-动作基准，通过模拟环境和3000个手动遥操作演示数据集，评估主流VLA模型在无人机移动、机械臂控制和高层规划方面的能力。


<details>
  <summary>Details</summary>
Motivation: 虽然视觉-语言-动作模型在地面实体智能中取得了显著成功，但在空中操作系统中的应用仍是一个未充分探索的领域。空中操作系统的浮动基座动力学、无人机与机械臂的强耦合以及多步长时程任务特性，对现有为静态或2D移动基座设计的VLA范式构成了严峻挑战。

Method: 构建了基于物理的模拟环境，发布了包含3000个手动遥操作演示的高质量多模态数据集，涵盖基座操作、物体与空间理解、语义推理和长时程规划。利用该平台系统评估主流VLA模型和最先进的VLM模型。

Result: 实验验证了将VLA范式迁移到空中系统的可行性，并通过针对空中任务的多维度指标，揭示了当前模型在无人机移动性、机械臂控制和高层规划方面的能力和边界。

Conclusion: AIR-VLA为通用空中机器人研究的未来建立了标准化测试平台和数据基础，填补了视觉-语言-动作模型在空中操作领域的空白。

Abstract: While Vision-Language-Action (VLA) models have achieved remarkable success in ground-based embodied intelligence, their application to Aerial Manipulation Systems (AMS) remains a largely unexplored frontier. The inherent characteristics of AMS, including floating-base dynamics, strong coupling between the UAV and the manipulator, and the multi-step, long-horizon nature of operational tasks, pose severe challenges to existing VLA paradigms designed for static or 2D mobile bases. To bridge this gap, we propose AIR-VLA, the first VLA benchmark specifically tailored for aerial manipulation. We construct a physics-based simulation environment and release a high-quality multimodal dataset comprising 3000 manually teleoperated demonstrations, covering base manipulation, object & spatial understanding, semantic reasoning, and long-horizon planning. Leveraging this platform, we systematically evaluate mainstream VLA models and state-of-the-art VLM models. Our experiments not only validate the feasibility of transferring VLA paradigms to aerial systems but also, through multi-dimensional metrics tailored to aerial tasks, reveal the capabilities and boundaries of current models regarding UAV mobility, manipulator control, and high-level planning. AIR-VLA establishes a standardized testbed and data foundation for future research in general-purpose aerial robotics. The resource of AIR-VLA will be available at https://anonymous.4open.science/r/AIR-VLA-dataset-B5CC/.

</details>


### [17] [From Instruction to Event: Sound-Triggered Mobile Manipulation](https://arxiv.org/abs/2601.21667)
*Hao Ju,Shaofei Huang,Hongyu Li,Zihan Ding,Si Liu,Meng Wang,Zhedong Zheng*

Main category: cs.RO

TL;DR: 本文提出声音触发的移动操作任务，让智能体在没有明确指令的情况下主动感知和交互发声物体，突破了传统指令驱动范式的局限性。


<details>
  <summary>Details</summary>
Motivation: 当前移动操作研究主要遵循指令驱动范式，智能体依赖预定义文本命令执行任务。这种设置将智能体限制在被动角色，限制了其自主性和对动态环境事件的反应能力。

Method: 开发了Habitat-Echo数据平台，整合声学渲染与物理交互；提出了包含高级任务规划器和低级策略模型的基线系统，使智能体能够主动检测和响应听觉事件。

Result: 实验表明，所提基线使智能体能够主动检测和响应听觉事件，无需逐案指令。在具有挑战性的双声源场景中，智能体成功从重叠的声学干扰中分离出主要声源执行首次交互，随后操作次要物体，验证了基线的鲁棒性。

Conclusion: 声音触发的移动操作突破了传统指令驱动范式的限制，使智能体能够更自主地感知和交互动态环境中的发声物体，为移动操作研究开辟了新方向。

Abstract: Current mobile manipulation research predominantly follows an instruction-driven paradigm, where agents rely on predefined textual commands to execute tasks. However, this setting confines agents to a passive role, limiting their autonomy and ability to react to dynamic environmental events. To address these limitations, we introduce sound-triggered mobile manipulation, where agents must actively perceive and interact with sound-emitting objects without explicit action instructions. To support these tasks, we develop Habitat-Echo, a data platform that integrates acoustic rendering with physical interaction. We further propose a baseline comprising a high-level task planner and low-level policy models to complete these tasks. Extensive experiments show that the proposed baseline empowers agents to actively detect and respond to auditory events, eliminating the need for case-by-case instructions. Notably, in the challenging dual-source scenario, the agent successfully isolates the primary source from overlapping acoustic interference to execute the first interaction, and subsequently proceeds to manipulate the secondary object, verifying the robustness of the baseline.

</details>


### [18] [CoFreeVLA: Collision-Free Dual-Arm Manipulation via Vision-Language-Action Model and Risk Estimation](https://arxiv.org/abs/2601.21712)
*Xuanran Zhai,Binkai Ou,Yemin Wang,Hui Yi Leong,Qiaojun Yu,Ce Hao,Yaohua Liu*

Main category: cs.RO

TL;DR: CoFreeVLA在VLA模型中加入了短时域自碰撞风险估计器，通过预测碰撞概率来筛选危险指令、调整恢复动作，并在双臂机器人任务中显著减少自碰撞、提高成功率。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言动作模型在双臂机器人部署中存在安全隐患，因为模型未能充分建模双臂之间以及手臂与抓取物体之间的自碰撞风险。

Method: 在端到端VLA模型基础上增加短时域自碰撞风险估计器，该估计器从本体感知、视觉嵌入和规划动作中预测碰撞概率，用于：1）筛选危险指令；2）通过风险引导调整恢复安全状态；3）优化策略生成更安全的动作序列。采用基于模型的碰撞标签进行预训练，并在真实机器人数据上进行后训练校准。

Result: 在PiPER机器人臂的五个双臂任务中，CoFreeVLA相比RDT和APEX方法显著减少了自碰撞，并提高了任务成功率。

Conclusion: CoFreeVLA通过集成自碰撞风险估计器，有效提升了VLA模型在双臂机器人操作中的安全性，为实际部署提供了更可靠的解决方案。

Abstract: Vision Language Action (VLA) models enable instruction following manipulation, yet dualarm deployment remains unsafe due to under modeled selfcollisions between arms and grasped objects. We introduce CoFreeVLA, which augments an endtoend VLA with a short horizon selfcollision risk estimator that predicts collision likelihood from proprioception, visual embeddings, and planned actions. The estimator gates risky commands, recovers to safe states via risk-guided adjustments, and shapes policy refinement for safer rollouts. It is pre-trained with model-based collision labels and posttrained on real robot rollouts for calibration. On five bimanual tasks with the PiPER robot arm, CoFreeVLA reduces selfcollisions and improves success rates versus RDT and APEX.

</details>


### [19] [Disentangling perception and reasoning for improving data efficiency in learning cloth manipulation without demonstrations](https://arxiv.org/abs/2601.21713)
*Donatien Delehelle,Fei Chen,Darwin Caldwell*

Main category: cs.RO

TL;DR: 本文质疑了布料操作中常见的端到端学习方法，提出了一种更高效、模块化的强化学习方法，显著减少了模型大小和训练时间，并成功实现了从仿真到真实世界的迁移。


<details>
  <summary>Details</summary>
Motivation: 布料操作是机器人学中的开放挑战，传统端到端学习方法虽然能实现仿真到现实的迁移，但使用高维图像输入导致模型庞大、训练时间长、计算成本高。本文质疑这种设计选择，探索更高效的模块化方法。

Method: 通过精心设计选择，在仿真环境中学习时显著减少模型大小和训练时间。采用模块化方法替代端到端学习，并展示了如何将仿真训练的模型迁移到真实世界。

Result: 在SoftGym基准测试中，相比现有基线方法，本文方法在任务性能上取得显著提升，同时使用了明显更小的模型。

Conclusion: 通过质疑布料操作中常见的端到端学习设计选择，本文证明了通过精心设计的模块化方法可以显著提高强化学习在布料操作中的效率，减少计算成本，同时保持从仿真到现实的可迁移性。

Abstract: Cloth manipulation is a ubiquitous task in everyday life, but it remains an open challenge for robotics. The difficulties in developing cloth manipulation policies are attributed to the high-dimensional state space, complex dynamics, and high propensity to self-occlusion exhibited by fabrics. As analytical methods have not been able to provide robust and general manipulation policies, reinforcement learning (RL) is considered a promising approach to these problems. However, to address the large state space and complex dynamics, data-based methods usually rely on large models and long training times. The resulting computational cost significantly hampers the development and adoption of these methods. Additionally, due to the challenge of robust state estimation, garment manipulation policies often adopt an end-to-end learning approach with workspace images as input. While this approach enables a conceptually straightforward sim-to-real transfer via real-world fine-tuning, it also incurs a significant computational cost by training agents on a highly lossy representation of the environment state. This paper questions this common design choice by exploring an efficient and modular approach to RL for cloth manipulation. We show that, through careful design choices, model size and training time can be significantly reduced when learning in simulation. Furthermore, we demonstrate how the resulting simulation-trained model can be transferred to the real world. We evaluate our approach on the SoftGym benchmark and achieve significant performance improvements over available baselines on our task, while using a substantially smaller model.

</details>


### [20] [Flocking behavior for dynamic and complex swarm structures](https://arxiv.org/abs/2601.21772)
*Carmen D. R. Pita-Romero,Pedro Arias-Perez,Miguel Fernandez-Cortizas,Rafael Perez-Segui,Pascual Campoy*

Main category: cs.RO

TL;DR: 提出基于虚拟质心的无人机集群编队算法，简化复杂结构构建与轨迹跟踪


<details>
  <summary>Details</summary>
Motivation: 多无人机保持复杂编队结构并实现复杂轨迹跟踪是一个重大挑战，需要更简单有效的解决方案

Method: 基于虚拟质心概念的集群行为算法，扩展经典虚拟行为方法，提供动态控制无人机数量和编队结构的理论框架

Result: 通过仿真测试和真实实验验证，算法即使在复杂编队和复杂轨迹下也表现出简单有效的特性

Conclusion: 基于虚拟质心的算法为无人机集群编队控制提供了简化而有效的解决方案，能够处理复杂结构和轨迹

Abstract: Maintaining the formation of complex structures with multiple UAVs and achieving complex trajectories remains a major challenge. This work presents an algorithm for implementing the flocking behavior of UAVs based on the concept of Virtual Centroid to easily develop a structure for the flock. The approach builds on the classical virtual-based behavior, providing a theoretical framework for incorporating enhancements to dynamically control both the number of agents and the formation of the structure. Simulation tests and real-world experiments were conducted, demonstrating its simplicity even with complex formations and complex trajectories.

</details>


### [21] [GAZELOAD A Multimodal Eye-Tracking Dataset for Mental Workload in Industrial Human-Robot Collaboration](https://arxiv.org/abs/2601.21829)
*Bsher Karbouj,Baha Eddin Gaaloul,Jorg Kruger*

Main category: cs.RO

TL;DR: GAZELOAD是一个用于工业人机协作中脑力负荷估计的多模态数据集，包含26名参与者在与协作机器人交互时的眼动追踪、环境测量和任务上下文数据。


<details>
  <summary>Details</summary>
Motivation: 在工业人机协作场景中，准确估计操作员的脑力负荷对于优化人机交互、提高工作效率和安全性至关重要。现有数据集往往缺乏多模态同步数据，特别是在真实工业环境下的眼动与环境因素结合的数据。

Method: 在实验室装配测试平台上，26名参与者佩戴Meta ARIA智能眼镜与两个协作机器人（UR5和Franka Emika Panda）交互。数据集时间同步了多种信号：眼动追踪信号（瞳孔直径、注视、扫视、眼动轨迹、注视转移熵、注视分散指数）、环境实时连续测量（照度）、任务和机器人上下文（工作台、任务块、诱导故障），并在任务难度和环境条件的受控操作下收集数据。

Result: 为每位参与者和按脑力负荷分级的任务块提供了CSV文件，包含250毫秒窗口聚合的眼动指标、环境日志以及1-10李克特量表的自报告脑力负荷评分。数据按参与者特定文件夹组织，并附带文档说明。

Conclusion: GAZELOAD数据集可用于开发和基准测试脑力负荷估计算法、特征提取和时间建模，特别适用于真实的工业人机协作场景。该数据集还能用于研究环境因素（如照明）对基于眼动的脑力负荷标记物的影响。

Abstract: This article describes GAZELOAD, a multimodal dataset for mental workload estimation in industrial human-robot collaboration. The data were collected in a laboratory assembly testbed where 26 participants interacted with two collaborative robots (UR5 and Franka Emika Panda) while wearing Meta ARIA smart glasses. The dataset time-synchronizes eye-tracking signals (pupil diameter, fixations, saccades, eye gaze, gaze transition entropy, fixation dispersion index) with environmental real-time and continuous measurements (illuminance) and task and robot context (bench, task block, induced faults), under controlled manipulations of task difficulty and ambient conditions. For each participant and workload-graded task block, we provide CSV files with ocular metrics aggregated into 250 ms windows, environmental logs, and self-reported mental workload ratings on a 1-10 Likert scale, organized in participant-specific folders alongside documentation. These data can be used to develop and benchmark algorithms for mental workload estimation, feature extraction, and temporal modeling in realistic industrial HRC scenarios, and to investigate the influence of environmental factors such as lighting on eye-based workload markers.

</details>


### [22] [LLM-Driven Scenario-Aware Planning for Autonomous Driving](https://arxiv.org/abs/2601.21876)
*He Li,Zhaowei Chen,Rui Gao,Guoliang Li,Qi Hao,Shuai Wang,Chengzhong Xu*

Main category: cs.RO

TL;DR: LAP：基于大语言模型的自动驾驶混合规划器切换框架，通过LLM场景理解和联合优化，在复杂交通中实现高速驾驶与精确驾驶的智能切换


<details>
  <summary>Details</summary>
Motivation: 现有混合规划器切换框架在密集交通中难以实现可靠模式切换和持续高效驾驶，主要受限于启发式场景识别和低频控制更新

Method: 利用大语言模型进行场景理解，将其推理集成到模式配置和运动规划的联合优化中，采用树搜索模型预测控制和交替最小化求解

Result: 高保真仿真结果显示，LAP在驾驶时间和成功率方面均优于其他基准方法

Conclusion: LAP通过LLM驱动的自适应规划，成功解决了自动驾驶在密集交通中高速驾驶效率与安全操控的平衡问题

Abstract: Hybrid planner switching framework (HPSF) for autonomous driving needs to reconcile high-speed driving efficiency with safe maneuvering in dense traffic. Existing HPSF methods often fail to make reliable mode transitions or sustain efficient driving in congested environments, owing to heuristic scene recognition and low-frequency control updates. To address the limitation, this paper proposes LAP, a large language model (LLM) driven, adaptive planning method, which switches between high-speed driving in low-complexity scenes and precise driving in high-complexity scenes, enabling high qualities of trajectory generation through confined gaps. This is achieved by leveraging LLM for scene understanding and integrating its inference into the joint optimization of mode configuration and motion planning. The joint optimization is solved using tree-search model predictive control and alternating minimization. We implement LAP by Python in Robot Operating System (ROS). High-fidelity simulation results show that the proposed LAP outperforms other benchmarks in terms of both driving time and success rate.

</details>


### [23] [Multi-Modular MANTA-RAY: A Modular Soft Surface Platform for Distributed Multi-Object Manipulation](https://arxiv.org/abs/2601.21884)
*Pratik Ingle,Jørn Lambertsen,Kasper Støy,Andres Faina*

Main category: cs.RO

TL;DR: 多模块MANTA-RAY平台通过分布式模块化设计和几何变换PID控制器，在降低致动器密度的同时保持操作性能，实现了脆弱异质物体的可扩展并行操作。


<details>
  <summary>Details</summary>
Motivation: 传统密集致动器阵列虽然能产生复杂形变，但高自由度增加了系统复杂性和可扩展性限制。现有研究主要关注单模块四致动器配置，多模块配置的可行性和优势尚未探索。

Method: 提出分布式模块化可扩展的MANTA-RAY平台，采用模块间物体传递策略和几何变换驱动的PID控制器，直接将倾斜角度控制输出映射到致动器命令，无需大量数据驱动或黑盒训练。

Result: 在3x3和4x4不同模块配置的仿真中验证性能，并通过2x2硬件原型实验验证可行性。系统成功操作了鸡蛋、苹果等具有不同几何形状、质量和纹理的脆弱物体，并实现了并行操作。

Conclusion: 多模块MANTA-RAY平台提高了可扩展性，实现了跨更大区域的多个物体协调操作，展示了在实际应用中的潜力。

Abstract: Manipulation surfaces control objects by actively deforming their shape rather than directly grasping them. While dense actuator arrays can generate complex deformations, they also introduce high degrees of freedom (DOF), increasing system complexity and limiting scalability. The MANTA-RAY (Manipulation with Adaptive Non-rigid Textile Actuation with Reduced Actuation densitY) platform addresses these challenges by leveraging a soft, fabric-based surface with reduced actuator density to manipulate fragile and heterogeneous objects. Previous studies focused on single-module implementations supported by four actuators, whereas the feasibility and benefits of a scalable, multi-module configuration remain unexplored. In this work, we present a distributed, modular, and scalable variant of the MANTA-RAY platform that maintains manipulation performance with a reduced actuator density. The proposed multi-module MANTA-RAY platform and control strategy employs object passing between modules and a geometric transformation driven PID controller that directly maps tilt-angle control outputs to actuator commands, eliminating the need for extensive data-driven or black-box training. We evaluate system performance in simulation across surface configurations of varying modules (3x3 and 4x4) and validate its feasibility through experiments on a physical 2x2 hardware prototype. The system successfully manipulates objects with diverse geometries, masses, and textures including fragile items such as eggs and apples as well as enabling parallel manipulation. The results demonstrate that the multi-module MANTA-RAY improves scalability and enables coordinated manipulation of multiple objects across larger areas, highlighting its potential for practical, real-world applications.

</details>


### [24] [MoE-ACT: Improving Surgical Imitation Learning Policies through Supervised Mixture-of-Experts](https://arxiv.org/abs/2601.21971)
*Lorenzo Mazza,Ariel Rodriguez,Rayan Younis,Martin Lelis,Ortrun Hellig,Chenpan Li,Sebastian Bodenstedt,Martin Wagner,Stefanie Speidel*

Main category: cs.RO

TL;DR: 提出了一种用于结构化手术操作任务的监督混合专家架构，能在仅使用立体内窥镜图像和不到150个演示的情况下，学习复杂的长时程操作，并在离体猪组织上实现零样本迁移。


<details>
  <summary>Details</summary>
Motivation: 手术机器人模仿学习面临数据稀缺、工作空间受限以及高安全性和可预测性要求的挑战。现有方法通常需要多摄像头设置或数千个演示，难以在实际手术环境中应用。

Method: 提出监督混合专家架构，可添加到任何自主策略之上。使用轻量级动作解码器策略（如ACT），仅通过立体内窥镜图像和少于150个演示学习复杂长时程操作。在肠道抓取和牵引任务中评估，机器人助手通过视觉线索解释外科医生意图，执行组织抓取和持续牵引。

Result: 通用VLA模型完全无法学习该任务。标准ACT在分布内条件下取得中等成功率，而采用监督MoE架构显著提升性能，在分布内获得更高成功率，并在分布外场景（新抓取位置、光照减弱、部分遮挡）表现出更强鲁棒性。能泛化到未见测试视角，并零样本迁移到离体猪组织。

Conclusion: 监督MoE架构为手术机器人模仿学习提供了有前景的途径，能在数据稀缺条件下实现复杂操作学习，并展示出良好的泛化能力和向体内手术部署的潜力。

Abstract: Imitation learning has achieved remarkable success in robotic manipulation, yet its application to surgical robotics remains challenging due to data scarcity, constrained workspaces, and the need for an exceptional level of safety and predictability. We present a supervised Mixture-of-Experts (MoE) architecture designed for phase-structured surgical manipulation tasks, which can be added on top of any autonomous policy. Unlike prior surgical robot learning approaches that rely on multi-camera setups or thousands of demonstrations, we show that a lightweight action decoder policy like Action Chunking Transformer (ACT) can learn complex, long-horizon manipulation from less than 150 demonstrations using solely stereo endoscopic images, when equipped with our architecture. We evaluate our approach on the collaborative surgical task of bowel grasping and retraction, where a robot assistant interprets visual cues from a human surgeon, executes targeted grasping on deformable tissue, and performs sustained retraction. We benchmark our method against state-of-the-art Vision-Language-Action (VLA) models and the standard ACT baseline. Our results show that generalist VLAs fail to acquire the task entirely, even under standard in-distribution conditions. Furthermore, while standard ACT achieves moderate success in-distribution, adopting a supervised MoE architecture significantly boosts its performance, yielding higher success rates in-distribution and demonstrating superior robustness in out-of-distribution scenarios, including novel grasp locations, reduced illumination, and partial occlusions. Notably, it generalizes to unseen testing viewpoints and also transfers zero-shot to ex vivo porcine tissue without additional training, offering a promising pathway toward in vivo deployment. To support this, we present qualitative preliminary results of policy roll-outs during in vivo porcine surgery.

</details>


### [25] [Macro-Scale Electrostatic Origami Motor](https://arxiv.org/abs/2601.21976)
*Alex S. Miller,Leo McElroy,Jeffrey H. Lang*

Main category: cs.RO

TL;DR: 本文开发了首个宏观尺度的可折叠折纸旋转电机，使用电晕放电产生扭矩，实现了2.5:1的展开比，最高转速1440 rpm，最大输出扭矩超过0.15 mN·m。


<details>
  <summary>Details</summary>
Motivation: 现有可折叠机器人要么在结构中嵌入线性执行器，要么附加非折叠旋转电机，且所有嵌入折叠结构的执行器都只能产生线性或折叠运动，无法实现连续旋转运动。在宏观尺度上尚未出现可折叠的连续旋转执行器。

Method: 开发了基于折纸结构的宏观尺度旋转电机，采用电晕放电产生扭矩。电机可以折叠成扁平状态，然后展开工作。通过原型设计和测试验证了其性能。

Result: 原型电机实现了2.5:1的展开比，在-29 kV驱动电压下达到最高转速1440 rpm，最大输出扭矩超过0.15 mN·m，主动部件扭矩密度为0.04 Nm/kg。

Conclusion: 成功开发了首个宏观尺度的可折叠折纸旋转电机，填补了可折叠连续旋转执行器的技术空白，为可折叠机器人提供了新的驱动解决方案。

Abstract: Foldable robots have been an active area of robotics research due to their high volume-to-mass ratio, easy packability, and shape adaptability. For locomotion, previously developed foldable robots have either embedded linear actuators in, or attached non-folding rotary motors to, their structure. Further, those actuators directly embedded in the structure of the folding medium all contributed to linear or folding motion, not to continuous rotary motion. On the macro-scale there has not yet been a folding continuous rotary actuator. This paper details the development and testing of the first macro-scale origami rotary motor that can be folded flat, and then unfurled to operate. Using corona discharge for torque production, the prototype motor achieved an expansion ratio of 2.5:1, reached a top speed of 1440 rpm when driven at -29 kV, and exhibited a maximum output torque over 0.15 mN m with an active component torque density of 0.04 Nm/kg.

</details>


### [26] [PocketDP3: Efficient Pocket-Scale 3D Visuomotor Policy](https://arxiv.org/abs/2601.22018)
*Jinhao Zhang,Zhexuan Zhou,Huizhe Li,Yichen Lai,Wenlong Xia,Haoming Song,Youmin Gong,Jie Me*

Main category: cs.RO

TL;DR: PocketDP3：一种轻量级3D扩散策略，用基于MLP-Mixer的Diffusion Mixer替代传统U-Net解码器，参数减少99%以上，支持两步推理，在多个基准测试中实现SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有3D视觉扩散策略存在架构不匹配问题：紧凑的点云编码器与庞大的解码器配对，导致解码器参数浪费严重。作者旨在设计更高效的架构以减少参数冗余。

Method: 提出PocketDP3，用轻量级Diffusion Mixer（基于MLP-Mixer块构建）替代传统的条件U-Net解码器，实现时间和通道维度的高效融合。无需额外一致性蒸馏技术即可支持两步推理。

Result: 在RoboTwin2.0、Adroit和MetaWorld三个仿真基准测试中，参数少于先前方法的1%，同时实现最先进性能并加速推理。真实世界实验验证了方法的实用性和可迁移性。

Conclusion: PocketDP3通过创新的轻量级架构解决了3D扩散策略中的参数浪费问题，在保持高性能的同时大幅减少模型规模，提高了实时部署的实用性。

Abstract: Recently, 3D vision-based diffusion policies have shown strong capability in learning complex robotic manipulation skills. However, a common architectural mismatch exists in these models: a tiny yet efficient point-cloud encoder is often paired with a massive decoder. Given a compact scene representation, we argue that this may lead to substantial parameter waste in the decoder. Motivated by this observation, we propose PocketDP3, a pocket-scale 3D diffusion policy that replaces the heavy conditional U-Net decoder used in prior methods with a lightweight Diffusion Mixer (DiM) built on MLP-Mixer blocks. This architecture enables efficient fusion across temporal and channel dimensions, significantly reducing model size. Notably, without any additional consistency distillation techniques, our method supports two-step inference without sacrificing performance, improving practicality for real-time deployment. Across three simulation benchmarks--RoboTwin2.0, Adroit, and MetaWorld--PocketDP3 achieves state-of-the-art performance with fewer than 1% of the parameters of prior methods, while also accelerating inference. Real-world experiments further demonstrate the practicality and transferability of our method in real-world settings. Code will be released.

</details>


### [27] [mjlab: A Lightweight Framework for GPU-Accelerated Robot Learning](https://arxiv.org/abs/2601.22074)
*Kevin Zakka,Qiayuan Liao,Brent Yi,Louis Le Lay,Koushil Sreenath,Pieter Abbeel*

Main category: cs.RO

TL;DR: mjlab是一个轻量级开源机器人学习框架，结合GPU加速仿真与可组合环境，安装简单，提供原生MuJoCo数据结构访问。


<details>
  <summary>Details</summary>
Motivation: 为机器人学习提供一个轻量级、易于使用的框架，减少设置复杂性，同时利用GPU加速提高仿真效率。

Method: 采用Isaac Lab引入的manager-based API，结合MuJoCo Warp进行GPU加速物理仿真，提供模块化构建块用于观测、奖励和事件。

Result: 开发出mjlab框架，单命令安装，依赖最小化，提供原生MuJoCo数据结构访问，包含速度跟踪、运动模仿和操作任务的参考实现。

Conclusion: mjlab是一个高效、易用的机器人学习框架，通过GPU加速和模块化设计降低了机器人学习的入门门槛。

Abstract: We present mjlab, a lightweight, open-source framework for robot learning that combines GPU-accelerated simulation with composable environments and minimal setup friction. mjlab adopts the manager-based API introduced by Isaac Lab, where users compose modular building blocks for observations, rewards, and events, and pairs it with MuJoCo Warp for GPU-accelerated physics. The result is a framework installable with a single command, requiring minimal dependencies, and providing direct access to native MuJoCo data structures. mjlab ships with reference implementations of velocity tracking, motion imitation, and manipulation tasks.

</details>


### [28] [ReactEMG Stroke: Healthy-to-Stroke Few-shot Adaptation for sEMG-Based Intent Detection](https://arxiv.org/abs/2601.22090)
*Runsheng Wang,Katelyn Lee,Xinyue Zhu,Lauren Winterbottom,Dawn M. Nilsen,Joel Stein,Matei Ciocarlie*

Main category: cs.RO

TL;DR: 该研究提出了一种从健康人群到中风患者的sEMG意图检测迁移学习框架，通过健康数据预训练模型，再用少量中风患者数据进行微调，显著提高了中风患者手部康复意图检测的准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 中风患者手部康复的表面肌电信号(sEMG)意图检测面临两大挑战：1)需要长时间、个体化的校准过程；2)对信号变异性敏感。现有方法在中风患者身上效果不佳，因为中风患者的肌肉活动模式与健康人群不同且存在个体差异。

Method: 提出健康到中风的适应管道：1)首先在大量健康人群sEMG数据上预训练意图检测模型；2)然后使用少量中风患者特定数据对模型进行微调。比较了三种适应策略：仅头部调优、参数高效的LoRA适配器和端到端微调。在新收集的3名慢性中风患者数据集上进行评估，测试集包含实际分布偏移（会话内漂移、姿势变化、臂带重新定位）。

Result: 健康预训练适应在所有条件下都优于零样本迁移和仅使用中风数据训练的方法：最佳适应方法将平均过渡准确率从0.42提高到0.61，原始准确率从0.69提高到0.78。结果表明，迁移可重复使用的健康领域EMG表示可以减少校准负担，同时提高实时中风后意图检测的鲁棒性。

Conclusion: 从健康人群到中风患者的迁移学习框架能够有效解决中风患者sEMG意图检测的校准负担和鲁棒性问题，为辅助手部康复提供了更实用、更可靠的控制信号解决方案。

Abstract: Surface electromyography (sEMG) is a promising control signal for assist-as-needed hand rehabilitation after stroke, but detecting intent from paretic muscles often requires lengthy, subject-specific calibration and remains brittle to variability. We propose a healthy-to-stroke adaptation pipeline that initializes an intent detector from a model pretrained on large-scale able-bodied sEMG, then fine-tunes it for each stroke participant using only a small amount of subject-specific data. Using a newly collected dataset from three individuals with chronic stroke, we compare adaptation strategies (head-only tuning, parameter-efficient LoRA adapters, and full end-to-end fine-tuning) and evaluate on held-out test sets that include realistic distribution shifts such as within-session drift, posture changes, and armband repositioning. Across conditions, healthy-pretrained adaptation consistently improves stroke intent detection relative to both zero-shot transfer and stroke-only training under the same data budget; the best adaptation methods improve average transition accuracy from 0.42 to 0.61 and raw accuracy from 0.69 to 0.78. These results suggest that transferring a reusable healthy-domain EMG representation can reduce calibration burden while improving robustness for real-time post-stroke intent detection.

</details>


### [29] [DynamicVLA: A Vision-Language-Action Model for Dynamic Object Manipulation](https://arxiv.org/abs/2601.22153)
*Haozhe Xie,Beichen Wen,Jiarui Zheng,Zhaoxi Chen,Fangzhou Hong,Haiwen Diao,Ziwei Liu*

Main category: cs.RO

TL;DR: DynamicVLA框架通过紧凑视觉编码、连续推理和潜在感知动作流技术，解决了VLA模型在动态物体操作中的挑战，显著提升了响应速度、感知和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 尽管视觉-语言-动作模型在静态操作中表现出良好的泛化能力，但在需要快速感知、时间预测和连续控制的动态场景中仍然面临挑战，这构成了动态物体操作领域的一个开放性问题。

Method: DynamicVLA框架包含三个关键技术：1) 使用卷积视觉编码器的紧凑0.4B VLA模型，实现空间高效、结构忠实的编码；2) 连续推理机制，允许重叠的推理和执行以降低延迟；3) 潜在感知动作流，通过强制时间对齐的动作执行来弥合感知-执行差距。此外还构建了DOM基准数据集。

Result: 广泛的评估显示在响应速度、感知能力和泛化性方面取得了显著改进，DynamicVLA成为跨体现形式的通用动态物体操作统一框架。

Conclusion: DynamicVLA通过整合时间推理和闭环适应，成功解决了VLA模型在动态操作中的局限性，为动态物体操作提供了有效的解决方案，并通过DOM基准填补了该领域的数据空白。

Abstract: Manipulating dynamic objects remains an open challenge for Vision-Language-Action (VLA) models, which, despite strong generalization in static manipulation, struggle in dynamic scenarios requiring rapid perception, temporal anticipation, and continuous control. We present DynamicVLA, a framework for dynamic object manipulation that integrates temporal reasoning and closed-loop adaptation through three key designs: 1) a compact 0.4B VLA using a convolutional vision encoder for spatially efficient, structurally faithful encoding, enabling fast multimodal inference; 2) Continuous Inference, enabling overlapping reasoning and execution for lower latency and timely adaptation to object motion; and 3) Latent-aware Action Streaming, which bridges the perception-execution gap by enforcing temporally aligned action execution. To fill the missing foundation of dynamic manipulation data, we introduce the Dynamic Object Manipulation (DOM) benchmark, built from scratch with an auto data collection pipeline that efficiently gathers 200K synthetic episodes across 2.8K scenes and 206 objects, and enables fast collection of 2K real-world episodes without teleoperation. Extensive evaluations demonstrate remarkable improvements in response speed, perception, and generalization, positioning DynamicVLA as a unified framework for general dynamic object manipulation across embodiments.

</details>
