<div id=toc></div>

# Table of Contents

- [cs.RO](#cs.RO) [Total: 17]


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [1] [E2HiL: Entropy-Guided Sample Selection for Efficient Real-World Human-in-the-Loop Reinforcement Learning](https://arxiv.org/abs/2601.19969)
*Haoyuan Deng,Yuanjiang Xue,Haoyang Du,Boyang Zhou,Zhenyu Wu,Ziwei Wang*

Main category: cs.RO

TL;DR: 提出了一种名为\method的样本高效人机交互强化学习框架，通过主动选择信息丰富的样本来减少人类干预需求，在真实世界操作任务中实现了更高的成功率和更低的人工成本。


<details>
  <summary>Details</summary>
Motivation: 现有的人机交互强化学习框架样本效率低，需要大量人工干预才能收敛，导致高昂的劳动力成本。需要开发更高效的框架来减少人类干预需求。

Method: 提出\method框架，通过稳定降低策略熵来改进探索与利用的平衡。首先构建不同样本对策略熵的影响函数，通过动作概率和软优势的协方差高效估计。然后选择影响函数值适中的样本，剪除导致熵急剧下降的捷径样本和影响可忽略的噪声样本。

Result: 在四个真实世界操作任务上的实验表明，\method相比最先进的人机交互强化学习方法，成功率提高了42.1%，同时减少了10.1%的人类干预，验证了其有效性。

Conclusion: \method框架通过主动选择信息丰富的样本，实现了样本高效的人机交互强化学习，显著减少了人类干预需求，在真实世界操作任务中表现出优越性能。

Abstract: Human-in-the-loop guidance has emerged as an effective approach for enabling faster convergence in online reinforcement learning (RL) of complex real-world manipulation tasks. However, existing human-in-the-loop RL (HiL-RL) frameworks often suffer from low sample efficiency, requiring substantial human interventions to achieve convergence and thereby leading to high labor costs. To address this, we propose a sample-efficient real-world human-in-the-loop RL framework named \method, which requires fewer human intervention by actively selecting informative samples. Specifically, stable reduction of policy entropy enables improved trade-off between exploration and exploitation with higher sample efficiency. We first build influence functions of different samples on the policy entropy, which is efficiently estimated by the covariance of action probabilities and soft advantages of policies. Then we select samples with moderate values of influence functions, where shortcut samples that induce sharp entropy drops and noisy samples with negligible effect are pruned. Extensive experiments on four real-world manipulation tasks demonstrate that \method achieves a 42.1\% higher success rate while requiring 10.1\% fewer human interventions compared to the state-of-the-art HiL-RL method, validating its effectiveness. The project page providing code, videos, and mathematical formulations can be found at https://e2hil.github.io/.

</details>


### [2] [Just in time Informed Trees: Manipulability-Aware Asymptotically Optimized Motion Planning](https://arxiv.org/abs/2601.19972)
*Kuanqi Cai,Liding Zhang,Xinwen Su,Kejia Chen,Chaoqun Wang,Sami Haddadin,Alois Knoll,Arash Ajoudani,Luis Figueredo*

Main category: cs.RO

TL;DR: JIT*算法通过即时模块和运动性能模块改进高维机器人路径规划，在复杂多障碍环境中比传统采样方法表现更好


<details>
  <summary>Details</summary>
Motivation: 传统采样方法在高维机器人路径规划中难以高效找到可行且最优的路径，特别是在机器人操作臂中存在运动奇点和自碰撞风险时

Method: 提出JIT*算法，包含两个核心模块：即时模块（含即时边和即时采样）动态优化边连接和瓶颈区域采样密度；运动性能模块通过动态切换平衡可操作性和轨迹成本

Result: JIT*在R4到R16维度上持续优于传统采样规划器，在单臂和双臂操作任务中表现出有效性

Conclusion: JIT*算法通过动态优化采样和运动控制，有效解决了高维机器人路径规划中的效率和安全性问题

Abstract: In high-dimensional robotic path planning, traditional sampling-based methods often struggle to efficiently identify both feasible and optimal paths in complex, multi-obstacle environments. This challenge is intensified in robotic manipulators, where the risk of kinematic singularities and self-collisions further complicates motion efficiency and safety. To address these issues, we introduce the Just-in-Time Informed Trees (JIT*) algorithm, an enhancement over Effort Informed Trees (EIT*), designed to improve path planning through two core modules: the Just-in-Time module and the Motion Performance module. The Just-in-Time module includes "Just-in-Time Edge," which dynamically refines edge connectivity, and "Just-in-Time Sample," which adjusts sampling density in bottleneck areas to enable faster initial path discovery. The Motion Performance module balances manipulability and trajectory cost through dynamic switching, optimizing motion control while reducing the risk of singularities. Comparative analysis shows that JIT* consistently outperforms traditional sampling-based planners across $\mathbb{R}^4$ to $\mathbb{R}^{16}$ dimensions. Its effectiveness is further demonstrated in single-arm and dual-arm manipulation tasks, with experimental results available in a video at https://youtu.be/nL1BMHpMR7c.

</details>


### [3] [A Taylor Series Approach to Correct Localization Errors in Robotic Field Mapping using Gaussian Processes](https://arxiv.org/abs/2601.20149)
*Muzaffar Qureshi,Tochukwu Elijah Ogri,Kyle Volle,Rushikesh Kamalapurkar*

Main category: cs.RO

TL;DR: 提出一种针对移动机器人定位误差的高斯过程模型更新方法，通过预计算雅可比和海森矩阵实现实时修正，避免完全重新训练


<details>
  <summary>Details</summary>
Motivation: 现实世界中的标量场映射应用通常依赖移动机器人收集测量数据，但机器人定位不完美导致测量位置存在误差，这种位置不确定性会降低高斯过程模型的均值和协方差估计精度

Method: 利用核函数的可微性，开发基于预计算雅可比和海森矩阵的二阶修正算法，当获得改进的位置估计时，能够实时更新高斯过程模型，而无需完全重新训练

Result: 仿真结果表明，相比完全重新训练模型，该方法在预测精度和计算效率方面都有显著提升

Conclusion: 该方法有效解决了移动机器人定位误差对高斯过程模型的影响，通过实时修正机制提高了标量场映射的准确性和计算效率

Abstract: Gaussian Processes (GPs) are powerful non-parametric Bayesian models for regression of scalar fields, formulated under the assumption that measurement locations are perfectly known and the corresponding field measurements have Gaussian noise. However, many real-world scalar field mapping applications rely on sensor-equipped mobile robots to collect field measurements, where imperfect localization introduces state uncertainty. Such discrepancies between the estimated and true measurement locations degrade GP mean and covariance estimates. To address this challenge, we propose a method for updating the GP models when improved estimates become available. Leveraging the differentiability of the kernel function, a second-order correction algorithm is developed using the precomputed Jacobians and Hessians of the GP mean and covariance functions for real-time refinement based on measurement location discrepancy data. Simulation results demonstrate improved prediction accuracy and computational efficiency compared to full model retraining.

</details>


### [4] [TRACER: Texture-Robust Affordance Chain-of-Thought for Deformable-Object Refinement](https://arxiv.org/abs/2601.20208)
*Wanjun Jia,Kang Li,Fan Yang,Mengfei Duan,Wenrui Chen,Yiming Jiang,Hui Zhang,Kailun Yang,Zhiyong Li,Yaonan Wang*

Main category: cs.RO

TL;DR: TRACER框架通过纹理鲁棒的affordance链式思维与可变形物体细化，解决了机器人操作可变形物体时语义指令与物理交互点对齐的挑战，显著提升了功能区域预测精度和长时任务成功率。


<details>
  <summary>Details</summary>
Motivation: 机器人操作可变形物体的核心挑战在于高维语义指令与物理交互点在复杂外观和纹理变化下的对齐问题。现有基于视觉的affordance预测方法由于可变形物体近乎无限的自由度、复杂动力学和异质模式，常常面临边界溢出和功能区域碎片化的问题。

Method: 提出TRACER框架，包含三个核心组件：1) 树状Affordance链式思维(TA-CoT)，将高层任务意图分解为层次化子任务语义；2) 空间约束边界细化(SCBR)机制，抑制预测溢出，引导感知响应收敛到真实交互流形；3) 交互收敛细化流(ICRF)，聚合被外观噪声污染的离散像素，增强功能区域的空间连续性和物理合理性。

Result: 在Fine-AGDDO15数据集和真实机器人平台上进行的大量实验表明，TRACER显著提高了对可变形物体固有纹理和模式的affordance定位精度，更重要的是，它提升了长时任务的成功率，有效弥合了高层语义推理与低层物理执行之间的差距。

Conclusion: TRACER框架通过建立从层次化语义推理到外观鲁棒且物理一致的功能区域细化的跨层次映射，成功解决了可变形物体操作中的语义-物理对齐问题，为机器人操作可变形物体提供了有效的解决方案。

Abstract: The central challenge in robotic manipulation of deformable objects lies in aligning high-level semantic instructions with physical interaction points under complex appearance and texture variations. Due to near-infinite degrees of freedom, complex dynamics, and heterogeneous patterns, existing vision-based affordance prediction methods often suffer from boundary overflow and fragmented functional regions. To address these issues, we propose TRACER, a Texture-Robust Affordance Chain-of-thought with dEformable-object Refinement framework, which establishes a cross-hierarchical mapping from hierarchical semantic reasoning to appearance-robust and physically consistent functional region refinement. Specifically, a Tree-structured Affordance Chain-of-Thought (TA-CoT) is formulated to decompose high-level task intentions into hierarchical sub-task semantics, providing consistent guidance across various execution stages. To ensure spatial integrity, a Spatial-Constrained Boundary Refinement (SCBR) mechanism is introduced to suppress prediction spillover, guiding the perceptual response to converge toward authentic interaction manifolds. Furthermore, an Interactive Convergence Refinement Flow (ICRF) is developed to aggregate discrete pixels corrupted by appearance noise, significantly enhancing the spatial continuity and physical plausibility of the identified functional regions. Extensive experiments conducted on the Fine-AGDDO15 dataset and a real-world robotic platform demonstrate that TRACER significantly improves affordance grounding precision across diverse textures and patterns inherent to deformable objects. More importantly, it enhances the success rate of long-horizon tasks, effectively bridging the gap between high-level semantic reasoning and low-level physical execution. The source code and dataset will be made publicly available at https://github.com/Dikay1/TRACER.

</details>


### [5] [TouchGuide: Inference-Time Steering of Visuomotor Policies via Touch Guidance](https://arxiv.org/abs/2601.20239)
*Zhemeng Zhang,Jiahua Ma,Xincheng Yang,Xin Wen,Yuzhi Zhang,Boyan Li,Yiran Qin,Jin Liu,Can Zhao,Li Kang,Haoqin Hong,Zhenfei Yin,Philip Torr,Hao Su,Ruimao Zhang,Daolin Ma*

Main category: cs.RO

TL;DR: TouchGuide：一种新颖的跨策略视觉-触觉融合范式，通过两阶段推理引导预训练的视觉运动策略，利用触觉反馈提升精细接触操作性能


<details>
  <summary>Details</summary>
Motivation: 机器人精细和接触丰富的操作仍然具有挑战性，主要原因是触觉反馈未得到充分利用。现有方法在融合视觉和触觉模态方面存在不足，需要更有效的融合策略来提升接触操作性能。

Method: 提出TouchGuide两阶段推理框架：第一阶段使用预训练的扩散或流匹配视觉运动策略仅基于视觉输入生成粗略动作；第二阶段通过任务特定的接触物理模型（CPM）提供触觉指导，使用对比学习训练CPM生成触觉感知的可行性分数，引导采样过程满足物理接触约束。同时开发TacUMI数据收集系统，利用刚性指尖获取直接触觉反馈，平衡精度与成本。

Result: 在五个具有挑战性的接触丰富任务（如系鞋带、芯片交接等）上进行广泛实验，TouchGuide始终显著优于最先进的视觉-触觉策略，证明了其有效性。

Conclusion: TouchGuide通过创新的跨策略视觉-触觉融合范式，有效利用触觉反馈提升机器人精细接触操作能力，同时TacUMI系统为高质量触觉数据收集提供了经济有效的解决方案。

Abstract: Fine-grained and contact-rich manipulation remain challenging for robots, largely due to the underutilization of tactile feedback. To address this, we introduce TouchGuide, a novel cross-policy visuo-tactile fusion paradigm that fuses modalities within a low-dimensional action space. Specifically, TouchGuide operates in two stages to guide a pre-trained diffusion or flow-matching visuomotor policy at inference time. First, the policy produces a coarse, visually-plausible action using only visual inputs during early sampling. Second, a task-specific Contact Physical Model (CPM) provides tactile guidance to steer and refine the action, ensuring it aligns with realistic physical contact conditions. Trained through contrastive learning on limited expert demonstrations, the CPM provides a tactile-informed feasibility score to steer the sampling process toward refined actions that satisfy physical contact constraints. Furthermore, to facilitate TouchGuide training with high-quality and cost-effective data, we introduce TacUMI, a data collection system. TacUMI achieves a favorable trade-off between precision and affordability; by leveraging rigid fingertips, it obtains direct tactile feedback, thereby enabling the collection of reliable tactile data. Extensive experiments on five challenging contact-rich tasks, such as shoe lacing and chip handover, show that TouchGuide consistently and significantly outperforms state-of-the-art visuo-tactile policies.

</details>


### [6] [Tactile-Force Alignment in Vision-Language-Action Models for Force-aware Manipulation](https://arxiv.org/abs/2601.20321)
*Yuzhe Huang,Pei Lin,Wanlin Li,Daohan Li,Jiajun Li,Jiaming Jiang,Chenxi Xiao,Ziyuan Jiao*

Main category: cs.RO

TL;DR: 提出TaF-VLA框架，通过触觉-力对齐而非触觉-视觉对齐，将高维触觉观测与物理交互力关联，提升VLA模型在接触密集型任务中的物理直觉和力感知能力。


<details>
  <summary>Details</summary>
Motivation: 当前VLA模型主要依赖视觉模态，缺乏接触密集型任务所需的物理直觉和精确力调节能力。现有将触觉传感融入VLA的方法通常将触觉输入视为辅助视觉纹理，忽视了表面变形与交互动力学之间的内在关联。

Method: 提出触觉-力对齐范式，开发自动触觉-力数据采集设备并构建TaF-Dataset。核心是Tactile-Force Adapter（TaF-Adapter），作为触觉传感器编码器，提取离散化潜在信息来编码触觉观测，确保学习到的表征捕获历史依赖、噪声不敏感的物理动力学而非静态视觉纹理。

Result: 在真实世界实验中，TaF-VLA策略显著优于最先进的触觉-视觉对齐和纯视觉基线，在接触密集型任务中验证了其通过跨模态物理推理实现稳健、力感知操作的能力。

Conclusion: 通过触觉-力对齐范式，TaF-VLA框架成功将高维触觉观测与物理交互力关联，为VLA模型提供了物理直觉，显著提升了接触密集型任务的性能，实现了从触觉-视觉对齐到触觉-力对齐的范式转变。

Abstract: Vision-Language-Action (VLA) models have recently emerged as powerful generalists for robotic manipulation. However, due to their predominant reliance on visual modalities, they fundamentally lack the physical intuition required for contact-rich tasks that require precise force regulation and physical reasoning. Existing attempts to incorporate vision-based tactile sensing into VLA models typically treat tactile inputs as auxiliary visual textures, thereby overlooking the underlying correlation between surface deformation and interaction dynamics. To bridge this gap, we propose a paradigm shift from tactile-vision alignment to tactile-force alignment. Here, we introduce TaF-VLA, a framework that explicitly grounds high-dimensional tactile observations in physical interaction forces. To facilitate this, we develop an automated tactile-force data acquisition device and curate the TaF-Dataset, comprising over 10 million synchronized tactile observations, 6-axis force/torque, and matrix force map. To align sequential tactile observations with interaction forces, the central component of our approach is the Tactile-Force Adapter (TaF-Adapter), a tactile sensor encoder that extracts discretized latent information for encoding tactile observations. This mechanism ensures that the learned representations capture history-dependent, noise-insensitive physical dynamics rather than static visual textures. Finally, we integrate this force-aligned encoder into a VLA backbone. Extensive real-world experiments demonstrate that TaF-VLA policy significantly outperforms state-of-the-art tactile-vision-aligned and vision-only baselines on contact-rich tasks, verifying its ability to achieve robust, force-aware manipulation through cross-modal physical reasoning.

</details>


### [7] [Demonstration-Free Robotic Control via LLM Agents](https://arxiv.org/abs/2601.20334)
*Brian Y. Tsui,Alan Y. Fang,Tiffany J. Hwu*

Main category: cs.RO

TL;DR: FAEA将前沿LLM智能体框架直接应用于机器人操作，无需任务特定演示或微调，在多个基准测试中达到接近VLA模型的性能


<details>
  <summary>Details</summary>
Motivation: 当前视觉-语言-动作模型需要任务特定演示和微调，且领域迁移能力差，研究是否通用LLM智能体框架可作为替代控制范式

Method: 应用未修改的前沿智能体框架Claude Agent SDK到具身操作，利用其迭代推理能力进行策略规划，无需演示或微调

Result: 在LIBERO、ManiSkill3和MetaWorld基准测试中分别达到84.9%、85.7%和96%成功率，接近需要少于100次演示的VLA模型性能

Conclusion: 通用智能体足以处理以深思熟虑的任务级规划为主的操纵任务，为机器人系统利用前沿模型基础设施开辟了新路径

Abstract: Robotic manipulation has increasingly adopted vision-language-action (VLA) models, which achieve strong performance but typically require task-specific demonstrations and fine-tuning, and often generalize poorly under domain shift. We investigate whether general-purpose large language model (LLM) agent frameworks, originally developed for software engineering, can serve as an alternative control paradigm for embodied manipulation. We introduce FAEA (Frontier Agent as Embodied Agent), which applies an LLM agent framework directly to embodied manipulation without modification. Using the same iterative reasoning that enables software agents to debug code, FAEA enables embodied agents to reason through manipulation strategies. We evaluate an unmodified frontier agent, Claude Agent SDK, across the LIBERO, ManiSkill3, and MetaWorld benchmarks. With privileged environment state access, FAEA achieves success rates of 84.9%, 85.7%, and 96%, respectively. This level of task success approaches that of VLA models trained with less than 100 demonstrations per task, without requiring demonstrations or fine-tuning. With one round of human feedback as an optional optimization, performance increases to 88.2% on LIBERO. This demonstration-free capability has immediate practical value: FAEA can autonomously explore novel scenarios in simulation and generate successful trajectories for training data augmentation in embodied learning. Our results indicate that general-purpose agents are sufficient for a class of manipulation tasks dominated by deliberative, task-level planning. This opens a path for robotics systems to leverage actively maintained agent infrastructure and benefit directly from ongoing advances in frontier models. Code is available at https://github.com/robiemusketeer/faea-sim

</details>


### [8] [RF-MatID: Dataset and Benchmark for Radio Frequency Material Identification](https://arxiv.org/abs/2601.20377)
*Xinyan Chen,Qinchun Li,Ruiqin Ma,Jiaqi Bai,Li Yi,Jianfei Yang*

Main category: cs.RO

TL;DR: RF-MatID：首个开源、大规模、宽频带、几何多样化的射频数据集，用于细粒度材料识别，包含16个细粒度类别、142k样本，涵盖4-43.5GHz频段，并建立了多设置、多协议的基准测试。


<details>
  <summary>Details</summary>
Motivation: 当前基于视觉的材料识别受光学传感器固有局限，而基于射频的方法能揭示材料内在属性但缺乏大规模公开数据集和学习方法的基准测试，阻碍了该领域发展。

Method: 创建RF-MatID数据集，包含16个细粒度材料类别（分为5个超类），覆盖4-43.5GHz宽频带，142k个频域和时域样本，系统控制入射角和距离变化，并建立多设置、多协议基准评估深度学习模型。

Result: 建立了首个开源、大规模、宽频带、几何多样化的射频材料识别数据集，提供了频域和时域表示，包含系统几何扰动，并建立了包含5种频率分配协议的多设置基准，支持分布内性能和分布外鲁棒性评估。

Conclusion: RF-MatID旨在促进可重复研究、加速算法进步、增强跨领域鲁棒性，并支持射频材料识别在实际应用中的发展，填补了该领域缺乏大规模公开数据集和基准测试的空白。

Abstract: Accurate material identification plays a crucial role in embodied AI systems, enabling a wide range of applications. However, current vision-based solutions are limited by the inherent constraints of optical sensors, while radio-frequency (RF) approaches, which can reveal intrinsic material properties, have received growing attention. Despite this progress, RF-based material identification remains hindered by the lack of large-scale public datasets and the limited benchmarking of learning-based approaches. In this work, we present RF-MatID, the first open-source, large-scale, wide-band, and geometry-diverse RF dataset for fine-grained material identification. RF-MatID includes 16 fine-grained categories grouped into 5 superclasses, spanning a broad frequency range from 4 to 43.5 GHz, and comprises 142k samples in both frequency- and time-domain representations. The dataset systematically incorporates controlled geometry perturbations, including variations in incidence angle and stand-off distance. We further establish a multi-setting, multi-protocol benchmark by evaluating state-of-the-art deep learning models, assessing both in-distribution performance and out-of-distribution robustness under cross-angle and cross-distance shifts. The 5 frequency-allocation protocols enable systematic frequency- and region-level analysis, thereby facilitating real-world deployment. RF-MatID aims to enable reproducible research, accelerate algorithmic advancement, foster cross-domain robustness, and support the development of real-world application in RF-based material identification.

</details>


### [9] [STORM: Slot-based Task-aware Object-centric Representation for robotic Manipulation](https://arxiv.org/abs/2601.20381)
*Alexandre Chapin,Emmanuel Dellandréa,Liming Chen*

Main category: cs.RO

TL;DR: STORM提出了一种轻量级的对象中心表示模块，通过多阶段训练策略将冻结的视觉基础模型增强为语义感知的槽位表示，用于机器人操控任务。


<details>
  <summary>Details</summary>
Motivation: 视觉基础模型为机器人提供强大的感知特征，但其密集表示缺乏明确的对象级结构，限制了在操控任务中的鲁棒性和可收缩性。

Method: STORM采用多阶段训练策略：首先通过语言嵌入进行视觉-语义预训练来稳定对象中心槽位，然后与下游操控策略联合适应，避免槽位退化并保持语义一致性。

Result: 在对象发现基准和模拟操控任务中，STORM相比直接使用冻结基础模型特征或端到端训练对象中心表示，提高了对视觉干扰的泛化能力和控制性能。

Conclusion: 多阶段适应是将通用基础模型特征转化为任务感知的对象中心表示的高效机制，适用于机器人控制。

Abstract: Visual foundation models provide strong perceptual features for robotics, but their dense representations lack explicit object-level structure, limiting robustness and contractility in manipulation tasks. We propose STORM (Slot-based Task-aware Object-centric Representation for robotic Manipulation), a lightweight object-centric adaptation module that augments frozen visual foundation models with a small set of semantic-aware slots for robotic manipulation. Rather than retraining large backbones, STORM employs a multi-phase training strategy: object-centric slots are first stabilized through visual--semantic pretraining using language embeddings, then jointly adapted with a downstream manipulation policy. This staged learning prevents degenerate slot formation and preserves semantic consistency while aligning perception with task objectives. Experiments on object discovery benchmarks and simulated manipulation tasks show that STORM improves generalization to visual distractors, and control performance compared to directly using frozen foundation model features or training object-centric representations end-to-end. Our results highlight multi-phase adaptation as an efficient mechanism for transforming generic foundation model features into task-aware object-centric representations for robotic control.

</details>


### [10] [A Practical Framework of Key Performance Indicators for Multi-Robot Lunar and Planetary Field Tests](https://arxiv.org/abs/2601.20529)
*Julia Richter,David Oberacker,Gabriela Ligeza,Valentin T. Bickel,Philip Arm,William Talbot,Marvin Grosse Besselmann,Florian Kehl,Tristan Schnell,Hendrik Kolvenbach,Rüdiger Dillmann,Arne Roennau,Marco Hutter*

Main category: cs.RO

TL;DR: 本文提出一个用于月球多机器人勘探任务的关键绩效指标框架，旨在解决现有实地试验因平台和实验设置差异而难以比较结果的问题，强调效率、鲁棒性和精度的场景依赖性优先级。


<details>
  <summary>Details</summary>
Motivation: 月球机器人勘探任务面临多样地形和恶劣环境，虽然已有许多模拟实地试验，但由于机器人平台和实验设置的差异，比较这些试验结果仍然困难。现有评估通常使用特定场景的工程指标，未能建立实地性能与科学目标之间的明确联系。

Method: 从三个现实的多机器人月球场景中推导出结构化KPI框架，这些场景反映了科学目标和操作约束。该框架强调效率、鲁棒性和精度的场景依赖性优先级，并专门为实地部署的实际应用而设计。通过多机器人实地测试验证了该框架。

Result: 在多机器人实地测试中验证了该框架，发现效率和鲁棒性相关的KPI实用且易于应用，而精度导向的KPI需要可靠的基准数据，这在户外模拟环境中并不总是可行获取。

Conclusion: 该框架可作为通用评估标准，实现多机器人实地试验的一致、目标导向比较，支持未来行星探索机器人系统的系统化开发。

Abstract: Robotic prospecting for critical resources on the Moon, such as ilmenite, rare earth elements, and water ice, requires robust exploration methods given the diverse terrain and harsh environmental conditions. Although numerous analog field trials address these goals, comparing their results remains challenging because of differences in robot platforms and experimental setups. These missions typically assess performance using selected, scenario-specific engineering metrics that fail to establish a clear link between field performance and science-driven objectives. In this paper, we address this gap by deriving a structured framework of KPI from three realistic multi-robot lunar scenarios reflecting scientific objectives and operational constraints. Our framework emphasizes scenario-dependent priorities in efficiency, robustness, and precision, and is explicitly designed for practical applicability in field deployments. We validated the framework in a multi-robot field test and found it practical and easy to apply for efficiency- and robustness-related KPI, whereas precision-oriented KPI require reliable ground-truth data that is not always feasible to obtain in outdoor analog environments. Overall, we propose this framework as a common evaluation standard enabling consistent, goal-oriented comparison of multi-robot field trials and supporting systematic development of robotic systems for future planetary exploration.

</details>


### [11] [Vibro-Sense: Robust Vibration-based Impulse Response Localization and Trajectory Tracking for Robotic Hands](https://arxiv.org/abs/2601.20555)
*Wadhah Zai El Amri,Nicolás Navarro-Guerrero*

Main category: cs.RO

TL;DR: 通过低成本压电麦克风和音频频谱变换器，实现机器人全身触觉定位，静态误差小于5mm，材料特性影响显著，系统对机器人自身运动具有鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统触觉皮肤成本高且集成复杂，需要一种可扩展、低成本的全身触觉感知替代方案。

Method: 在机器人手上安装7个低成本压电麦克风，利用音频频谱变换器解码物理交互产生的振动信号，通过振动特征实现触觉定位。

Result: 静态条件下定位误差小于5mm；硬质材料（如金属）在脉冲响应定位中表现优异，纹理材料（如木材）在轨迹跟踪中提供更好的摩擦特征；系统对机器人自身运动具有鲁棒性。

Conclusion: 复杂物理接触动力学可以从简单的振动信号中有效解码，为机器人领域提供了广泛、经济可行的触觉感知途径，相关数据集、模型和实验设置已开源。

Abstract: Rich contact perception is crucial for robotic manipulation, yet traditional tactile skins remain expensive and complex to integrate. This paper presents a scalable alternative: high-accuracy whole-body touch localization via vibro-acoustic sensing. By equipping a robotic hand with seven low-cost piezoelectric microphones and leveraging an Audio Spectrogram Transformer, we decode the vibrational signatures generated during physical interaction. Extensive evaluation across stationary and dynamic tasks reveals a localization error of under 5 mm in static conditions. Furthermore, our analysis highlights the distinct influence of material properties: stiff materials (e.g., metal) excel in impulse response localization due to sharp, high-bandwidth responses, whereas textured materials (e.g., wood) provide superior friction-based features for trajectory tracking. The system demonstrates robustness to the robot's own motion, maintaining effective tracking even during active operation. Our primary contribution is demonstrating that complex physical contact dynamics can be effectively decoded from simple vibrational signals, offering a viable pathway to widespread, affordable contact perception in robotics. To accelerate research, we provide our full datasets, models, and experimental setups as open-source resources.

</details>


### [12] [MeCo: Enhancing LLM-Empowered Multi-Robot Collaboration via Similar Task Memoization](https://arxiv.org/abs/2601.20577)
*Baiqing Wang,Helei Cui,Bo Zhang,Xiaolong Zheng,Bin Guo,Zhiwen Yu*

Main category: cs.RO

TL;DR: MeCo框架通过"缓存与重用"机制，利用相似性测试识别和复用先前解决的任务方案，显著减少多机器人协作中的冗余计算，提高效率和成功率。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的多机器人协作方法虽然灵活，但在处理相似任务时需要重新规划，忽略了任务间的相似性，导致计算效率低下。需要一种能够识别和复用相似任务解决方案的框架来减少冗余计算。

Method: 提出MeCo框架，采用"缓存与重用"（记忆化）原则，引入新的相似性测试方法，检索先前解决的高相关性任务，实现有效的计划重用而无需重新调用LLM。同时创建MeCoBench基准测试来评估相似任务协作场景的性能。

Result: 实验结果表明，与最先进方法相比，MeCo显著降低了规划成本并提高了成功率。

Conclusion: MeCo框架通过相似性感知的多机器人协作，有效解决了现有LLM赋能方法在处理相似任务时的效率问题，为多机器人系统的实际应用提供了更高效的解决方案。

Abstract: Multi-robot systems have been widely deployed in real-world applications, providing significant improvements in efficiency and reductions in labor costs. However, most existing multi-robot collaboration methods rely on extensive task-specific training, which limits their adaptability to new or diverse scenarios. Recent research leverages the language understanding and reasoning capabilities of large language models (LLMs) to enable more flexible collaboration without specialized training. Yet, current LLM-empowered approaches remain inefficient: when confronted with identical or similar tasks, they must replan from scratch because they omit task-level similarities. To address this limitation, we propose MeCo, a similarity-aware multi-robot collaboration framework that applies the principle of ``cache and reuse'' (a.k.a., memoization) to reduce redundant computation. Unlike simple task repetition, identifying and reusing solutions for similar but not identical tasks is far more challenging, particularly in multi-robot settings. To this end, MeCo introduces a new similarity testing method that retrieves previously solved tasks with high relevance, enabling effective plan reuse without re-invoking LLMs. Furthermore, we present MeCoBench, the first benchmark designed to evaluate performance on similar-task collaboration scenarios. Experimental results show that MeCo substantially reduces planning costs and improves success rates compared with state-of-the-art approaches.

</details>


### [13] [GPO: Growing Policy Optimization for Legged Robot Locomotion and Whole-Body Control](https://arxiv.org/abs/2601.20668)
*Shuhao Liao,Peizhuo Li,Xinrong Yang,Linnan Chang,Zhaoxin Fan,Qing Wang,Lei Shi,Yuhong Cao,Wenjun Wu,Guillaume Sartoretti*

Main category: cs.RO

TL;DR: GPO是一种用于腿式机器人强化学习的训练框架，通过时变动作变换限制早期动作空间以促进有效数据收集，然后逐步扩展动作空间以增强探索，在扭矩控制任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 腿式机器人的强化学习训练面临高维连续动作空间、硬件限制和探索不足等挑战。现有方法在位置控制中表现良好，但在扭矩控制中效果不佳，因为扭矩控制需要更有效的动作空间探索和梯度信号获取。

Method: 提出Growing Policy Optimization (GPO)框架，应用时变动作变换：在训练早期限制有效动作空间以鼓励更有效的数据收集和策略学习，然后逐步扩展动作空间以增强探索并实现更高期望回报。证明该变换保持了PPO更新规则，仅引入有界、渐近消失的梯度失真。

Result: 在四足和六足机器人上评估GPO，包括将仿真训练的策略零样本部署到硬件上。使用GPO训练的策略始终获得更好的性能。GPO为学习腿式运动提供了一个通用、环境无关的优化框架。

Conclusion: GPO通过时变动作变换有效解决了腿式机器人扭矩控制中的探索难题，为腿式运动学习提供了稳定且性能优越的训练框架，具有良好的通用性和实际部署能力。

Abstract: Training reinforcement learning (RL) policies for legged robots remains challenging due to high-dimensional continuous actions, hardware constraints, and limited exploration. Existing methods for locomotion and whole-body control work well for position-based control with environment-specific heuristics (e.g., reward shaping, curriculum design, and manual initialization), but are less effective for torque-based control, where sufficiently exploring the action space and obtaining informative gradient signals for training is significantly more difficult. We introduce Growing Policy Optimization (GPO), a training framework that applies a time-varying action transformation to restrict the effective action space in the early stage, thereby encouraging more effective data collection and policy learning, and then progressively expands it to enhance exploration and achieve higher expected return. We prove that this transformation preserves the PPO update rule and introduces only bounded, vanishing gradient distortion, thereby ensuring stable training. We evaluate GPO on both quadruped and hexapod robots, including zero-shot deployment of simulation-trained policies on hardware. Policies trained with GPO consistently achieve better performance. These results suggest that GPO provides a general, environment-agnostic optimization framework for learning legged locomotion.

</details>


### [14] [Tendon-based modelling, estimation and control for a simulated high-DoF anthropomorphic hand model](https://arxiv.org/abs/2601.20682)
*Péter Polcz,Katalin Schäffer,Miklós Koller*

Main category: cs.RO

TL;DR: 提出一种基于肌腱位移和张力测量来估计仿人机械手关节角度的计算方法，无需直接关节传感，并通过雅可比PI控制器实现闭环手势跟踪。


<details>
  <summary>Details</summary>
Motivation: 肌腱驱动的仿人机械手通常缺乏直接关节角度传感，因为集成关节编码器会影响机械紧凑性和灵巧性。需要一种间接估计关节位置的方法。

Method: 1. 基于Denavit-Hartenberg约定建立仿人机械手高效运动学建模框架；2. 使用简化肌腱模型推导肌腱状态与关节位置的非线性方程组；3. 通过非线性优化方法求解；4. 采用带前馈项的雅可比PI控制器实现闭环控制。

Result: 在MuJoCo仿真环境中使用解剖学正确的生物机电手（每个长指5自由度，拇指6自由度）验证了所提估计和控制框架的有效性和局限性。

Conclusion: 提出了一种无需直接关节传感即可估计仿人机械手关节角度的方法，并通过仿真验证了该框架在手势跟踪中的可行性，为紧凑型仿人机械手控制提供了解决方案。

Abstract: Tendon-driven anthropomorphic robotic hands often lack direct joint angle sensing, as the integration of joint encoders can compromise mechanical compactness and dexterity. This paper presents a computational method for estimating joint positions from measured tendon displacements and tensions. An efficient kinematic modeling framework for anthropomorphic hands is first introduced based on the Denavit-Hartenberg convention. Using a simplified tendon model, a system of nonlinear equations relating tendon states to joint positions is derived and solved via a nonlinear optimization approach. The estimated joint angles are then employed for closed-loop control through a Jacobian-based proportional-integral (PI) controller augmented with a feedforward term, enabling gesture tracking without direct joint sensing. The effectiveness and limitations of the proposed estimation and control framework are demonstrated in the MuJoCo simulation environment using the Anatomically Correct Biomechatronic Hand, featuring five degrees of freedom for each long finger and six degrees of freedom for the thumb.

</details>


### [15] [One Step Is Enough: Dispersive MeanFlow Policy Optimization](https://arxiv.org/abs/2601.20701)
*Guowei Zou,Haitao Wang,Hejun Wu,Yukun Qian,Yuhang Wang,Weibing Li*

Main category: cs.RO

TL;DR: DMPO是一个用于实时机器人控制的单步生成策略框架，通过MeanFlow实现无需知识蒸馏的单步推理，结合分散正则化和RL微调，在保持性能的同时实现5-20倍推理加速，满足>120Hz的实时控制需求。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散和流匹配的生成策略需要多步采样，这在时间关键的实时机器人控制场景中存在根本性限制，无法满足快速动作生成的需求。

Method: 提出DMPO统一框架，包含三个关键组件：1) MeanFlow - 通过数学推导实现无需知识蒸馏的单步推理；2) 分散正则化 - 防止表示崩溃；3) 强化学习微调 - 超越专家演示。采用轻量级模型架构。

Result: 在RoboMimic操作和OpenAI Gym运动基准测试中，DMPO表现出与多步基线相当或更优的性能，实现5-20倍推理加速，满足>120Hz实时控制需求，在高性能GPU上可达数百赫兹。在Franka-Emika-Panda机器人上的物理部署验证了实际应用性。

Conclusion: DMPO通过创新的单步生成框架解决了实时机器人控制中的速度瓶颈，在保持性能的同时显著提升推理速度，为时间关键的控制场景提供了实用解决方案。

Abstract: Real-time robotic control demands fast action generation. However, existing generative policies based on diffusion and flow matching require multi-step
  sampling, fundamentally limiting deployment in time-critical scenarios. We propose Dispersive MeanFlow Policy Optimization (DMPO), a unified framework that
  enables true one-step generation through three key components: MeanFlow for mathematically-derived single-step inference without knowledge distillation,
  dispersive regularization to prevent representation collapse, and reinforcement learning (RL) fine-tuning to surpass expert demonstrations. Experiments
  across RoboMimic manipulation and OpenAI Gym locomotion benchmarks demonstrate competitive or superior performance compared to multi-step baselines. With
  our lightweight model architecture and the three key algorithmic components working in synergy, DMPO exceeds real-time control requirements (>120Hz) with
  5-20x inference speedup, reaching hundreds of Hertz on high-performance GPUs. Physical deployment on a Franka-Emika-Panda robot validates real-world
  applicability.

</details>


### [16] [A Methodology for Designing Knowledge-Driven Missions for Robots](https://arxiv.org/abs/2601.20797)
*Guillermo GP-Lenza,Carmen DR. Pita-Romero,Miguel Fernandez-Cortizas,Pascual Campoy*

Main category: cs.RO

TL;DR: 本文提出了一种在ROS 2系统中实现知识图谱的综合方法，旨在提高自主机器人任务的效率和智能性，并在Aerostack2框架中通过模拟搜救任务进行了验证。


<details>
  <summary>Details</summary>
Motivation: 当前自主机器人系统在复杂任务中面临决策效率低下和智能性不足的问题。通过将知识图谱集成到ROS 2系统中，可以更好地组织和利用任务相关信息，从而提高机器人任务的执行效率和智能决策能力。

Method: 方法包括五个关键步骤：1) 定义初始和目标条件；2) 结构化任务和子任务；3) 规划任务序列；4) 在知识图谱中表示任务相关数据；5) 使用高级语言设计任务。整个流程在Aerostack2框架中实现，并在Gazebo环境中通过模拟搜救任务进行验证。

Result: 在模拟的搜救任务中，无人机能够自主定位目标，证明了该方法在提高决策能力和任务性能方面的有效性。知识图谱的使用显著改善了系统的智能性和执行效率。

Conclusion: 该研究提出的知识图谱集成方法为ROS 2系统提供了一种有效的解决方案，能够显著提升自主机器人任务的智能性和执行效率，在复杂任务场景中具有重要应用价值。

Abstract: This paper presents a comprehensive methodology for implementing knowledge graphs in ROS 2 systems, aiming to enhance the efficiency and intelligence of autonomous robotic missions. The methodology encompasses several key steps: defining initial and target conditions, structuring tasks and subtasks, planning their sequence, representing task-related data in a knowledge graph, and designing the mission using a high-level language. Each step builds on the previous one to ensure a cohesive process from initial setup to final execution. A practical implementation within the Aerostack2 framework is demonstrated through a simulated search and rescue mission in a Gazebo environment, where drones autonomously locate a target. This implementation highlights the effectiveness of the methodology in improving decision-making and mission performance by leveraging knowledge graphs.

</details>


### [17] [End-to-end example-based sim-to-real RL policy transfer based on neural stylisation with application to robotic cutting](https://arxiv.org/abs/2601.20846)
*Jamie Hathaway,Alireza Rastegarpanah,Rustam Stolkin*

Main category: cs.RO

TL;DR: 提出一种基于神经风格迁移的强化学习策略仿真到现实迁移方法，通过变分自编码器学习特征表示并生成弱配对轨迹，在机器人切割未知材料任务中表现优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 强化学习在机器人控制中依赖大量仿真数据，但仿真与现实之间存在领域差距，且现实世界样本有限，限制了实际部署。

Method: 将神经风格迁移从图像处理重新解释到强化学习领域，使用变分自编码器联合学习自监督特征表示，生成弱配对的源-目标轨迹以提高合成轨迹的物理真实性。

Result: 在机器人切割未知材料任务中，相比基线方法（包括先前工作、CycleGAN和条件变分自编码器时间序列翻译），本方法在任务完成时间和行为稳定性方面表现更好，仅需少量现实数据，对几何和材料变化具有鲁棒性。

Conclusion: 该方法展示了在真实世界奖励信息不可用的接触密集型任务中策略适应的可行性，为解决仿真到现实迁移问题提供了有效框架。

Abstract: Whereas reinforcement learning has been applied with success to a range of robotic control problems in complex, uncertain environments, reliance on extensive data - typically sourced from simulation environments - limits real-world deployment due to the domain gap between simulated and physical systems, coupled with limited real-world sample availability. We propose a novel method for sim-to-real transfer of reinforcement learning policies, based on a reinterpretation of neural style transfer from image processing to synthesise novel training data from unpaired unlabelled real world datasets. We employ a variational autoencoder to jointly learn self-supervised feature representations for style transfer and generate weakly paired source-target trajectories to improve physical realism of synthesised trajectories. We demonstrate the application of our approach based on the case study of robot cutting of unknown materials. Compared to baseline methods, including our previous work, CycleGAN, and conditional variational autoencoder-based time series translation, our approach achieves improved task completion time and behavioural stability with minimal real-world data. Our framework demonstrates robustness to geometric and material variation, and highlights the feasibility of policy adaptation in challenging contact-rich tasks where real-world reward information is unavailable.

</details>
