<div id=toc></div>

# Table of Contents

- [cs.RO](#cs.RO) [Total: 37]


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [1] [LatentAM: Real-Time, Large-Scale Latent Gaussian Attention Mapping via Online Dictionary Learning](https://arxiv.org/abs/2602.12314)
*Junwoon Lee,Yulun Tian*

Main category: cs.RO

TL;DR: LatentAM是一个在线3D高斯泼溅映射框架，通过流式RGB-D观测构建可扩展的潜在特征地图，用于开放词汇机器人感知，无需特定模型解码器或预训练，支持即插即用不同视觉语言模型。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常需要蒸馏高维视觉语言模型嵌入并使用模型特定解码器，这限制了灵活性和可扩展性。需要一种模型无关、无需预训练的方法，能够在线适应场景语义变化，同时支持长轨迹和大规模环境。

Method: 1. 为每个高斯基元关联紧凑查询向量，通过注意力机制与可学习字典转换为近似VLM嵌入；2. 提出在线字典学习方法，从流式观测高效初始化并在线优化；3. 采用基于体素哈希的高效地图管理策略，GPU优化局部活动地图，CPU存储全局地图以控制内存使用。

Result: 在公共基准和大型自定义数据集上，LatentAM相比最先进方法获得显著更好的特征重建保真度，同时在评估数据集上实现接近实时的速度（12-35 FPS）。

Conclusion: LatentAM提供了一种模型无关、无需预训练的在线3D高斯泼溅映射框架，能够高效构建可扩展的潜在特征地图，支持开放词汇机器人感知，在特征重建质量和计算效率方面均表现出色。

Abstract: We present LatentAM, an online 3D Gaussian Splatting (3DGS) mapping framework that builds scalable latent feature maps from streaming RGB-D observations for open-vocabulary robotic perception. Instead of distilling high-dimensional Vision-Language Model (VLM) embeddings using model-specific decoders, LatentAM proposes an online dictionary learning approach that is both model-agnostic and pretraining-free, enabling plug-and-play integration with different VLMs at test time. Specifically, our approach associates each Gaussian primitive with a compact query vector that can be converted into approximate VLM embeddings using an attention mechanism with a learnable dictionary. The dictionary is initialized efficiently from streaming observations and optimized online to adapt to evolving scene semantics under trust-region regularization. To scale to long trajectories and large environments, we further propose an efficient map management strategy based on voxel hashing, where optimization is restricted to an active local map on the GPU, while the global map is stored and indexed on the CPU to maintain bounded GPU memory usage. Experiments on public benchmarks and a large-scale custom dataset demonstrate that LatentAM attains significantly better feature reconstruction fidelity compared to state-of-the-art methods, while achieving near-real-time speed (12-35 FPS) on the evaluated datasets. Our project page is at: https://junwoonlee.github.io/projects/LatentAM

</details>


### [2] [ForeAct: Steering Your VLA with Efficient Visual Foresight Planning](https://arxiv.org/abs/2602.12322)
*Zhuoyang Zhang,Shang Yang,Qinghao Hu,Luke J. Huang,James Hou,Yufei Sun,Yao Lu,Song Han*

Main category: cs.RO

TL;DR: ForeAct是一种视觉前瞻规划器，通过想象未来观察和子任务描述来指导VLA模型逐步执行任务，在开放世界环境中显著提升执行准确性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 在开放世界环境中，视觉-语言-动作模型将高级语言指令转换为具体可执行动作具有挑战性，需要更好的规划能力来应对复杂多步任务。

Method: 提出视觉前瞻规划框架，包含高效的前瞻图像生成模块（0.33秒内生成640×480未来观察图像）和视觉语言模型推理模块。前瞻生成器在100万+多任务跨具身数据上预训练，学习鲁棒的具身动力学。

Result: 在11个多样化多步真实世界任务基准测试中，平均成功率87.4%，相比基线π0（46.5%）绝对提升40.9%，相比文本子任务引导增强的π0（57.1%）绝对提升30.3%。

Conclusion: ForeAct通过视觉前瞻规划显著提升VLA模型在复杂开放世界任务中的性能，且无需修改现有VLA架构即可无缝集成，展示了视觉想象对未来状态预测在具身智能中的重要性。

Abstract: Vision-Language-Action (VLA) models convert high-level language instructions into concrete, executable actions, a task that is especially challenging in open-world environments. We present Visual Foresight Planning (ForeAct), a general and efficient planner that guides a VLA step-by-step using imagined future observations and subtask descriptions. With an imagined future observation, the VLA can focus on visuo-motor inference rather than high-level semantic reasoning, leading to improved accuracy and generalization. Our planner comprises a highly efficient foresight image generation module that predicts a high-quality 640$\times$480 future observation from the current visual input and language instruction within only 0.33s on an H100 GPU, together with a vision-language model that reasons over the task and produces subtask descriptions for both the generator and the VLA. Importantly, state-of-the-art VLAs can integrate our planner seamlessly by simply augmenting their visual inputs, without any architectural modification. The foresight generator is pretrained on over 1 million multi-task, cross-embodiment episodes, enabling it to learn robust embodied dynamics. We evaluate our framework on a benchmark that consists of 11 diverse, multi-step real-world tasks. It achieves an average success rate of 87.4%, demonstrating a +40.9% absolute improvement over the $π_0$ baseline (46.5%) and a +30.3% absolute improvement over $π_0$ augmented with textual subtask guidance (57.1%).

</details>


### [3] [Schur-MI: Fast Mutual Information for Robotic Information Gathering](https://arxiv.org/abs/2602.12346)
*Kalvik Jakkala,Jason O'Kane,Srinivas Akella*

Main category: cs.RO

TL;DR: Schur-MI是一种高斯过程互信息计算方法，通过预计算和Schur补分解将计算复杂度从O(|V|³)降低到O(|A|³)，实现12.7倍加速，使互信息目标能用于实时机器人信息采集。


<details>
  <summary>Details</summary>
Motivation: 互信息(MI)在机器人信息采集中具有理论优势，但其高计算成本（主要来自重复的对数行列式计算）限制了在实时规划中的应用。

Method: 提出Schur-MI方法：1) 利用RIG的迭代结构预计算和重用昂贵的中间量；2) 使用Schur补分解避免大型行列式计算。

Result: 在真实世界测深数据集上实现高达12.7倍的加速，并通过自主水面车辆的自适应信息路径规划现场试验验证了实用性。

Conclusion: Schur-MI使互信息计算在在线规划中变得可行，有助于弥合信息论目标与实时机器人探索之间的差距。

Abstract: Mutual information (MI) is a principled and widely used objective for robotic information gathering (RIG), providing strong theoretical guarantees for sensor placement (SP) and informative path planning (IPP). However, its high computational cost, dominated by repeated log-determinant evaluations, has limited its use in real-time planning. This letter presents Schur-MI, a Gaussian process (GP) MI formulation that (i) leverages the iterative structure of RIG to precompute and reuse expensive intermediate quantities across planning steps, and (ii) uses a Schur-complement factorization to avoid large determinant computations. Together, these methods reduce the per-evaluation cost of MI from $\mathcal{O}(|\mathcal{V}|^3)$ to $\mathcal{O}(|\mathcal{A}|^3)$, where $\mathcal{V}$ and $\mathcal{A}$ denote the candidate and selected sensing locations, respectively. Experiments on real-world bathymetry datasets show that Schur-MI achieves up to a $12.7\times$ speedup over the standard MI formulation. Field trials with an autonomous surface vehicle (ASV) performing adaptive IPP further validate its practicality. By making MI computation tractable for online planning, Schur-MI helps bridge the gap between information-theoretic objectives and real-time robotic exploration.

</details>


### [4] [LongNav-R1: Horizon-Adaptive Multi-Turn RL for Long-Horizon VLA Navigation](https://arxiv.org/abs/2602.12351)
*Yue Hu,Avery Xi,Qixin Xiao,Seth Isaacson,Henry X. Liu,Ram Vasudevan,Maani Ghaffari*

Main category: cs.RO

TL;DR: LongNav-R1：一个端到端多轮强化学习框架，用于优化视觉-语言-动作模型在长视野导航任务中的性能，通过多轮对话式交互和自适应策略优化提升导航成功率。


<details>
  <summary>Details</summary>
Motivation: 现有单轮范式无法处理长视野导航任务，需要让智能体能够推理历史交互的因果效应和序列化未来结果，同时避免人类演示带来的行为僵化问题。

Method: 提出LongNav-R1多轮RL框架，将导航决策过程重构为VLA策略与具身环境之间的连续多轮对话；引入Horizon-Adaptive Policy Optimization机制，在优势估计中显式考虑不同视野长度，实现扩展序列上的准确时间信用分配。

Result: 在物体导航基准测试中，仅用4,000条轨迹就将Qwen3-VL-2B的成功率从64.3%提升到73.0%，表现出卓越的样本效率和优于现有方法的性能，在长视野真实世界导航设置中展现出零样本泛化能力。

Conclusion: LongNav-R1通过多轮RL框架和自适应策略优化，有效解决了长视野导航中的行为多样性和时间信用分配问题，显著提升了VLA模型的导航性能，具有实际应用价值。

Abstract: This paper develops LongNav-R1, an end-to-end multi-turn reinforcement learning (RL) framework designed to optimize Visual-Language-Action (VLA) models for long-horizon navigation. Unlike existing single-turn paradigm, LongNav-R1 reformulates the navigation decision process as a continuous multi-turn conversation between the VLA policy and the embodied environment. This multi-turn RL framework offers two distinct advantages: i) it enables the agent to reason about the causal effects of historical interactions and sequential future outcomes; and ii) it allows the model to learn directly from online interactions, fostering diverse trajectory generation and avoiding the behavioral rigidity often imposed by human demonstrations. Furthermore, we introduce Horizon-Adaptive Policy Optimization. This mechanism explicitly accounts for varying horizon lengths during advantage estimation, facilitating accurate temporal credit assignment over extended sequences. Consequently, the agent develops diverse navigation behaviors and resists collapse during long-horizon tasks. Experiments on object navigation benchmarks validate the framework's efficacy: With 4,000 rollout trajectories, LongNav-R1 boosts the Qwen3-VL-2B success rate from 64.3% to 73.0%. These results demonstrate superior sample efficiency and significantly outperform state-of-the-art methods. The model's generalizability and robustness are further validated by its zero-shot performance in long-horizon real-world navigation settings. All source code will be open-sourced upon publication.

</details>


### [5] [Predicting Dynamic Map States from Limited Field-of-View Sensor Data](https://arxiv.org/abs/2602.12360)
*Knut Peterson,David Han*

Main category: cs.RO

TL;DR: 提出一种将有限视场时间序列数据转换为单图像格式的方法，利用现有图像到图像学习模型预测动态地图状态


<details>
  <summary>Details</summary>
Motivation: 自主系统在真实场景中部署时，传感器常受限于有限的视场约束，需要在有限视场条件下推断环境信息并预测周围状态以保持安全准确的操作

Method: 将动态传感器数据表示为简单的单图像格式，该格式同时捕捉空间和时间信息，然后利用各种现有的图像到图像学习模型进行地图状态预测

Result: 在多种传感场景中能够以高准确度预测地图状态，证明了该方法的有效性

Conclusion: 通过将有限视场时间序列数据转换为单图像表示，可以有效地利用现有图像到图像学习模型进行动态地图状态预测

Abstract: When autonomous systems are deployed in real-world scenarios, sensors are often subject to limited field-of-view (FOV) constraints, either naturally through system design, or through unexpected occlusions or sensor failures. In conditions where a large FOV is unavailable, it is important to be able to infer information about the environment and predict the state of nearby surroundings based on available data to maintain safe and accurate operation. In this work, we explore the effectiveness of deep learning for dynamic map state prediction based on limited FOV time series data. We show that by representing dynamic sensor data in a simple single-image format that captures both spatial and temporal information, we can effectively use a wide variety of existing image-to-image learning models to predict map states with high accuracy in a diverse set of sensing scenarios.

</details>


### [6] [Zero-Shot Adaptation to Robot Structural Damage via Natural Language-Informed Kinodynamics Modeling](https://arxiv.org/abs/2602.12385)
*Anuj Pokhrel,Aniket Datar,Mohammad Nazeri,Francesco Cancelliere,Xuesu Xiao*

Main category: cs.RO

TL;DR: 提出ZLIK方法，利用自然语言描述结构损伤，通过自监督学习将损伤语义信息与运动动力学行为关联，实现零样本适应不同损伤类型的运动动力学建模


<details>
  <summary>Details</summary>
Motivation: 自主移动机器人在野外高速行驶或崎岖地形操作时会承受显著机械应力，导致不可避免的结构损伤。这些损伤会显著改变运动动力学行为，但由于损伤类型的异质性，量化各种损伤对运动动力学的影响具有挑战性

Method: 提出零样本语言信息运动动力学（ZLIK）方法，使用自监督学习将损伤描述的语义信息与运动动力学行为关联，以数据驱动方式学习前向运动动力学模型。使用BeamNG.tech高保真软体物理模拟器收集各种结构受损车辆的数据

Result: 学习模型实现了对不同损伤的零样本适应，运动动力学误差降低高达81%，并在模拟到现实以及全尺寸到1/10比例尺的泛化方面表现出色

Conclusion: 自然语言能够有效描述和捕捉结构损伤的多样性，ZLIK方法通过将损伤语义信息与运动动力学行为关联，实现了对异质结构损伤的零样本适应和跨域泛化

Abstract: High-performance autonomous mobile robots endure significant mechanical stress during in-the-wild operations, e.g., driving at high speeds or over rugged terrain. Although these platforms are engineered to withstand such conditions, mechanical degradation is inevitable. Structural damage manifests as consistent and notable changes in kinodynamic behavior compared to a healthy vehicle. Given the heterogeneous nature of structural failures, quantifying various damages to inform kinodynamics is challenging. We posit that natural language can describe and thus capture this variety of damages. Therefore, we propose Zero-shot Language Informed Kinodynamics (ZLIK), which employs self-supervised learning to ground semantic information of damage descriptions in kinodynamic behaviors to learn a forward kinodynamics model in a data-driven manner. Using the high-fidelity soft-body physics simulator BeamNG.tech, we collect data from a variety of structurally compromised vehicles. Our learned model achieves zero-shot adaptation to different damages with up to 81% reduction in kinodynamics error and generalizes across the sim-to-real and full-to-1/10$^{\text{th}}$ scale gaps.

</details>


### [7] [Self-Refining Vision Language Model for Robotic Failure Detection and Reasoning](https://arxiv.org/abs/2602.12405)
*Carl Qi,Xiaojie Wang,Silong Yong,Stephen Sheng,Huitan Mao,Sriram Srinivasan,Manikantan Nambi,Amy Zhang,Yesh Dattatreya*

Main category: cs.RO

TL;DR: ARMOR：一种用于机器人故障检测和推理的自适应多轮多任务模型，通过迭代自精炼过程处理稀疏二元标签和少量丰富推理标注，在开放世界故障场景中实现SOTA性能


<details>
  <summary>Details</summary>
Motivation: 现实世界中的机器人故障通常具有细微性、组合性和难以枚举的特点，而丰富的推理标注成本高昂。现有方法要么将故障推理视为封闭集分类问题，要么假设有充足的人工标注，无法有效应对开放世界的复杂故障场景。

Method: ARMOR将检测和推理构建为多任务自精炼过程，模型迭代预测检测结果和自然语言推理。训练时利用异构监督（大规模稀疏二元标签+小规模丰富推理标注），通过离线+在线模仿学习优化。推理时生成多个精炼轨迹，通过自确定性度量选择最置信的预测。

Result: 在多样化环境中，ARMOR相比先前方法提升高达30%的故障检测率，推理性能（通过LLM模糊匹配分数衡量）提升高达100%，展现出对异构监督和超越预定义故障模式的开放推理的鲁棒性。

Conclusion: ARMOR通过多任务自精炼框架有效解决了开放世界机器人故障推理问题，能够在有限标注下实现高性能的故障检测和推理，为构建可靠可信的机器人系统提供了新方法。

Abstract: Reasoning about failures is crucial for building reliable and trustworthy robotic systems. Prior approaches either treat failure reasoning as a closed-set classification problem or assume access to ample human annotations. Failures in the real world are typically subtle, combinatorial, and difficult to enumerate, whereas rich reasoning labels are expensive to acquire. We address this problem by introducing ARMOR: Adaptive Round-based Multi-task mOdel for Robotic failure detection and reasoning. We formulate detection and reasoning as a multi-task self-refinement process, where the model iteratively predicts detection outcomes and natural language reasoning conditioned on past outputs. During training, ARMOR learns from heterogeneous supervision - large-scale sparse binary labels and small-scale rich reasoning annotations - optimized via a combination of offline and online imitation learning. At inference time, ARMOR generates multiple refinement trajectories and selects the most confident prediction via a self-certainty metric. Experiments across diverse environments show that ARMOR achieves state-of-the-art performance by improving over the previous approaches by up to 30% on failure detection rate and up to 100% in reasoning measured through LLM fuzzy match score, demonstrating robustness to heterogeneous supervision and open-ended reasoning beyond predefined failure modes. We provide dditional visualizations on our website: https://sites.google.com/utexas.edu/armor

</details>


### [8] [MiDAS: A Multimodal Data Acquisition System and Dataset for Robot-Assisted Minimally Invasive Surgery](https://arxiv.org/abs/2602.12407)
*Keshara Weerasinghe,Seyed Hamid Reza Roodabeh,Andrew Hawkins,Zhaomeng Zhang,Zachary Schrader,Homa Alemzadeh*

Main category: cs.RO

TL;DR: MiDAS是一个开源、平台无关的系统，用于非侵入式、时间同步的多模态手术机器人数据采集，无需依赖专有机器人接口。


<details>
  <summary>Details</summary>
Motivation: 机器人辅助微创手术研究依赖多模态数据，但专有机器人遥测数据的获取存在障碍，需要平台无关的解决方案。

Method: MiDAS整合电磁和RGB-D手部追踪、脚踏板传感和手术视频捕捉，在Raven-II和da Vinci Xi平台上验证，收集了缝合任务的多模态数据集。

Result: 外部手部和脚部传感能近似内部机器人运动学，非侵入式运动信号在手势识别性能上与专有遥测数据相当。

Conclusion: MiDAS实现了可重复的多模态RMIS数据采集，发布了带标注的数据集，包括首个在高保真模拟模型上捕获疝气修复缝合的多模态数据集。

Abstract: Background: Robot-assisted minimally invasive surgery (RMIS) research increasingly relies on multimodal data, yet access to proprietary robot telemetry remains a major barrier. We introduce MiDAS, an open-source, platform-agnostic system enabling time-synchronized, non-invasive multimodal data acquisition across surgical robotic platforms.
  Methods: MiDAS integrates electromagnetic and RGB-D hand tracking, foot pedal sensing, and surgical video capturing without requiring proprietary robot interfaces. We validated MiDAS on the open-source Raven-II and the clinical da Vinci Xi by collecting multimodal datasets of peg transfer and hernia repair suturing tasks performed by surgical residents. Correlation analysis and downstream gesture recognition experiments were conducted.
  Results: External hand and foot sensing closely approximated internal robot kinematics and non-invasive motion signals achieved gesture recognition performance comparable to proprietary telemetry.
  Conclusion: MiDAS enables reproducible multimodal RMIS data collection and is released with annotated datasets, including the first multimodal dataset capturing hernia repair suturing on high-fidelity simulation models.

</details>


### [9] [Control Barrier Functions with Audio Risk Awareness for Robot Safe Navigation on Construction Sites](https://arxiv.org/abs/2602.12416)
*Johannes Mootz,Reza Akhavian*

Main category: cs.RO

TL;DR: 提出基于控制障碍函数的音频增强安全滤波器，利用音频检测风险信号动态调整安全边界，提高施工环境中自主机器人的避障安全性


<details>
  <summary>Details</summary>
Motivation: 施工环境动态且视觉遮挡严重，传统感知方法受限，音频信息在自主系统中未充分利用，需要更鲁棒的安全保障机制

Method: 基于控制障碍函数的安全滤波器，结合轻量级实时电镐检测器（基于信号包络和周期性），将音频风险作为外生信号直接调制障碍函数

Result: CBF安全滤波器在所有试验中消除安全违规，圆形CBF目标到达率40.2%，椭圆CBF目标到达率76.5%，椭圆公式能更好避免死锁

Conclusion: 音频感知与CBF控制器的集成为安全关键动态环境中的自主机器人提供了更丰富的多模态安全推理途径

Abstract: Construction automation increasingly requires autonomous mobile robots, yet robust autonomy remains challenging on construction sites. These environments are dynamic and often visually occluded, which complicates perception and navigation. In this context, valuable information from audio sources remains underutilized in most autonomy stacks. This work presents a control barrier function (CBF)-based safety filter that provides safety guarantees for obstacle avoidance while adapting safety margins during navigation using an audio-derived risk cue. The proposed framework augments the CBF with a lightweight, real-time jackhammer detector based on signal envelope and periodicity. Its output serves as an exogenous risk that is directly enforced in the controller by modulating the barrier function. The approach is evaluated in simulation with two CBF formulations (circular and goal-aligned elliptical) with a unicycle robot navigating a cluttered construction environment. Results show that the CBF safety filter eliminates safety violations across all trials while reaching the target in 40.2% (circular) vs. 76.5% (elliptical), as the elliptical formulation better avoids deadlock. This integration of audio perception into a CBF-based controller demonstrates a pathway toward richer multimodal safety reasoning in autonomous robots for safety-critical and dynamic environments.

</details>


### [10] [An Autonomous, End-to-End, Convex-Based Framework for Close-Range Rendezvous Trajectory Design and Guidance with Hardware Testbed Validation](https://arxiv.org/abs/2602.12421)
*Minduli C. Wijayatunga,Julian Guinane,Nathan D. Wallace,Xiaofeng Wu*

Main category: cs.RO

TL;DR: CORTEX是一个用于近距离交会对接的自主感知实时轨迹设计与制导框架，结合深度学习感知与凸优化轨迹设计，具备参考轨迹重规划和安全轨道中止逻辑，在软件仿真和硬件在环实验中验证了性能。


<details>
  <summary>Details</summary>
Motivation: 自主卫星服务任务需要在严格的安全和操作约束下执行近距离交会对接，同时保持计算可行性以适应机载使用，并能应对感知、执行和动力学中的不确定性。

Method: CORTEX框架整合了深度学习感知管道与基于凸优化的轨迹设计和制导，包括参考轨迹重生成和安全轨道中止逻辑，以从传感器故障和发动机故障导致的大偏差中恢复。

Result: 在高保真软件仿真中，蒙特卡洛测试在最严苛情况下实现终端对接误差：相对位置36.85±44.46mm，相对速度1.25±2.26mm/s；在平面气浮测试台上，18个案例（10个标称，8个非标称）实现终端误差：位置8.09±5.29mm，速度2.23±1.72mm/s。

Conclusion: CORTEX框架为自主卫星近距离交会对接提供了一个计算可行、鲁棒的实时轨迹设计与制导解决方案，通过软件仿真和硬件实验验证了其在各种故障情况下的性能。

Abstract: Autonomous satellite servicing missions must execute close-range rendezvous under stringent safety and operational constraints while remaining computationally tractable for onboard use and robust to uncertainty in sensing, actuation, and dynamics. This paper presents CORTEX (Convex Optimization for Rendezvous Trajectory Execution), an autonomous, perception-enabled, real-time trajectory design and guidance framework for close-range rendezvous. CORTEX integrates a deep-learning perception pipeline with convex-optimisation-based trajectory design and guidance, including reference regeneration and abort-to-safe-orbit logic to recover from large deviations caused by sensor faults and engine failures.
  CORTEX is validated in high-fidelity software simulation and hardware-in-the-loop experiments. The software pipeline (Basilisk) models high-fidelity relative dynamics, realistic thruster execution, perception, and attitude control. Hardware testing uses (i) an optical navigation testbed to assess perception-to-estimation performance and (ii) a planar air-bearing testbed to evaluate the end-to-end guidance loop under representative actuation and subsystem effects. A Monte-Carlo campaign in simulation includes initial-state uncertainty, thrust-magnitude errors, and missed-thrust events; under the strongest case investigated, CORTEX achieves terminal docking errors of $36.85 \pm 44.46$ mm in relative position and $1.25 \pm 2.26$ mm/s in relative velocity. On the planar air-bearing testbed, 18 cases are executed (10 nominal; 8 off-nominal requiring recomputation and/or abort due to simulated engine failure and sensor malfunctions), yielding terminal errors of $8.09 \pm 5.29$ mm in position and $2.23 \pm 1.72$ mm/s in velocity.

</details>


### [11] [Gradient-Enhanced Partitioned Gaussian Processes for Real-Time Quadrotor Dynamics Modeling](https://arxiv.org/abs/2602.12487)
*Xinhuan Sang,Adam Rozman,Sheryl Grace,Roberto Tron*

Main category: cs.RO

TL;DR: 提出了一种基于状态空间分区和近似的四旋翼动力学高斯过程模型，通过梯度信息和分区策略实现实时推理，利用中保真度势流模拟数据捕捉空气动力学效应。


<details>
  <summary>Details</summary>
Motivation: 传统基于高斯过程的方法虽然能提供可靠的贝叶斯预测和不确定性量化，但计算成本高，不适合实时仿真。需要一种既能保持准确性又能实现实时推理的方法来处理四旋翼在复杂非定常环境中的空气动力学预测和控制。

Method: 1) 集成梯度信息提高准确性；2) 引入新颖的分区和近似策略降低在线计算成本，为每个非重叠区域关联局部高斯过程；3) 通过将训练数据分为局部近场和远场子集，利用Schur补实现大部分矩阵求逆离线计算；4) 使用CHARM中保真度空气动力学求解器生成包含转子-转子相互作用和表观风向等空气动力学效应的训练数据集；5) 通过有限差分获取导数信息。

Result: 提出的带梯度条件的分区高斯过程比不带梯度信息的标准分区高斯过程具有更高的准确性，同时大幅减少计算时间，在标准桌面硬件上实现超过30Hz的实时推理频率，能够预测SUI Endurance四旋翼的力和扭矩以及三个指定位置的噪声。

Conclusion: 该框架为复杂非定常环境中的实时空气动力学预测和控制算法提供了高效基础，通过梯度信息和分区策略成功解决了传统高斯过程方法计算成本高的问题，实现了准确性和实时性的平衡。

Abstract: We present a quadrotor dynamics Gaussian Process (GP) with gradient information that achieves real-time inference via state-space partitioning and approximation, and that includes aerodynamic effects using data from mid-fidelity potential flow simulations. While traditional GP-based approaches provide reliable Bayesian predictions with uncertainty quantification, they are computationally expensive and thus unsuitable for real-time simulations. To address this challenge, we integrate gradient information to improve accuracy and introduce a novel partitioning and approximation strategy to reduce online computational cost. In particular, for the latter, we associate a local GP with each non-overlapping region; by splitting the training data into local near and far subsets, and by using Schur complements, we show that a large part of the matrix inversions required for inference can be performed offline, enabling real-time inference at frequencies above 30 Hz on standard desktop hardware. To generate a training dataset that captures aerodynamic effects, such as rotor-rotor interactions and apparent wind direction, we use the CHARM code, which is a mid-fidelity aerodynamic solver. It is applied to the SUI Endurance quadrotor to predict force and torque, along with noise at three specified locations. The derivative information is obtained via finite differences. Experimental results demonstrate that the proposed partitioned GP with gradient conditioning achieves higher accuracy than standard partitioned GPs without gradient information, while greatly reducing computational time. This framework provides an efficient foundation for real-time aerodynamic prediction and control algorithms in complex and unsteady environments.

</details>


### [12] [Composable Model-Free RL for Navigation with Input-Affine Systems](https://arxiv.org/abs/2602.12492)
*Xinhuan Sang,Abdelrahman Abdelgawad,Roberto Tron*

Main category: cs.RO

TL;DR: 提出一种可组合的无模型强化学习方法，通过为每个环境元素学习价值函数和最优策略，在线组合实现目标到达和避障，提供形式化安全保证。


<details>
  <summary>Details</summary>
Motivation: 自主机器人在复杂动态环境中需要实时安全导航，但预测所有可能行为不可行，需要一种能够提供形式化安全保证的无模型方法。

Method: 基于连续时间HJB方程推导价值函数，利用优势函数的二次结构，提出无模型actor-critic算法学习静态/动态障碍物的策略和价值函数，通过QCQP组合多个到达/避障模型。

Result: 该方法在仿真中相比应用于离散时间近似的PPO基线表现出更好的性能，能够提供基于价值函数水平集的形式化避障保证。

Conclusion: 提出的可组合无模型强化学习方法为自主机器人在复杂动态环境中的安全导航提供了有效的解决方案，是CLF/CBF控制器的无模型替代方案。

Abstract: As autonomous robots move into complex, dynamic real-world environments, they must learn to navigate safely in real time, yet anticipating all possible behaviors is infeasible. We propose a composable, model-free reinforcement learning method that learns a value function and an optimal policy for each individual environment element (e.g., goal or obstacle) and composes them online to achieve goal reaching and collision avoidance. Assuming unknown nonlinear dynamics that evolve in continuous time and are input-affine, we derive a continuous-time Hamilton-Jacobi-Bellman (HJB) equation for the value function and show that the corresponding advantage function is quadratic in the action and optimal policy. Based on this structure, we introduce a model-free actor-critic algorithm that learns policies and value functions for static or moving obstacles using gradient descent. We then compose multiple reach/avoid models via a quadratically constrained quadratic program (QCQP), yielding formal obstacle-avoidance guarantees in terms of value-function level sets, providing a model-free alternative to CLF/CBF-based controllers. Simulations demonstrate improved performance over a PPO baseline applied to a discrete-time approximation.

</details>


### [13] [Monocular Reconstruction of Neural Tactile Fields](https://arxiv.org/abs/2602.12508)
*Pavan Mantripragada,Siddhanth Deshmukh,Eadom Dessalene,Manas Desai,Yiannis Aloimonos*

Main category: cs.RO

TL;DR: 提出神经触觉场，从单目RGB图像预测接触时的触觉响应，帮助机器人规划路径时区分不同阻力的物体


<details>
  <summary>Details</summary>
Motivation: 现实世界中的机器人需要在可变形、可屈服和可重构的环境中规划路径，需要超越静态几何占用的交互感知3D表示

Method: 引入神经触觉场，从单目RGB图像预测空间位置在接触时的预期触觉响应，并与现成的路径规划器集成

Result: 相比最先进的单目3D重建方法（LRM和Direct3D），体积3D重建提升85.8%，表面重建提升26.7%

Conclusion: 神经触觉场使机器人能够生成避免高阻力物体并有意通过低阻力区域的路径，而不是将所有占用空间视为同样不可通行

Abstract: Robots operating in the real world must plan through environments that deform, yield, and reconfigure under contact, requiring interaction-aware 3D representations that extend beyond static geometric occupancy. To address this, we introduce neural tactile fields, a novel 3D representation that maps spatial locations to the expected tactile response upon contact. Our model predicts these neural tactile fields from a single monocular RGB image -- the first method to do so. When integrated with off-the-shelf path planners, neural tactile fields enable robots to generate paths that avoid high-resistance objects while deliberately routing through low-resistance regions (e.g. foliage), rather than treating all occupied space as equally impassable. Empirically, our learning framework improves volumetric 3D reconstruction by $85.8\%$ and surface reconstruction by $26.7\%$ compared to state-of-the-art monocular 3D reconstruction methods (LRM and Direct3D).

</details>


### [14] [CRAFT: Adapting VLA Models to Contact-rich Manipulation via Force-aware Curriculum Fine-tuning](https://arxiv.org/abs/2602.12532)
*Yike Zhang,Yaonan Wang,Xinxin Sun,Kaizhen Huang,Zhiyuan Xu,Junjie Ji,Zhengping Che,Jian Tang,Jingtao Sun*

Main category: cs.RO

TL;DR: CRAFT框架通过力感知课程微调和变分信息瓶颈模块，解决VLA模型在接触丰富操作任务中的性能瓶颈，提升机器人对力信号的敏感度和控制稳定性。


<details>
  <summary>Details</summary>
Motivation: 当前Vision-Language-Action模型在执行一般指令方面表现出色，但在接触丰富的操作任务中存在明显不足。这些任务需要精确对齐、稳定接触维持和有效处理可变形物体。核心挑战在于高熵视觉语言输入与低熵但关键的力信号之间的不平衡，导致模型过度依赖感知而控制不稳定。

Method: 提出CRAFT框架：1）采用力感知课程微调策略，通过变分信息瓶颈模块在早期训练中调节视觉和语言嵌入；2）设计同源领导者-跟随者遥操作系统，收集跨多种接触丰富任务的同步视觉、语言和力数据；3）课程策略让模型先优先处理力信号，再逐步恢复对完整多模态信息的访问。

Result: 真实世界实验表明，CRAFT能持续提高任务成功率，泛化到未见过的物体和新任务变体，并有效适应不同的VLA架构，实现鲁棒且可泛化的接触丰富操作。

Conclusion: CRAFT框架通过力感知课程学习和变分信息瓶颈，成功解决了VLA模型在接触丰富操作中的关键挑战，为机器人执行复杂物理交互任务提供了有效的解决方案。

Abstract: Vision-Language-Action (VLA) models have shown a strong capability in enabling robots to execute general instructions, yet they struggle with contact-rich manipulation tasks, where success requires precise alignment, stable contact maintenance, and effective handling of deformable objects. A fundamental challenge arises from the imbalance between high-entropy vision and language inputs and low-entropy but critical force signals, which often leads to over-reliance on perception and unstable control. To address this, we introduce CRAFT, a force-aware curriculum fine-tuning framework that integrates a variational information bottleneck module to regulate vision and language embeddings during early training. This curriculum strategy encourages the model to prioritize force signals initially, before progressively restoring access to the full multimodal information. To enable force-aware learning, we further design a homologous leader-follower teleoperation system that collects synchronized vision, language, and force data across diverse contact-rich tasks. Real-world experiments demonstrate that CRAFT consistently improves task success, generalizes to unseen objects and novel task variations, and adapts effectively across diverse VLA architectures, enabling robust and generalizable contact-rich manipulation.

</details>


### [15] [Eva-Tracker: ESDF-update-free, Visibility-aware Planning with Target Reacquisition for Robust Aerial Tracking](https://arxiv.org/abs/2602.12549)
*Yue Lin,Yang Liu,Dong Wang,Huchuan Lu*

Main category: cs.RO

TL;DR: Eva-Tracker：一种用于空中跟踪的可见性感知轨迹规划框架，通过预计算的FoV-ESDF消除ESDF更新开销，实现高效的目标重获和连续可见性跟踪。


<details>
  <summary>Details</summary>
Motivation: 传统的欧几里得符号距离场（ESDF）在可见性评估中广泛使用，但频繁的ESDF更新带来了巨大的计算开销，影响跟踪系统的实时性能。

Method: 1. 设计目标轨迹预测方法和可见性感知初始路径生成算法，保持适当观测距离、避免遮挡、支持快速重规划；2. 提出FoV-ESDF（视场ESDF），预计算针对跟踪器视场的ESDF，无需更新即可快速评估可见性；3. 使用可微分的FoV-ESDF目标函数优化轨迹，确保整个跟踪过程中的连续可见性。

Result: 大量仿真和真实世界实验表明，该方法相比现有最先进方法，能以更低的计算代价提供更鲁棒的跟踪结果。

Conclusion: Eva-Tracker通过消除ESDF更新需求并引入恢复能力强的路径生成方法，实现了高效、鲁棒的空中目标跟踪，显著降低了计算开销。

Abstract: The Euclidean Signed Distance Field (ESDF) is widely used in visibility evaluation to prevent occlusions and collisions during tracking. However, frequent ESDF updates introduce considerable computational overhead. To address this issue, we propose Eva-Tracker, a visibility-aware trajectory planning framework for aerial tracking that eliminates ESDF updates and incorporates a recovery-capable path generation method for target reacquisition. First, we design a target trajectory prediction method and a visibility-aware initial path generation algorithm that maintain an appropriate observation distance, avoid occlusions, and enable rapid replanning to reacquire the target when it is lost. Then, we propose the Field of View ESDF (FoV-ESDF), a precomputed ESDF tailored to the tracker's field of view, enabling rapid visibility evaluation without requiring updates. Finally, we optimize the trajectory using differentiable FoV-ESDF-based objectives to ensure continuous visibility throughout the tracking process. Extensive simulations and real-world experiments demonstrate that our approach delivers more robust tracking results with lower computational effort than existing state-of-the-art methods. The source code is available at https://github.com/Yue-0/Eva-Tracker.

</details>


### [16] [Hemispherical Angular Power Mapping of Installed mmWave Radar Modules Under Realistic Deployment Constraints](https://arxiv.org/abs/2602.12584)
*Maaz Qureshi,Mohammad Omid Bagheri,William Melek,George Shaker*

Main category: cs.RO

TL;DR: 提出了一种用于现场验证已安装毫米波模块的半球形角度接收功率映射方法，可在实际部署约束下进行电磁验证


<details>
  <summary>Details</summary>
Motivation: 毫米波雷达模块在实际传感平台中，封装、安装硬件和附近结构会显著改变有效发射模式，而传统基于暗室和转台的测量方法在设备嵌入主机环境后往往不实用

Method: 采用半球形角度接收功率映射方法，通过几何一致定位和准静态采集，将校准接收探头放置在规定的(phi, theta, r)位置，使用标准射频仪器记录幅度接收功率，生成捕获安装相关辐射特性的半球形角度功率图

Result: 在60GHz雷达模块上的概念验证测量展示了可重复的半球形映射，其角度趋势与全波仿真结果良好一致

Conclusion: 该方法支持对嵌入式毫米波发射器的实用现场表征，为实际部署约束下的安装毫米波模块提供了有效的电磁验证手段

Abstract: Characterizing the angular radiation behavior of installed millimeter-wave (mmWave) radar modules is increasingly important in practical sensing platforms, where packaging, mounting hardware, and nearby structures can significantly alter the effective emission profile. However, once a device is embedded in its host environment, conventional chamber- and turntable-based antenna measurements are often impractical. This paper presents a hemispherical angular received-power mapping methodology for in-situ EM validation of installed mmWave modules under realistic deployment constraints. The approach samples the accessible half-space around a stationary device-under-test by placing a calibrated receiving probe at prescribed (phi, theta, r) locations using geometry-consistent positioning and quasi-static acquisition. Amplitude-only received-power is recorded using standard RF instrumentation to generate hemispherical angular power maps that capture installation-dependent radiation characteristics. Proof-of-concept measurements on a 60-GHz radar module demonstrate repeatable hemi-spherical mapping with angular trends in good agreement with full-wave simulation, supporting practical on-site characterization of embedded mmWave transmitters.

</details>


### [17] [PISHYAR: A Socially Intelligent Smart Cane for Indoor Social Navigation and Multimodal Human-Robot Interaction for Visually Impaired People](https://arxiv.org/abs/2602.12597)
*Mahdi Haghighat Joo,Maryam Karimi Jafari,Alireza Taheri*

Main category: cs.RO

TL;DR: PISHYAR是一款结合社交感知导航与多模态人机交互的智能手杖，通过计算机视觉、路径规划和AI对话系统为视障用户提供物理移动和交互辅助。


<details>
  <summary>Details</summary>
Motivation: 为视障和低视力用户开发超越传统导航功能的智能辅助设备，不仅提供物理移动支持，还能进行社交感知导航和自然交互，提升用户的社会参与度和生活质量。

Method: 系统包含两个核心组件：1) 社交导航框架（Raspberry Pi 5 + OAK-D Lite相机 + YOLOv8目标检测 + COMPOSER群体活动识别 + D* Lite动态路径规划 + 触觉反馈）；2) 多模态LLM-VLM交互框架（语音识别+视觉语言模型+大语言模型+TTS，支持语音和视觉模式动态切换）。

Result: 在模拟和真实室内环境中，系统实现了约80%的整体准确率，可靠避障和社交合规导航；群体活动识别在不同人群场景中表现稳健；8名视障用户的初步研究显示高接受度、良好可用性、信任度和社交感知。

Conclusion: PISHYAR展示了作为多模态辅助移动设备的潜力，能够超越传统导航功能，为视障用户提供社交互动支持，是物理移动和交互辅助的有效结合。

Abstract: This paper presents PISHYAR, a socially intelligent smart cane designed by our group to combine socially aware navigation with multimodal human-AI interaction to support both physical mobility and interactive assistance. The system consists of two components: (1) a social navigation framework implemented on a Raspberry Pi 5 that integrates real-time RGB-D perception using an OAK-D Lite camera, YOLOv8-based object detection, COMPOSER-based collective activity recognition, D* Lite dynamic path planning, and haptic feedback via vibration motors for tasks such as locating a vacant seat; and (2) an agentic multimodal LLM-VLM interaction framework that integrates speech recognition, vision language models, large language models, and text-to-speech, with dynamic routing between voice-only and vision-only modes to enable natural voice-based communication, scene description, and object localization from visual input. The system is evaluated through a combination of simulation-based tests, real-world field experiments, and user-centered studies. Results from simulated and real indoor environments demonstrate reliable obstacle avoidance and socially compliant navigation, achieving an overall system accuracy of approximately 80% under different social conditions. Group activity recognition further shows robust performance across diverse crowd scenarios. In addition, a preliminary exploratory user study with eight visually impaired and low-vision participants evaluates the agentic interaction framework through structured tasks and a UTAUT-based questionnaire reveals high acceptance and positive perceptions of usability, trust, and perceived sociability during our experiments. The results highlight the potential of PISHYAR as a multimodal assistive mobility aid that extends beyond navigation to provide socially interactive support for such users.

</details>


### [18] [RLinf-Co: Reinforcement Learning-Based Sim-Real Co-Training for VLA Models](https://arxiv.org/abs/2602.12628)
*Liangzhi Shi,Shuaihang Chen,Feng Gao,Yinuo Chen,Kang Chen,Tonghe Zhang,Hongzhi Zhang,Weinan Zhang,Chao Yu,Yu Wang*

Main category: cs.RO

TL;DR: 提出RL-Co框架，通过强化学习在模拟环境中进行闭环交互训练，同时用真实数据监督损失防止灾难性遗忘，显著提升真实世界机器人操作性能和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有基于监督微调的模拟-真实协同训练方法将模拟视为静态演示源，无法利用大规模闭环交互，导致真实世界性能提升和泛化能力有限。

Method: 提出两阶段RL-Co框架：1) 用真实和模拟演示混合数据进行监督微调预热策略；2) 在模拟环境中进行强化学习微调，同时添加真实数据的辅助监督损失来锚定策略并防止灾难性遗忘。

Result: 在四个真实世界桌面操作任务上评估，使用OpenVLA和π₀.₅两种VLA架构，相比仅用真实数据微调和基于SFT的协同训练，OpenVLA提升24%真实世界成功率，π₀.₅提升20%。

Conclusion: RL协同训练不仅提高成功率，还带来更强的未见任务变体泛化能力和显著改善的真实世界数据效率，为利用模拟增强真实机器人部署提供了实用且可扩展的途径。

Abstract: Simulation offers a scalable and low-cost way to enrich vision-language-action (VLA) training, reducing reliance on expensive real-robot demonstrations. However, most sim-real co-training methods rely on supervised fine-tuning (SFT), which treats simulation as a static source of demonstrations and does not exploit large-scale closed-loop interaction. Consequently, real-world gains and generalization are often limited. In this paper, we propose an \underline{\textit{RL}}-based sim-real \underline{\textit{Co}}-training \modify{(RL-Co)} framework that leverages interactive simulation while preserving real-world capabilities. Our method follows a generic two-stage design: we first warm-start the policy with SFT on a mixture of real and simulated demonstrations, then fine-tune it with reinforcement learning in simulation while adding an auxiliary supervised loss on real-world data to anchor the policy and mitigate catastrophic forgetting. We evaluate our framework on four real-world tabletop manipulation tasks using two representative VLA architectures, OpenVLA and $π_{0.5}$, and observe consistent improvements over real-only fine-tuning and SFT-based co-training, including +24% real-world success on OpenVLA and +20% on $π_{0.5}$. Beyond higher success rates, RL co-training yields stronger generalization to unseen task variations and substantially improved real-world data efficiency, providing a practical and scalable pathway for leveraging simulation to enhance real-robot deployment.

</details>


### [19] [Real-to-Sim for Highly Cluttered Environments via Physics-Consistent Inter-Object Reasoning](https://arxiv.org/abs/2602.12633)
*Tianyi Xiang,Jiahang Cao,Sikai Guo,Guoyang Zhao,Andrew F. Luo,Jun Ma*

Main category: cs.RO

TL;DR: 提出了一种物理约束的Real-to-Sim流水线，从单视角RGB-D数据重建物理一致的3D场景，通过可微分优化和接触图建模空间依赖关系，实现高物理保真度的接触动力学重建。


<details>
  <summary>Details</summary>
Motivation: 从单视角观测重建物理有效的3D场景是连接视觉感知与机器人控制的关键。在需要精确接触推理的场景中（如高度杂乱环境中的机器人操作），仅几何保真度不足，标准感知流水线常忽略物理约束，导致无效状态（如漂浮物体或严重穿透），使下游仿真不可靠。

Method: 提出物理约束的Real-to-Sim流水线，核心是可微分优化流水线，通过接触图显式建模空间依赖关系，联合优化物体姿态和物理属性，使用可微分刚体仿真进行细化。

Result: 在仿真和真实环境中的广泛评估表明，重建场景实现了高物理保真度，能忠实复制真实世界的接触动力学，支持稳定可靠的接触丰富操作。

Conclusion: 该方法成功解决了单视角重建中的物理一致性问题，通过显式建模接触关系和可微分优化，实现了可靠的物理场景重建，为接触丰富的机器人操作提供了可靠基础。

Abstract: Reconstructing physically valid 3D scenes from single-view observations is a prerequisite for bridging the gap between visual perception and robotic control. However, in scenarios requiring precise contact reasoning, such as robotic manipulation in highly cluttered environments, geometric fidelity alone is insufficient. Standard perception pipelines often neglect physical constraints, resulting in invalid states, e.g., floating objects or severe inter-penetration, rendering downstream simulation unreliable. To address these limitations, we propose a novel physics-constrained Real-to-Sim pipeline that reconstructs physically consistent 3D scenes from single-view RGB-D data. Central to our approach is a differentiable optimization pipeline that explicitly models spatial dependencies via a contact graph, jointly refining object poses and physical properties through differentiable rigid-body simulation. Extensive evaluations in both simulation and real-world settings demonstrate that our reconstructed scenes achieve high physical fidelity and faithfully replicate real-world contact dynamics, enabling stable and reliable contact-rich manipulation.

</details>


### [20] [PMG: Parameterized Motion Generator for Human-like Locomotion Control](https://arxiv.org/abs/2602.12656)
*Chenxi Han,Yuheng Min,Zihao Huang,Ao Hong,Hang Liu,Yi Cheng,Houde Liu*

Main category: cs.RO

TL;DR: PMG（参数化运动生成器）通过分析人体运动结构，使用紧凑的参数化运动数据和高维控制命令实时生成参考轨迹，结合模仿学习和优化式sim-to-real模块，在ZERITH Z1人形机器人上实现了自然、类人的运动控制。


<details>
  <summary>Details</summary>
Motivation: 当前基于数据驱动的强化学习和运动跟踪方法在人形机器人运动控制方面已有进展，但仍存在关键挑战：低级运动跟踪和轨迹跟随控制器成熟，但全身参考引导方法难以适应高级命令接口和多样化任务场景，需要大量高质量数据集，在不同速度和姿态下表现脆弱，且对机器人特定校准敏感。

Method: 提出参数化运动生成器（PMG），基于人体运动结构分析，仅使用紧凑的参数化运动数据和高维控制命令实时合成参考轨迹。结合模仿学习流程和基于优化的sim-to-real电机参数识别模块，形成完整系统。

Result: 在ZERITH Z1人形机器人原型上验证了完整方法，PMG在单一集成系统中产生自然的类人运动，精确响应高维控制输入（包括基于VR的遥操作），并实现高效、可验证的sim-to-real迁移。

Conclusion: 这些结果为自然且可部署的人形机器人控制建立了实用、经过实验验证的途径，解决了现有方法在数据集需求、鲁棒性和校准敏感性方面的局限性。

Abstract: Recent advances in data-driven reinforcement learning and motion tracking have substantially improved humanoid locomotion, yet critical practical challenges remain. In particular, while low-level motion tracking and trajectory-following controllers are mature, whole-body reference-guided methods are difficult to adapt to higher-level command interfaces and diverse task contexts: they require large, high-quality datasets, are brittle across speed and pose regimes, and are sensitive to robot-specific calibration. To address these limitations, we propose the Parameterized Motion Generator (PMG), a real-time motion generator grounded in an analysis of human motion structure that synthesizes reference trajectories using only a compact set of parameterized motion data together with High-dimensional control commands. Combined with an imitation-learning pipeline and an optimization-based sim-to-real motor parameter identification module, we validate the complete approach on our humanoid prototype ZERITH Z1 and show that, within a single integrated system, PMG produces natural, human-like locomotion, responds precisely to high-dimensional control inputs-including VR-based teleoperation-and enables efficient, verifiable sim-to-real transfer. Together, these results establish a practical, experimentally validated pathway toward natural and deployable humanoid control.

</details>


### [21] [Xiaomi-Robotics-0: An Open-Sourced Vision-Language-Action Model with Real-Time Execution](https://arxiv.org/abs/2602.12684)
*Rui Cai,Jun Guo,Xinze He,Piaopiao Jin,Jie Li,Bingxuan Lin,Futeng Liu,Wei Liu,Fei Ma,Kun Ma,Feng Qiu,Heng Qu,Yifei Su,Qiao Sun,Dong Wang,Donghao Wang,Yunhong Wang,Rujie Wu,Diyun Xiang,Yu Yang,Hangjun Ye,Yuan Zhang,Quanyun Zhou*

Main category: cs.RO

TL;DR: Xiaomi-Robotics-0是一个高性能的视觉-语言-动作模型，通过精心设计的训练方案和部署策略，实现了快速流畅的实时机器人控制，在仿真基准和真实机器人任务中均取得优异表现。


<details>
  <summary>Details</summary>
Motivation: 为了解决现有VLA模型在实时机器人部署中的推理延迟问题，同时保持视觉语义知识不丢失，需要开发一个既能高效执行又能保持高性能的机器人控制模型。

Method: 采用两阶段训练：首先在大规模跨具身机器人轨迹和视觉语言数据上进行预训练，获得通用的动作生成能力；然后通过异步执行训练技术解决推理延迟问题；部署时对齐连续预测动作块的时间步以确保实时流畅执行。

Result: 在仿真基准测试中达到最先进性能；在需要精确灵巧双手操作的真实机器人任务中，使用消费级GPU即可实现快速流畅的执行，获得高成功率和吞吐量。

Conclusion: Xiaomi-Robotics-0通过创新的训练和部署策略，成功实现了高性能、低延迟的实时机器人控制，为未来研究提供了有价值的开源模型和代码。

Abstract: In this report, we introduce Xiaomi-Robotics-0, an advanced vision-language-action (VLA) model optimized for high performance and fast and smooth real-time execution. The key to our method lies in a carefully designed training recipe and deployment strategy. Xiaomi-Robotics-0 is first pre-trained on large-scale cross-embodiment robot trajectories and vision-language data, endowing it with broad and generalizable action-generation capabilities while avoiding catastrophic forgetting of the visual-semantic knowledge of the underlying pre-trained VLM. During post-training, we propose several techniques for training the VLA model for asynchronous execution to address the inference latency during real-robot rollouts. During deployment, we carefully align the timesteps of consecutive predicted action chunks to ensure continuous and seamless real-time rollouts. We evaluate Xiaomi-Robotics-0 extensively in simulation benchmarks and on two challenging real-robot tasks that require precise and dexterous bimanual manipulation. Results show that our method achieves state-of-the-art performance across all simulation benchmarks. Moreover, Xiaomi-Robotics-0 can roll out fast and smoothly on real robots using a consumer-grade GPU, achieving high success rates and throughput on both real-robot tasks. To facilitate future research, code and model checkpoints are open-sourced at https://xiaomi-robotics-0.github.io

</details>


### [22] [SignScene: Visual Sign Grounding for Mapless Navigation](https://arxiv.org/abs/2602.12686)
*Nicky Zimmerman,Joel Loo,Benjamin Koh,Zishuo Wang,David Hsu*

Main category: cs.RO

TL;DR: 机器人利用现实世界中的导航标志进行无地图导航，通过SignScene空间语义表示方法将标志语义指令映射到3D场景元素和导航动作，在Spot机器人上实现实际导航


<details>
  <summary>Details</summary>
Motivation: 人类可以利用导航标志在陌生环境中导航而无需地图，但机器人面临标志解释的挑战：现实世界标志多样复杂，其抽象语义内容需要与局部3D场景建立联系

Method: 提出SignScene方法，这是一种以标志为中心的空间语义表示，捕捉导航相关的场景元素和标志信息，并以适合视觉语言模型有效推理的形式呈现给模型

Result: 在9种不同环境类型收集的114个查询数据集上评估，实现了88%的接地准确率，显著优于基线方法，并在Spot机器人上实现了实际的无地图导航

Conclusion: SignScene方法成功解决了标志接地问题，使机器人能够像人类一样利用现实世界中的导航标志进行无地图导航，展示了视觉语言模型在机器人导航中的实际应用价值

Abstract: Navigational signs enable humans to navigate unfamiliar environments without maps. This work studies how robots can similarly exploit signs for mapless navigation in the open world. A central challenge lies in interpreting signs: real-world signs are diverse and complex, and their abstract semantic contents need to be grounded in the local 3D scene. We formalize this as sign grounding, the problem of mapping semantic instructions on signs to corresponding scene elements and navigational actions. Recent Vision-Language Models (VLMs) offer the semantic common-sense and reasoning capabilities required for this task, but are sensitive to how spatial information is represented. We propose SignScene, a sign-centric spatial-semantic representation that captures navigation-relevant scene elements and sign information, and presents them to VLMs in a form conducive to effective reasoning. We evaluate our grounding approach on a dataset of 114 queries collected across nine diverse environment types, achieving 88% grounding accuracy and significantly outperforming baselines. Finally, we demonstrate that it enables real-world mapless navigation on a Spot robot using only signs.

</details>


### [23] [ALOE: Action-Level Off-Policy Evaluation for Vision-Language-Action Model Post-Training](https://arxiv.org/abs/2602.12691)
*Rushuai Yang,Hecheng Wang,Chiming Liu,Xiaohan Yan,Yunlong Wang,Xuan Du,Shuoyu Yue,Yongcheng Liu,Chuheng Zhang,Lizhe Qi,Yi Chen,Wei Shan,Maoqing Yao*

Main category: cs.RO

TL;DR: ALOE框架通过动作级别的离策略评估，使用基于分块的时序差分自举来评估单个动作序列而非最终任务结果，改善稀疏奖励下的信用分配，提升VLA系统的在线强化学习效果。


<details>
  <summary>Details</summary>
Motivation: 现有VLA系统在在线强化学习中通常采用保守的同策略估计以保证稳定性，但这种方法避免直接评估当前高容量策略，限制了学习效果。需要解决从混合数据中准确评估当前行为质量的离策略评估问题。

Method: 提出ALOE框架，采用基于分块的时序差分自举方法，评估单个动作序列而非预测最终任务结果。这种设计改善了稀疏奖励下对关键动作块的信用分配，支持稳定的策略改进。

Result: 在三个真实世界操作任务中评估：智能手机包装（高精度任务）、衣物折叠（长时程可变形物体任务）、双手抓取放置（多物体感知任务）。在所有任务中，ALOE提高了学习效率而不影响执行速度。

Conclusion: ALOE表明离策略强化学习可以以可靠的方式重新引入真实世界VLA后训练中，提高了学习效率并保持了执行速度。

Abstract: We study how to improve large foundation vision-language-action (VLA) systems through online reinforcement learning (RL) in real-world settings. Central to this process is the value function, which provides learning signals to guide VLA learning from experience. In practice, the value function is estimated from trajectory fragments collected from different data sources, including historical policies and intermittent human interventions. Estimating the value function of current behavior quality from the mixture data is inherently an off-policy evaluation problem. However, prior work often adopts conservative on-policy estimation for stability, which avoids direct evaluation of the current high-capacity policy and limits learning effectiveness. In this paper, we propose ALOE, an action-level off-policy evaluation framework for VLA post-training. ALOE applies chunking-based temporal-difference bootstrapping to evaluate individual action sequences instead of predicting final task outcomes. This design improves effective credit assignment to critical action chunks under sparse rewards and supports stable policy improvement. We evaluate our method on three real-world manipulation tasks, including smartphone packing as a high-precision task, laundry folding as a long-horizon deformable-object task, and bimanual pick-and-place involving multi-object perception. Across all tasks, ALOE improves learning efficiency without compromising execution speed, showing that off-policy RL can be reintroduced in a reliable manner for real-world VLA post-training. Videos and additional materials are available at our project website.

</details>


### [24] [Constrained PSO Six-Parameter Fuzzy PID Tuning Method for Balanced Optimization of Depth Tracking Performance in Underwater Vehicles](https://arxiv.org/abs/2602.12700)
*Yanxi Ding,Tingyue Jia*

Main category: cs.RO

TL;DR: 提出一种基于约束粒子群优化(PSO)的六参数模糊PID控制器调参方法，用于水下航行器深度控制，在满足执行器约束的同时显著提升跟踪性能。


<details>
  <summary>Details</summary>
Motivation: 水下航行器深度控制需要同时满足快速跟踪、低超调和执行器约束的要求。传统模糊PID调参依赖经验方法，难以在性能提升和控制成本之间找到稳定可复现的平衡解。

Method: 采用约束粒子群优化方法对六参数模糊PID控制器进行调参：调整基准PID参数以及模糊控制器的输入量化因子和输出比例增益，实现模糊PID系统整体调参强度和动态响应特性的协同优化。引入时间加权绝对误差积分、调整时间、相对超调、控制能量和饱和占用率等指标，构建约束驱动的综合评价体系。

Result: 在保持控制能量和饱和水平一致的情况下，该方法显著提升了深度跟踪性能：时间加权绝对误差积分从0.2631降至0.1473，调整时间从2.301秒缩短至1.613秒，相对超调从0.1494降至0.01839。控制能量从7980变化至7935，满足能量约束，饱和占用率从0.004降至0.003。

Conclusion: 提出的约束六参数联合调参策略在水下航行器深度控制场景中具有有效性和工程意义，能够实现性能提升与控制约束的良好平衡。

Abstract: Depth control of underwater vehicles in engineering applications must simultaneously satisfy requirements for rapid tracking, low overshoot, and actuator constraints. Traditional fuzzy PID tuning often relies on empirical methods, making it difficult to achieve a stable and reproducible equilibrium solution between performance enhancement and control cost. This paper proposes a constrained particle swarm optimization (PSO) method for tuning six-parameter fuzzy PID controllers. By adjusting the benchmark PID parameters alongside the fuzzy controller's input quantization factor and output proportional gain, it achieves synergistic optimization of the overall tuning strength and dynamic response characteristics of the fuzzy PID system. To ensure engineering feasibility of the optimization results, a time-weighted absolute error integral, adjustment time, relative overshoot control energy, and saturation occupancy rate are introduced. Control energy constraints are applied to construct a constraint-driven comprehensive evaluation system, suppressing pseudo-improvements achieved solely by increasing control inputs. Simulation results demonstrate that, while maintaining consistent control energy and saturation levels, the proposed method significantly enhances deep tracking performance: the time-weighted absolute error integral decreases from 0.2631 to 0.1473, the settling time shortens from 2.301 s to 1.613 s, and the relative overshoot reduces from 0.1494 to 0.01839. Control energy varied from 7980 to 7935, satisfying the energy constraint, while saturation occupancy decreased from 0.004 to 0.003. These results validate the effectiveness and engineering significance of the proposed constrained six-parameter joint tuning strategy for depth control in underwater vehicle navigation scenarios.

</details>


### [25] [TRANS: Terrain-aware Reinforcement Learning for Agile Navigation of Quadruped Robots under Social Interactions](https://arxiv.org/abs/2602.12724)
*Wei Zhu,Irfan Tito Kurniawan,Ye Zhao,Mistuhiro Hayashibe*

Main category: cs.RO

TL;DR: TRANS是一个用于四足机器人社交导航的深度强化学习框架，通过两阶段训练和三个DRL管道实现地形感知的敏捷导航


<details>
  <summary>Details</summary>
Motivation: 传统四足机器人导航将运动规划与运动控制分离，忽略了全身约束和地形感知；端到端方法需要高频感知但噪声大且计算成本高；现有方法大多假设静态环境，限制了在人群环境中的应用

Method: 提出两阶段训练框架和三个DRL管道：TRANS-Loco使用非对称actor-critic模型进行四足运动，无需显式地形或接触观测；TRANS-Nav使用对称actor-critic框架进行社交导航，将转换的LiDAR数据直接映射到差速运动学下的动作；TRANS统一管道整合前两者，支持地形感知的四足导航

Result: 与运动控制和社交导航基准的全面比较证明了TRANS的有效性；硬件实验进一步证实了其从仿真到现实的迁移潜力

Conclusion: TRANS框架成功解决了四足机器人在非结构化地形和社交环境中的导航问题，实现了地形感知的敏捷导航，并展示了良好的仿真到现实迁移能力

Abstract: This study introduces TRANS: Terrain-aware Reinforcement learning for Agile Navigation under Social interactions, a deep reinforcement learning (DRL) framework for quadrupedal social navigation over unstructured terrains. Conventional quadrupedal navigation typically separates motion planning from locomotion control, neglecting whole-body constraints and terrain awareness. On the other hand, end-to-end methods are more integrated but require high-frequency sensing, which is often noisy and computationally costly. In addition, most existing approaches assume static environments, limiting their use in human-populated settings. To address these limitations, we propose a two-stage training framework with three DRL pipelines. (1) TRANS-Loco employs an asymmetric actor-critic (AC) model for quadrupedal locomotion, enabling traversal of uneven terrains without explicit terrain or contact observations. (2) TRANS-Nav applies a symmetric AC framework for social navigation, directly mapping transformed LiDAR data to ego-agent actions under differential-drive kinematics. (3) A unified pipeline, TRANS, integrates TRANS-Loco and TRANS-Nav, supporting terrain-aware quadrupedal navigation in uneven and socially interactive environments. Comprehensive benchmarks against locomotion and social navigation baselines demonstrate the effectiveness of TRANS. Hardware experiments further confirm its potential for sim-to-real transfer.

</details>


### [26] [SafeFlowMPC: Predictive and Safe Trajectory Planning for Robot Manipulators with Learning-based Policies](https://arxiv.org/abs/2602.12794)
*Thies Oelerich,Gerald Ebmer,Christian Hartl-Nesic,Andreas Kugi*

Main category: cs.RO

TL;DR: SafeFlowMPC结合流匹配和在线优化，在保证安全性的同时实现实时机器人控制，在7自由度机械臂上验证了抓取和人机交接任务。


<details>
  <summary>Details</summary>
Motivation: 机器人融入日常生活需要灵活性和实时反应能力。基于学习的方法能训练强大策略但缺乏可解释性和安全保证；基于优化的方法有安全保证但缺乏灵活性和泛化能力。需要结合两者优势。

Method: 提出SafeFlowMPC方法，结合流匹配（flow matching）和在线优化，使用次优模型预测控制（MPC）公式，保证实时执行的同时确保安全性。

Result: 在KUKA 7自由度机械臂上进行了三个真实世界实验：两个抓取实验和一个动态人机物体交接实验，表现出色。

Conclusion: SafeFlowMPC成功结合了学习和优化的优势，在保证安全性的同时实现了实时机器人控制，在复杂任务中表现出良好性能。

Abstract: The emerging integration of robots into everyday life brings several major challenges. Compared to classical industrial applications, more flexibility is needed in combination with real-time reactivity. Learning-based methods can train powerful policies based on demonstrated trajectories, such that the robot generalizes a task to similar situations. However, these black-box models lack interpretability and rigorous safety guarantees. Optimization-based methods provide these guarantees but lack the required flexibility and generalization capabilities. This work proposes SafeFlowMPC, a combination of flow matching and online optimization to combine the strengths of learning and optimization. This method guarantees safety at all times and is designed to meet the demands of real-time execution by using a suboptimal model-predictive control formulation. SafeFlowMPC achieves strong performance in three real-world experiments on a KUKA 7-DoF manipulator, namely two grasping experiment and a dynamic human-robot object handover experiment. A video of the experiments is available at http://www.acin.tuwien.ac.at/42d6. The code is available at https://github.com/TU-Wien-ACIN-CDS/SafeFlowMPC.

</details>


### [27] [SKYSURF: A Self-learning Framework for Persistent Surveillance using Cooperative Aerial Gliders](https://arxiv.org/abs/2602.12838)
*Houssem Eddine Mohamadi,Nadjia Kara*

Main category: cs.RO

TL;DR: 本文提出了一种用于自主部署滑翔能力无人机的局部-全局行为管理和决策方法，通过合作无人机建模、任务规划、路径规划和延迟学习策略，显著提升了监视持久性和目标检测效率。


<details>
  <summary>Details</summary>
Motivation: 小型无人机的监视应用受限于机载电源的续航时间，需要寻找替代的可再生升力来源。从上升的浮力空气中提取能量是一个有前景的解决方案，但需要有效的自主部署和管理方法。

Method: 1. 将合作无人机建模为非确定性有限状态理性智能体；2. 设计任务规划模块分配任务并生成动态导航航点；3. 应用可见性和预测概念的新路径规划方案避免碰撞；4. 采用延迟学习和调谐策略优化路径跟踪控制器的增益。

Result: 1. 显著提升监视持久性（更长时间保持空中飞行）；2. 目标检测能力是非合作和半合作方法的两倍；3. 功耗大幅降低（六小时仅消耗约6%的电池电量）；4. 通过三个基准测试和15种进化算法的严格比较分析验证了方法的有效性。

Conclusion: 提出的局部-全局行为管理和决策方法能够有效解决滑翔能力无人机的自主部署问题，在维持监视持久性、最大化目标检测和降低功耗方面表现出色，为小型无人机监视应用提供了可行的解决方案。

Abstract: The success of surveillance applications involving small unmanned aerial vehicles (UAVs) depends on how long the limited on-board power would persist. To cope with this challenge, alternative renewable sources of lift are sought. One promising solution is to extract energy from rising masses of buoyant air. This paper proposes a local-global behavioral management and decision-making approach for the autonomous deployment of soaring-capable UAVs. The cooperative UAVs are modeled as non-deterministic finite state-based rational agents. In addition to a mission planning module for assigning tasks and issuing dynamic navigation waypoints for a new path planning scheme, in which the concepts of visibility and prediction are applied to avoid the collisions. Moreover, a delayed learning and tuning strategy is employed optimize the gains of the path tracking controller. Rigorous comparative analyses carried out with three benchmarking baselines and 15 evolutionary algorithms highlight the adequacy of the proposed approach for maintaining the surveillance persistency (staying aloft for longer periods without landing) and maximizing the detection of targets (two times better than non-cooperative and semi-cooperative approaches) with less power consumption (almost 6% of battery consumed in six hours).

</details>


### [28] [Adding internal audio sensing to internal vision enables human-like in-hand fabric recognition with soft robotic fingertips](https://arxiv.org/abs/2602.12918)
*Iris Andrussow,Jans Solano,Benjamin A. Richardson,Georg Martius,Katherine J. Kuchenbecker*

Main category: cs.RO

TL;DR: 机器人通过新型触觉传感器系统结合视觉和音频感知来识别不同织物，实现了97%的分类准确率


<details>
  <summary>Details</summary>
Motivation: 人类指尖能同时感知空间力模式和纹理振动，但机器人触觉传感器难以同时实现高空间分辨率和高时间采样率，需要开发能同时感知这两种触觉信息的系统

Method: 开发了包含Minsight（50Hz视觉传感器）和Minsound（50Hz-15kHz MEMS麦克风）的双传感器系统，机器人模仿人类动作夹持和摩擦织物样本，使用基于Transformer的方法进行分类

Result: 在20种常见织物数据集上达到97%的最高分类准确率，音频传感器对分类性能贡献显著，外部麦克风增强了在嘈杂环境下的鲁棒性，并能学习织物拉伸性、厚度和粗糙度的通用表征

Conclusion: 音频-视觉触觉感知系统能有效识别织物特性，音频传感器在织物分类中具有重要作用，该方法能泛化到训练数据之外学习织物的通用物理特性

Abstract: Distinguishing the feel of smooth silk from coarse cotton is a trivial everyday task for humans. When exploring such fabrics, fingertip skin senses both spatio-temporal force patterns and texture-induced vibrations that are integrated to form a haptic representation of the explored material. It is challenging to reproduce this rich, dynamic perceptual capability in robots because tactile sensors typically cannot achieve both high spatial resolution and high temporal sampling rate. In this work, we present a system that can sense both types of haptic information, and we investigate how each type influences robotic tactile perception of fabrics. Our robotic hand's middle finger and thumb each feature a soft tactile sensor: one is the open-source Minsight sensor that uses an internal camera to measure fingertip deformation and force at 50 Hz, and the other is our new sensor Minsound that captures vibrations through an internal MEMS microphone with a bandwidth from 50 Hz to 15 kHz. Inspired by the movements humans make to evaluate fabrics, our robot actively encloses and rubs folded fabric samples between its two sensitive fingers. Our results test the influence of each sensing modality on overall classification performance, showing high utility for the audio-based sensor. Our transformer-based method achieves a maximum fabric classification accuracy of 97 % on a dataset of 20 common fabrics. Incorporating an external microphone away from Minsound increases our method's robustness in loud ambient noise conditions. To show that this audio-visual tactile sensing approach generalizes beyond the training data, we learn general representations of fabric stretchiness, thickness, and roughness.

</details>


### [29] [INHerit-SG: Incremental Hierarchical Semantic Scene Graphs with RAG-Style Retrieval](https://arxiv.org/abs/2602.12971)
*YukTungSamuel Fang,Zhikang Shi,Jiabin Qiu,Zixuan Chen,Jieqi Shi,Hao Xu,Jing Huo,Yang Gao*

Main category: cs.RO

TL;DR: INHerit-SG提出了一种面向机器人导航的语义场景图系统，通过结构化知识库、异步双进程架构和事件触发更新机制，解决现有方法在可解释性和实时性方面的不足，显著提升复杂查询的成功率。


<details>
  <summary>Details</summary>
Motivation: 现有语义场景图方法主要依赖离线批处理或隐式特征嵌入，难以支持复杂环境中可解释的人类意图推理，与具身任务需求存在根本性不匹配。

Method: 1) 将地图重新定义为结构化、RAG就绪的知识库，引入自然语言描述作为显式语义锚点；2) 采用异步双进程架构和Floor-Room-Area-Object层次结构，解耦几何分割与语义推理；3) 事件触发的地图更新机制仅在有意义语义事件发生时重组图结构；4) 部署多角色LLM分解查询并处理逻辑否定，采用硬到软过滤策略确保鲁棒推理。

Result: 在HM3DSem-SQR新数据集和真实环境中评估，系统在复杂查询上达到最先进性能，并展示了下游导航任务的可扩展性。

Conclusion: INHerit-SG通过显式可解释性提高了复杂检索的成功率和可靠性，使系统能够适应更广泛的人类交互任务，为机器人导航提供了更有效的语义环境抽象方法。

Abstract: Driven by advancements in foundation models, semantic scene graphs have emerged as a prominent paradigm for high-level 3D environmental abstraction in robot navigation. However, existing approaches are fundamentally misaligned with the needs of embodied tasks. As they rely on either offline batch processing or implicit feature embeddings, the maps can hardly support interpretable human-intent reasoning in complex environments. To address these limitations, we present INHerit-SG. We redefine the map as a structured, RAG-ready knowledge base where natural-language descriptions are introduced as explicit semantic anchors to better align with human intent. An asynchronous dual-process architecture, together with a Floor-Room-Area-Object hierarchy, decouples geometric segmentation from time-consuming semantic reasoning. An event-triggered map update mechanism reorganizes the graph only when meaningful semantic events occur. This strategy enables our graph to maintain long-term consistency with relatively low computational overhead. For retrieval, we deploy multi-role Large Language Models (LLMs) to decompose queries into atomic constraints and handle logical negations, and employ a hard-to-soft filtering strategy to ensure robust reasoning. This explicit interpretability improves the success rate and reliability of complex retrievals, enabling the system to adapt to a broader spectrum of human interaction tasks. We evaluate INHerit-SG on a newly constructed dataset, HM3DSem-SQR, and in real-world environments. Experiments demonstrate that our system achieves state-of-the-art performance on complex queries, and reveal its scalability for downstream navigation tasks. Project Page: https://fangyuktung.github.io/INHeritSG.github.io/

</details>


### [30] [Learning Native Continuation for Action Chunking Flow Policies](https://arxiv.org/abs/2602.12978)
*Yufeng Liu,Hang Yu,Juntu Zhao,Bocheng Li,Di Zhang,Mingzhu Li,Wenxuan Wu,Yingdong Hu,Junyuan Xie,Junliang Guo,Dequan Wang,Yang Gao*

Main category: cs.RO

TL;DR: Legato是一种用于动作分块流式VLA模型的训练时延续方法，通过初始化去噪过程为已知动作和噪声的混合，重塑学习到的流动态，并使用随机化调度条件，显著改善了轨迹平滑度和任务完成时间。


<details>
  <summary>Details</summary>
Motivation: 现有的实时分块（RTC）方法虽然缓解了分块边界的不连续性问题，但它是外部于策略的，导致虚假的多模态切换和本质上不光滑的轨迹。需要一种内在的、训练时的方法来改善动作分块VLA模型的执行连续性。

Method: Legato包含三个关键技术：1）从已知动作和噪声的调度形状混合中初始化去噪过程，让模型接触部分动作信息；2）重塑学习到的流动态，确保在每步指导下的去噪过程在训练和推理之间保持一致；3）使用随机化调度条件训练，支持变化的推理延迟并实现可控的平滑度。

Result: 在五个操作任务上的广泛真实世界实验表明，Legato在轨迹平滑度和任务完成时间上都比RTC有约10%的改进，减少了虚假的多模态切换和执行犹豫，产生了更平滑的轨迹。

Conclusion: Legato作为一种训练时延续方法，能够有效改善动作分块流式VLA模型的执行连续性，通过内在的机制而非外部修正，实现了更平滑的轨迹和更高效的任务完成。

Abstract: Action chunking enables Vision Language Action (VLA) models to run in real time, but naive chunked execution often exhibits discontinuities at chunk boundaries. Real-Time Chunking (RTC) alleviates this issue but is external to the policy, leading to spurious multimodal switching and trajectories that are not intrinsically smooth. We propose Legato, a training-time continuation method for action-chunked flow-based VLA policies. Specifically, Legato initializes denoising from a schedule-shaped mixture of known actions and noise, exposing the model to partial action information. Moreover, Legato reshapes the learned flow dynamics to ensure that the denoising process remains consistent between training and inference under per-step guidance. Legato further uses randomized schedule condition during training to support varying inference delays and achieve controllable smoothness. Empirically, Legato produces smoother trajectories and reduces spurious multimodal switching during execution, leading to less hesitation and shorter task completion time. Extensive real-world experiments show that Legato consistently outperforms RTC across five manipulation tasks, achieving approximately 10% improvements in both trajectory smoothness and task completion time.

</details>


### [31] [How Swarms Differ: Challenges in Collective Behaviour Comparison](https://arxiv.org/abs/2602.13016)
*André Fialho Jesus,Jonas Kuckling*

Main category: cs.RO

TL;DR: 研究群体行为特征集对行为相似性度量的影响，评估现有特征集的鲁棒性，并提出基于自组织映射的方法识别难以区分的特征空间区域


<details>
  <summary>Details</summary>
Motivation: 群体行为需要通过数值特征进行表达（如分类或模仿学习），但现有方法通常针对特定情境设计特征集，缺乏对鲁棒性的考虑。自动设计群体行为的方法依赖于定量测量行为相似性的能力，因此需要研究特征集对行为相似性度量的影响。

Method: 1. 从先前群体机器人研究中选取特征集和相似性度量方法；2. 评估这些特征集在狭窄行为情境之外的鲁棒性；3. 分析特征集与相似性度量的相互作用；4. 提出基于自组织映射的方法来识别特征空间中难以区分行为的区域。

Result: 研究发现特征集与相似性度量的相互作用使得某些组合更适合区分相似行为群体。同时，自组织映射方法能够有效识别特征空间中行为难以区分的区域。

Conclusion: 特征集的选择对群体行为相似性度量至关重要，特征集与相似性度量的适当组合能更好地区分相似行为。提出的自组织映射方法有助于识别特征空间的局限性，为设计更鲁棒的特征集提供了新思路。

Abstract: Collective behaviours often need to be expressed through numerical features, e.g., for classification or imitation learning. This problem is often addressed by proposing an ad-hoc feature set for a particular swarm behaviour context, usually without further consideration of the solution's resilience outside of the conceived context. Yet, the development of automatic methods to design swarm behaviours is dependent on the ability to measure quantitatively the similarity of swarm behaviours. Hence, we investigate the impact of feature sets for collective behaviours. We select swarm feature sets and similarity measures from prior swarm robotics works, which mainly considered a narrow behavioural context and assess their robustness. We demonstrate that the interplay of feature set and similarity measure makes some combinations more suitable to distinguish groups of similar behaviours. We also propose a self-organised map-based approach to identify regions of the feature space where behaviours cannot be easily distinguished.

</details>


### [32] [SENSE-STEP: Learning Sim-to-Real Locomotion for a Sensory-Enabled Soft Quadruped Robot](https://arxiv.org/abs/2602.13078)
*Storm de Kam,Ebrahim Shahabi,Cosimo Della Santina*

Main category: cs.RO

TL;DR: 本文提出了一种基于学习的控制框架，用于配备触觉吸盘脚的气动软体四足机器人，通过仿真训练和实验验证，实现了在平坦和倾斜表面的鲁棒闭环运动控制。


<details>
  <summary>Details</summary>
Motivation: 软体四足机器人的鲁棒闭环运动控制面临诸多挑战，包括高维动力学、执行器迟滞、难以建模的接触交互，以及传统本体感知提供的地面接触信息有限。为解决这些问题，需要开发能够有效利用多模态传感信息的控制方法。

Method: 提出基于学习的控制框架，为配备触觉吸盘脚的气动软体四足机器人设计控制策略。采用分阶段学习过程：从参考步态开始，在随机化环境条件下逐步优化。控制器将本体感知和触觉反馈映射到协调的气动执行和吸盘命令。

Result: 在真实机器人上部署时，闭环策略优于开环基线：在平坦表面上前进速度提高41%，在5度斜坡上提高91%。消融研究显示触觉力估计和惯性反馈对稳定运动的作用，相比无传感器反馈配置，性能提升高达56%。

Conclusion: 该学习型控制框架成功实现了软体四足机器人的鲁棒闭环运动控制，通过结合本体感知和触觉反馈，显著提升了在平坦和倾斜表面的运动性能，验证了多模态传感在软体机器人控制中的重要性。

Abstract: Robust closed-loop locomotion remains challenging for soft quadruped robots due to high-dimensional dynamics, actuator hysteresis, and difficult-to-model contact interactions, while conventional proprioception provides limited information about ground contact. In this paper, we present a learning-based control framework for a pneumatically actuated soft quadruped equipped with tactile suction-cup feet, and we validate the approach experimentally on physical hardware. The control policy is trained in simulation through a staged learning process that starts from a reference gait and is progressively refined under randomized environmental conditions. The resulting controller maps proprioceptive and tactile feedback to coordinated pneumatic actuation and suction-cup commands, enabling closed-loop locomotion on flat and inclined surfaces. When deployed on the real robot, the closed-loop policy outperforms an open-loop baseline, increasing forward speed by 41% on a flat surface and by 91% on a 5-degree incline. Ablation studies further demonstrate the role of tactile force estimates and inertial feedback in stabilizing locomotion, with performance improvements of up to 56% compared to configurations without sensory feedback.

</details>


### [33] [UniManip: General-Purpose Zero-Shot Robotic Manipulation with Agentic Operational Graph](https://arxiv.org/abs/2602.13086)
*Haichao Liu,Yuanjiang Xue,Yuheng Zhou,Haoyuan Deng,Yinan Liang,Lihua Xie,Ziwei Wang*

Main category: cs.RO

TL;DR: UniManip框架通过双层智能操作图统一语义推理和物理基础，实现零样本泛化的机器人操作，在未见过的物体和任务上比现有方法成功率提高22.5%-25.0%


<details>
  <summary>Details</summary>
Motivation: 现有机器人操作方法在零样本泛化方面存在不足：端到端视觉-语言-动作模型缺乏长时任务所需的精度，而传统分层规划器在面对开放世界变化时存在语义刚性。需要一种能无缝桥接高层语义意图和低层物理交互的通用机器人操作框架。

Method: 提出UniManip框架，基于双层智能操作图（AOG），包含高层智能层用于任务编排和低层场景层用于动态状态表示。系统作为动态智能循环运行：从非结构化感知实例化对象中心场景图，通过安全感知局部规划器将这些表示参数化为无碰撞轨迹，并利用结构化内存自主诊断和恢复执行失败。

Result: 在未见过的物体和任务上展示了强大的零样本能力，相比最先进的VLA和分层基线方法，成功率分别提高22.5%和25.0%。系统能够直接从固定基座设置零样本迁移到移动操作，无需微调或重新配置。

Conclusion: UniManip通过统一语义推理和物理基础的双层智能操作图，解决了机器人操作中的零样本泛化问题，为通用机器人操作提供了有效框架，并展示了从固定基座到移动操作的直接迁移能力。

Abstract: Achieving general-purpose robotic manipulation requires robots to seamlessly bridge high-level semantic intent with low-level physical interaction in unstructured environments. However, existing approaches falter in zero-shot generalization: end-to-end Vision-Language-Action (VLA) models often lack the precision required for long-horizon tasks, while traditional hierarchical planners suffer from semantic rigidity when facing open-world variations. To address this, we present UniManip, a framework grounded in a Bi-level Agentic Operational Graph (AOG) that unifies semantic reasoning and physical grounding. By coupling a high-level Agentic Layer for task orchestration with a low-level Scene Layer for dynamic state representation, the system continuously aligns abstract planning with geometric constraints, enabling robust zero-shot execution. Unlike static pipelines, UniManip operates as a dynamic agentic loop: it actively instantiates object-centric scene graphs from unstructured perception, parameterizes these representations into collision-free trajectories via a safety-aware local planner, and exploits structured memory to autonomously diagnose and recover from execution failures. Extensive experiments validate the system's robust zero-shot capability on unseen objects and tasks, demonstrating a 22.5% and 25.0% higher success rate compared to state-of-the-art VLA and hierarchical baselines, respectively. Notably, the system enables direct zero-shot transfer from fixed-base setups to mobile manipulation without fine-tuning or reconfiguration. Our open-source project page can be found at https://henryhcliu.github.io/unimanip.

</details>


### [34] [Temporally-Sampled Efficiently Adaptive State Lattices for Autonomous Ground Robot Navigation in Partially Observed Environments](https://arxiv.org/abs/2602.13159)
*Ashwin Satish Menon,Eric R. Damm,Eli S. Lancaster,Felix A. Sanchez,Jason M. Gregory,Thomas M. Howard*

Main category: cs.RO

TL;DR: 提出TSEASL架构解决越野机器人导航中区域规划器频繁变更参考轨迹导致的安全问题，通过仲裁机制考虑历史轨迹优化，提高导航稳定性和安全性


<details>
  <summary>Details</summary>
Motivation: 越野环境中机器人传感器只能部分观测环境，导致最优路径需要持续修正。传统导航架构中，区域规划器输出的参考轨迹在连续规划周期中可能差异很大，这种快速变化的指导会导致不安全的导航行为，经常需要在越野自主穿越中进行人工安全干预

Method: 提出TSEASL（时间采样高效自适应状态网格）区域规划器仲裁架构，该架构考虑更新和优化的历史轨迹与当前生成轨迹的对比，通过仲裁机制选择更稳定的参考轨迹

Result: 在Clearpath Robotics Warthog无人地面车辆和真实地图数据上的测试表明，使用TSEASL时机器人在基线规划器需要人工干预的相同位置不再需要干预，同时记录了比基线更高的规划器稳定性水平

Conclusion: TSEASL架构有效解决了越野导航中参考轨迹频繁变化导致的安全问题，提高了导航稳定性和安全性。论文最后讨论了进一步改进TSEASL以使其更适用于各种越野自主场景的方法

Abstract: Due to sensor limitations, environments that off-road mobile robots operate in are often only partially observable. As the robots move throughout the environment and towards their goal, the optimal route is continuously revised as the sensors perceive new information. In traditional autonomous navigation architectures, a regional motion planner will consume the environment map and output a trajectory for the local motion planner to use as a reference. Due to the continuous revision of the regional plan guidance as a result of changing map information, the reference trajectories which are passed down to the local planner can differ significantly across sequential planning cycles. This rapidly changing guidance can result in unsafe navigation behavior, often requiring manual safety interventions during autonomous traversals in off-road environments. To remedy this problem, we propose Temporally-Sampled Efficiently Adaptive State Lattices (TSEASL), which is a regional planner arbitration architecture that considers updated and optimized versions of previously generated trajectories against the currently generated trajectory. When tested on a Clearpath Robotics Warthog Unmanned Ground Vehicle as well as real map data collected from the Warthog, results indicate that when running TSEASL, the robot did not require manual interventions in the same locations where the robot was running the baseline planner. Additionally, higher levels of planner stability were recorded with TSEASL over the baseline. The paper concludes with a discussion of further improvements to TSEASL in order to make it more generalizable to various off-road autonomy scenarios.

</details>


### [35] [Human Emotion-Mediated Soft Robotic Arts: Exploring the Intersection of Human Emotions, Soft Robotics and Arts](https://arxiv.org/abs/2602.13163)
*Saitarun Nadipineni,Chenhao Hong,Tanishtha Ramlall,Chapa Sirithunge,Kaspar Althoefer,Fumiya Iida,Thilina Dulantha Lalitharatne*

Main category: cs.RO

TL;DR: 该研究探索了人类情感、软体机器人和艺术的交叉领域，通过脑电α波测量情感状态，并将其映射到软体角色和花朵的动态运动中，创建情感介导的软体机器人艺术展示。


<details>
  <summary>Details</summary>
Motivation: 软体机器人具有灵活性、适应性和安全性，适合需要细腻、有机、逼真运动的艺术应用。研究旨在探索人类情感如何通过软体机器人体现，为艺术表达和互动创造新形式。

Method: 1. 基于脑电/EEG信号测量α波来量化人类情感状态；2. 将α波映射到两个软体装置（软体角色和软体花朵）的动态运动中；3. 通过实验演示这一概念。

Result: 研究成功展示了软体机器人如何体现人类情感状态，通过α波控制软体装置的运动，实现了情感介导的艺术表达和互动。

Conclusion: 软体机器人能够体现人类情感状态，为艺术表达和互动提供了新媒介，展示了艺术展示如何通过软体机器人具象化，开辟了情感介导软体机器人艺术的新领域。

Abstract: Soft robotics has emerged as a versatile field with applications across various domains, from healthcare to industrial automation, and more recently, art and interactive installations. The inherent flexibility, adaptability, and safety of soft robots make them ideal for applications that require delicate, organic, and lifelike movement, allowing for immersive and responsive interactions. This study explores the intersection of human emotions, soft robotics, and art to establish and create new forms of human emotion-mediated soft robotic art. In this paper, we introduce two soft embodiments: a soft character and a soft flower as an art display that dynamically responds to brain signals based on alpha waves, reflecting different emotion levels. We present how human emotions can be measured as alpha waves based on brain/EEG signals, how we map the alpha waves to the dynamic movements of the two soft embodiments, and demonstrate our proposed concept using experiments. The findings of this study highlight how soft robotics can embody human emotional states, offering a new medium for insightful artistic expression and interaction, and demonstrating how art displays can be embodied.

</details>


### [36] [Steerable Vision-Language-Action Policies for Embodied Reasoning and Hierarchical Control](https://arxiv.org/abs/2602.13193)
*William Chen,Jagdeep Singh Bhatia,Catherine Glossop,Nikhil Mathihalli,Ria Doshi,Andy Tang,Danny Driess,Karl Pertsch,Sergey Levine*

Main category: cs.RO

TL;DR: 提出Steerable Policies方法，通过在多个抽象层次上训练视觉-语言-动作模型，增强低层控制能力，从而更好地利用预训练视觉-语言模型的常识知识，提升机器人任务泛化性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常采用分层架构，视觉-语言模型通过自然语言指令控制低层策略，但这种接口限制了视觉-语言模型对低层行为的指导能力。需要一种方法能够更好地将视觉-语言模型的常识知识落地到机器人行为控制中。

Method: 提出Steerable Policies方法，训练视觉-语言-动作模型处理不同抽象层次的合成指令，包括子任务、动作和像素坐标等。通过增强低层可控性，解锁预训练视觉-语言模型的知识。使用两种控制方式：学习的高层具身推理器和通过上下文学习推理命令抽象的现成视觉-语言模型。

Result: 在大量真实世界操作实验中，这两种新方法都优于现有的具身推理视觉-语言-动作模型和基于视觉-语言模型的分层基线方法，在挑战性的泛化和长时程任务上表现更佳。

Conclusion: Steerable Policies通过增强低层可控性，能够更好地利用预训练视觉-语言模型的常识知识，显著提升机器人任务泛化能力，为视觉-语言模型知识落地到机器人控制提供了有效途径。

Abstract: Pretrained vision-language models (VLMs) can make semantic and visual inferences across diverse settings, providing valuable common-sense priors for robotic control. However, effectively grounding this knowledge in robot behaviors remains an open challenge. Prior methods often employ a hierarchical approach where VLMs reason over high-level commands to be executed by separate low-level policies, e.g., vision-language-action models (VLAs). The interface between VLMs and VLAs is usually natural language task instructions, which fundamentally limits how much VLM reasoning can steer low-level behavior. We thus introduce Steerable Policies: VLAs trained on rich synthetic commands at various levels of abstraction, like subtasks, motions, and grounded pixel coordinates. By improving low-level controllability, Steerable Policies can unlock pretrained knowledge in VLMs, enabling improved task generalization. We demonstrate this benefit by controlling our Steerable Policies with both a learned high-level embodied reasoner and an off-the-shelf VLM prompted to reason over command abstractions via in-context learning. Across extensive real-world manipulation experiments, these two novel methods outperform prior embodied reasoning VLAs and VLM-based hierarchical baselines, including on challenging generalization and long-horizon tasks.
  Website: steerable-policies.github.io

</details>


### [37] [Imitating What Works: Simulation-Filtered Modular Policy Learning from Human Videos](https://arxiv.org/abs/2602.13197)
*Albert J. Zhai,Kuo-Hao Zeng,Jiasen Lu,Ali Farhadi,Shenlong Wang,Wei-Chiu Ma*

Main category: cs.RO

TL;DR: PSI框架通过人类视频数据训练模块化机器人操作策略，利用仿真中的抓取-轨迹过滤实现无机器人数据的学习，显著提升任务兼容抓取能力


<details>
  <summary>Details</summary>
Motivation: 人类视频数据为机器人学习提供了可扩展的数据源，但现有方法在抓取行为学习上存在局限，特别是对于非人形手机器人。模块化策略设计虽然能生成稳定抓取，但任务兼容性不足，阻碍后续动作执行。

Method: 提出Perceive-Simulate-Imitate (PSI)框架：1) 处理人类视频运动数据；2) 在仿真中进行抓取-轨迹配对过滤；3) 扩展轨迹数据并添加抓取适宜性标签；4) 通过监督学习训练任务导向的抓取能力。

Result: 真实世界实验表明，该框架能够高效学习精确操作技能，无需任何机器人数据，相比简单使用抓取生成器的方法，性能显著更鲁棒。

Conclusion: PSI框架通过仿真扩展人类视频数据，有效解决了任务兼容抓取的学习问题，为基于人类视频的机器人操作技能学习提供了可行的模块化解决方案。

Abstract: The ability to learn manipulation skills by watching videos of humans has the potential to unlock a new source of highly scalable data for robot learning. Here, we tackle prehensile manipulation, in which tasks involve grasping an object before performing various post-grasp motions. Human videos offer strong signals for learning the post-grasp motions, but they are less useful for learning the prerequisite grasping behaviors, especially for robots without human-like hands. A promising way forward is to use a modular policy design, leveraging a dedicated grasp generator to produce stable grasps. However, arbitrary stable grasps are often not task-compatible, hindering the robot's ability to perform the desired downstream motion. To address this challenge, we present Perceive-Simulate-Imitate (PSI), a framework for training a modular manipulation policy using human video motion data processed by paired grasp-trajectory filtering in simulation. This simulation step extends the trajectory data with grasp suitability labels, which allows for supervised learning of task-oriented grasping capabilities. We show through real-world experiments that our framework can be used to learn precise manipulation skills efficiently without any robot data, resulting in significantly more robust performance than using a grasp generator naively.

</details>
