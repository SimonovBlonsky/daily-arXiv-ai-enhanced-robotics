<div id=toc></div>

# Table of Contents

- [cs.RO](#cs.RO) [Total: 26]


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [1] [Signal or 'Noise': Human Reactions to Robot Errors in the Wild](https://arxiv.org/abs/2602.05010)
*Maia Stiber,Sameer Khan,Russell Taylor,Chien-Ming Huang*

Main category: cs.RO

TL;DR: 研究探索了在真实世界环境中人们对机器人错误的社交反应，通过咖啡机器人实地部署发现参与者会表达丰富但"嘈杂"的社交信号，特别是在群体互动中。


<details>
  <summary>Details</summary>
Motivation: 在现实世界中，机器人经常出错，但人们对这些错误的社会反应在实验室环境之外了解甚少。先前研究表明社交信号在受限互动中对错误管理是可靠且有用的，但尚不清楚这在真实世界中是否成立，特别是在非社交机器人进行重复和群体互动、出现连续或传播错误的情况下。

Method: 研究者构建了一个咖啡机器人，并在公共场所进行了实地部署（N=49名参与者），观察人们在真实世界环境中对机器人错误的社交反应。

Result: 参与者对错误和其他刺激持续表达多样化的社交信号，特别是在群体互动中。研究发现真实世界中的社交信号丰富（参与者自愿提供互动信息），但"嘈杂"。

Conclusion: 研究讨论了在真实世界人机交互中使用社交信号的教训、益处和挑战，表明社交信号在野外环境中具有价值但需要处理其复杂性。

Abstract: In the real world, robots frequently make errors, yet little is known about people's social responses to errors outside of lab settings. Prior work has shown that social signals are reliable and useful for error management in constrained interactions, but it is unclear if this holds in the real world - especially with a non-social robot in repeated and group interactions with successive or propagated errors. To explore this, we built a coffee robot and conducted a public field deployment ($N = 49$). We found that participants consistently expressed varied social signals in response to errors and other stimuli, particularly during group interactions. Our findings suggest that social signals in the wild are rich (with participants volunteering information about the interaction), but "noisy." We discuss lessons, benefits, and challenges for using social signals in real-world HRI.

</details>


### [2] [Differentiable Inverse Graphics for Zero-shot Scene Reconstruction and Robot Grasping](https://arxiv.org/abs/2602.05029)
*Octavio Arriaga,Proneet Sharma,Jichen Guo,Marc Otto,Siddhant Kadwe,Rebecca Adam*

Main category: cs.RO

TL;DR: 提出一种可微分神经图形模型，结合神经基础模型与基于物理的可微分渲染，实现零样本场景重建和机器人抓取，无需额外3D数据或测试样本。


<details>
  <summary>Details</summary>
Motivation: 当前最先进模型依赖大量训练数据和测试样本构建黑盒场景表示，需要更数据高效、可解释且泛化能力强的机器人自主系统来应对新环境中的未知物体。

Method: 结合神经基础模型与物理可微分渲染，通过求解一系列约束优化问题，从单张RGBD图像和边界框估计物理一致的场景参数（网格、光照、材质、6D姿态）。

Result: 在标准模型无关少样本基准测试中超越现有算法，并在零样本抓取任务中验证了场景重建的准确性。

Conclusion: 该方法为实现更数据高效、可解释且泛化能力强的机器人自主系统提供了途径，能够在无需大量数据集或测试采样的条件下实现零样本物理一致场景重建和抓取。

Abstract: Operating effectively in novel real-world environments requires robotic systems to estimate and interact with previously unseen objects. Current state-of-the-art models address this challenge by using large amounts of training data and test-time samples to build black-box scene representations. In this work, we introduce a differentiable neuro-graphics model that combines neural foundation models with physics-based differentiable rendering to perform zero-shot scene reconstruction and robot grasping without relying on any additional 3D data or test-time samples. Our model solves a series of constrained optimization problems to estimate physically consistent scene parameters, such as meshes, lighting conditions, material properties, and 6D poses of previously unseen objects from a single RGBD image and bounding boxes. We evaluated our approach on standard model-free few-shot benchmarks and demonstrated that it outperforms existing algorithms for model-free few-shot pose estimation. Furthermore, we validated the accuracy of our scene reconstructions by applying our algorithm to a zero-shot grasping task. By enabling zero-shot, physically-consistent scene reconstruction and grasping without reliance on extensive datasets or test-time sampling, our approach offers a pathway towards more data efficient, interpretable and generalizable robot autonomy in novel environments.

</details>


### [3] [Reinforcement Learning Enhancement Using Vector Semantic Representation and Symbolic Reasoning for Human-Centered Autonomous Emergency Braking](https://arxiv.org/abs/2602.05079)
*Vinal Asodia,Iman Sharifi,Saber Fallah*

Main category: cs.RO

TL;DR: 本文提出了一种结合神经符号特征表示和软一阶逻辑奖励函数的新方法，用于提升自动驾驶决策的上下文感知和价值对齐能力。


<details>
  <summary>Details</summary>
Motivation: 现有基于摄像头的深度强化学习方法存在两个主要问题：很少将高层场景上下文整合到特征表示中，以及依赖僵化的固定奖励函数。这限制了自动驾驶系统在复杂交通环境中的鲁棒性和安全性。

Method: 提出了一种新颖的流水线，生成包含语义、空间和形状信息的神经符号特征表示，特别关注安全关键的道路使用者。同时提出了软一阶逻辑（SFOL）奖励函数，通过符号推理模块平衡人类价值观。从分割图中提取语义和空间谓词，并应用于语言规则以获得奖励权重。

Result: 在CARLA仿真环境中的定量实验表明，与基线表示和奖励公式相比，所提出的神经符号表示和SFOL奖励函数在不同交通密度和遮挡水平下提高了策略鲁棒性和安全相关性能指标。

Conclusion: 研究结果表明，将整体表示和软推理整合到强化学习中，可以支持自动驾驶做出更具上下文感知和价值对齐的决策。

Abstract: The problem with existing camera-based Deep Reinforcement Learning approaches is twofold: they rarely integrate high-level scene context into the feature representation, and they rely on rigid, fixed reward functions. To address these challenges, this paper proposes a novel pipeline that produces a neuro-symbolic feature representation that encompasses semantic, spatial, and shape information, as well as spatially boosted features of dynamic entities in the scene, with an emphasis on safety-critical road users. It also proposes a Soft First-Order Logic (SFOL) reward function that balances human values via a symbolic reasoning module. Here, semantic and spatial predicates are extracted from segmentation maps and applied to linguistic rules to obtain reward weights. Quantitative experiments in the CARLA simulation environment show that the proposed neuro-symbolic representation and SFOL reward function improved policy robustness and safety-related performance metrics compared to baseline representations and reward formulations across varying traffic densities and occlusion levels. The findings demonstrate that integrating holistic representations and soft reasoning into Reinforcement Learning can support more context-aware and value-aligned decision-making for autonomous driving.

</details>


### [4] [A Framework for Combining Optimization-Based and Analytic Inverse Kinematics](https://arxiv.org/abs/2602.05092)
*Thomas Cohn,Lihan Tang,Alexandre Amice,Russ Tedrake*

Main category: cs.RO

TL;DR: 提出一种新的优化逆运动学方法，通过使用解析逆运动学解作为变量变换，使优化器更容易求解，在碰撞避免、抓取选择和类人机器人稳定性等挑战性问题中取得了更高的成功率。


<details>
  <summary>Details</summary>
Motivation: 传统解析方法和优化方法在解决逆运动学问题时各有优缺点，但难以统一利用两者的优势。优化方法面临的主要挑战是关节角度与末端执行器位姿之间的复杂非线性关系，特别是在处理碰撞避免等非凸约束时，优化算法可能失败率较高。

Method: 提出新的优化逆运动学公式，使用解析逆运动学解作为变量变换。这种方法从根本上简化了优化问题的求解难度，并在三种流行的求解器上进行了测试，代表了三种不同的约束非线性优化范式。

Result: 广泛的实验比较表明，新公式在各种挑战性逆运动学问题（包括碰撞避免、抓取选择和类人机器人稳定性）中，比旧公式和基线方法实现了更高的成功率。

Conclusion: 通过将解析逆运动学解作为变量变换，提出的新优化公式显著提高了逆运动学问题的求解成功率，有效结合了解析方法和优化方法的优势。

Abstract: Analytic and optimization methods for solving inverse kinematics (IK) problems have been deeply studied throughout the history of robotics. The two strategies have complementary strengths and weaknesses, but developing a unified approach to take advantage of both methods has proved challenging. A key challenge faced by optimization approaches is the complicated nonlinear relationship between the joint angles and the end-effector pose. When this must be handled concurrently with additional nonconvex constraints like collision avoidance, optimization IK algorithms may suffer high failure rates. We present a new formulation for optimization IK that uses an analytic IK solution as a change of variables, and is fundamentally easier for optimizers to solve. We test our methodology on three popular solvers, representing three different paradigms for constrained nonlinear optimization. Extensive experimental comparisons demonstrate that our new formulation achieves higher success rates than the old formulation and baseline methods across various challenging IK problems, including collision avoidance, grasp selection, and humanoid stability.

</details>


### [5] [PLATO Hand: Shaping Contact Behavior with Fingernails for Precise Manipulation](https://arxiv.org/abs/2602.05156)
*Dong Ho Kang,Aaron Kim,Mingyo Seo,Kazuto Yokoyama,Tetsuya Narita,Luis Sentis*

Main category: cs.RO

TL;DR: PLATO Hand是一种灵巧的机器人手，采用混合指尖设计（刚性指甲嵌入柔性指腹），通过结构化接触几何实现多种交互模式，提升夹持稳定性、力感知能力和边缘敏感操作性能。


<details>
  <summary>Details</summary>
Motivation: 传统机器人手在精确操作中面临接触行为控制困难的问题，特别是在边缘敏感任务中。需要一种能够适应不同物体几何形状并实现多样化交互模式的指尖设计。

Method: 开发了PLATO Hand机器人手，采用混合指尖设计：刚性指甲嵌入柔性指腹中。建立了基于应变能的弯曲-压痕模型来指导指尖设计，解释引导接触如何保持局部压痕同时抑制全局弯曲。

Result: 实验结果显示：1）提高了夹持稳定性；2）增强了力可观测性；3）成功执行了边缘敏感操作任务，包括纸张分离、卡片拾取和橘子剥皮。

Conclusion: 结构化接触几何与力-运动透明机制的结合，为精确操作提供了一种基于物理原理的、具体化的方法，展示了混合指尖设计在灵巧操作中的优势。

Abstract: We present the PLATO Hand, a dexterous robotic hand with a hybrid fingertip that embeds a rigid fingernail within a compliant pulp. This design shapes contact behavior to enable diverse interaction modes across a range of object geometries. We develop a strain-energy-based bending-indentation model to guide the fingertip design and to explain how guided contact preserves local indentation while suppressing global bending. Experimental results show that the proposed robotic hand design demonstrates improved pinching stability, enhanced force observability, and successful execution of edge-sensitive manipulation tasks, including paper singulation, card picking, and orange peeling. Together, these results show that coupling structured contact geometry with a force-motion transparent mechanism provides a principled, physically embodied approach to precise manipulation.

</details>


### [6] [Informative Path Planning with Guaranteed Estimation Uncertainty](https://arxiv.org/abs/2602.05198)
*Kalvik Jakkala,Saurav Agarwal,Jason O'Kane,Srinivas Akella*

Main category: cs.RO

TL;DR: 该论文提出了一种保证估计不确定性的信息路径规划方法，通过最短路径确保高斯过程后验方差在监测区域内低于用户指定阈值，解决了传统方法在几何覆盖和信息采样之间的权衡问题。


<details>
  <summary>Details</summary>
Motivation: 环境监测机器人需要在距离和能量约束下重建空间场。传统的boustrophedon割草机式调查提供几何覆盖保证但会过度采样可预测区域，而信息路径规划方法利用空间相关性减少过度采样但缺乏重建质量保证。需要一种既能保证估计不确定性又能优化路径的方法。

Method: 提出三阶段方法：(1)从先验信息学习高斯过程模型；(2)将学习到的GP核转换为每个候选传感位置的二值覆盖图，指示哪些位置的不确定性可降至指定目标以下；(3)规划接近最短路径，其组合覆盖满足全局不确定性约束。针对异质现象采用非平稳核捕捉空间变化相关结构，并处理带障碍物的非凸环境。

Result: 在真实地形数据上的实验表明，该方法比现有基线使用更少的传感位置和更短的旅行距离就能满足不确定性目标。自主水面和水下航行器的实地测深实验证明了现实可行性。

Conclusion: 该方法成功桥接了传统几何覆盖和信息路径规划方法，在保证估计不确定性的同时优化路径长度，为环境监测机器人提供了具有理论保证的实用解决方案。

Abstract: Environmental monitoring robots often need to reconstruct spatial fields (e.g., salinity, temperature, bathymetry) under tight distance and energy constraints. Classical boustrophedon lawnmower surveys provide geometric coverage guarantees but can waste effort by oversampling predictable regions. In contrast, informative path planning (IPP) methods leverage spatial correlations to reduce oversampling, yet typically offer no guarantees on reconstruction quality. This paper bridges these approaches by addressing informative path planning with guaranteed estimation uncertainty: computing the shortest path whose measurements ensure that the Gaussian-process (GP) posterior variance -- an intrinsic uncertainty measure that lower-bounds the mean-squared prediction error under the GP model -- falls below a user-specified threshold over the monitoring region.
  We propose a three-stage approach: (i) learn a GP model from available prior information; (ii) transform the learned GP kernel into binary coverage maps for each candidate sensing location, indicating which locations' uncertainty can be reduced below a specified target; and (iii) plan a near-shortest route whose combined coverage satisfies the global uncertainty constraint. To address heterogeneous phenomena, we incorporate a nonstationary kernel that captures spatially varying correlation structure, and we accommodate non-convex environments with obstacles. Algorithmically, we present methods with provable approximation guarantees for sensing-location selection and for the joint selection-and-routing problem under a travel budget. Experiments on real-world topographic data show that our planners meet the uncertainty target using fewer sensing locations and shorter travel distances than a recent baseline, and field experiments with bathymetry-mapping autonomous surface and underwater vehicles demonstrate real-world feasibility.

</details>


### [7] [MobileManiBench: Simplifying Model Verification for Mobile Manipulation](https://arxiv.org/abs/2602.05233)
*Wenbo Wang,Fangyun Wei,QiXiu Li,Xi Chen,Yaobo Liang,Chang Xu,Jiaolong Yang,Baining Guo*

Main category: cs.RO

TL;DR: MobileManiBench：基于模拟的大规模移动机器人操作基准，通过强化学习自动生成多样化轨迹，支持VLA模型在真实部署前的验证


<details>
  <summary>Details</summary>
Motivation: 当前视觉-语言-动作模型依赖静态桌面场景的遥操作数据集，限制了移动机器人操作能力的发展，需要可扩展的模拟基准来验证VLA架构

Method: 基于NVIDIA Isaac Sim构建模拟优先框架，使用强化学习自动生成多样化操作轨迹，包含丰富的标注信息（语言指令、多视角RGB-D分割图像、同步状态和动作）

Result: 创建了包含2个移动平台、2个同步相机、630个物体、5种技能、100个任务、300K轨迹的大规模基准，支持机器人本体、感知模态和政策架构的受控研究

Conclusion: MobileManiBench为VLA模型提供了可扩展的验证平台，加速了数据效率和泛化能力研究，并为复杂模拟环境中的感知、推理和控制提供了基准测试

Abstract: Vision-language-action models have advanced robotic manipulation but remain constrained by reliance on the large, teleoperation-collected datasets dominated by the static, tabletop scenes. We propose a simulation-first framework to verify VLA architectures before real-world deployment and introduce MobileManiBench, a large-scale benchmark for mobile-based robotic manipulation. Built on NVIDIA Isaac Sim and powered by reinforcement learning, our pipeline autonomously generates diverse manipulation trajectories with rich annotations (language instructions, multi-view RGB-depth-segmentation images, synchronized object/robot states and actions). MobileManiBench features 2 mobile platforms (parallel-gripper and dexterous-hand robots), 2 synchronized cameras (head and right wrist), 630 objects in 20 categories, 5 skills (open, close, pull, push, pick) with over 100 tasks performed in 100 realistic scenes, yielding 300K trajectories. This design enables controlled, scalable studies of robot embodiments, sensing modalities, and policy architectures, accelerating research on data efficiency and generalization. We benchmark representative VLA models and report insights into perception, reasoning, and control in complex simulated environments.

</details>


### [8] [Low-Cost Underwater In-Pipe Centering and Inspection Using a Minimal-Sensing Robot](https://arxiv.org/abs/2602.05265)
*Kalvik Jakkala,Jason O'Kane*

Main category: cs.RO

TL;DR: 该论文提出了一种使用IMU、压力传感器和两个声纳（向下单波束声纳和旋转360度声纳）实现水下机器人自主管道巡检的极简传感策略，无需复杂传感器阵列或外部跟踪系统。


<details>
  <summary>Details</summary>
Motivation: 水下管道自主巡检面临几何空间受限、水体浑浊和可靠定位线索稀缺等挑战，需要开发能够在这些恶劣条件下工作的轻量级、计算高效的传感和控制系统。

Method: 提出基于IMU、压力传感器和两个声纳的极简传感方案；开发从单波束声纳强度数据中提取距离估计的计算高效方法；建立利用两个声纳距离估计管道中心的闭式几何模型；设计自适应置信度加权PD控制器保持对中。

Result: 在直径46厘米的淹没管道中使用BlueROV2重型ROV进行实验，证明系统能够在环境流动和结构变形条件下实现稳定对中和完整管道穿越，验证了轻量级传感架构的可行性。

Conclusion: 研究表明，通过轻量级、计算高效的传感和处理架构可以实现可靠的管道内导航和检查，推进了受限环境中自主水下检查的实用性。

Abstract: Autonomous underwater inspection of submerged pipelines is challenging due to confined geometries, turbidity, and the scarcity of reliable localization cues. This paper presents a minimal-sensing strategy that enables a free-swimming underwater robot to center itself and traverse a flooded pipe of known radius using only an IMU, a pressure sensor, and two sonars: a downward-facing single-beam sonar and a rotating 360 degree sonar. We introduce a computationally efficient method for extracting range estimates from single-beam sonar intensity data, enabling reliable wall detection in noisy and reverberant conditions. A closed-form geometric model leverages the two sonar ranges to estimate the pipe center, and an adaptive, confidence-weighted proportional-derivative (PD) controller maintains alignment during traversal. The system requires no Doppler velocity log, external tracking, or complex multi-sensor arrays. Experiments in a submerged 46 cm-diameter pipe using a Blue Robotics BlueROV2 heavy remotely operated vehicle demonstrate stable centering and successful full-pipe traversal despite ambient flow and structural deformations. These results show that reliable in-pipe navigation and inspection can be achieved with a lightweight, computationally efficient sensing and processing architecture, advancing the practicality of autonomous underwater inspection in confined environments.

</details>


### [9] [Affordance-Aware Interactive Decision-Making and Execution for Ambiguous Instructions](https://arxiv.org/abs/2602.05273)
*Hengxuan Xu,Fengbo Lan,Zhixin Zhao,Shengjie Wang,Mengqiao Liu,Jieqian Sun,Yu Cheng,Tao Zhang*

Main category: cs.RO

TL;DR: AIDE是一个双流框架，通过交互式探索与视觉语言推理相结合，使机器人能够在模糊指令下识别任务相关物体并执行任务，在开放世界场景中优于现有VLM方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于视觉语言模型的方法在让机器人探索陌生环境并根据模糊人类指令行动时面临挑战，主要原因是推理效率低和缺乏环境交互，这阻碍了实时任务规划和执行。

Method: 提出AIDE双流框架：多阶段推理作为决策流，加速决策作为执行流，实现零样本可供性分析和模糊指令解释，将交互式探索与视觉语言推理相结合。

Result: 在仿真和真实环境中的广泛实验表明，AIDE实现了超过80%的任务规划成功率和在10Hz频率下超过95%的闭环连续执行准确率，在多样开放世界场景中优于现有VLM方法。

Conclusion: AIDE通过集成交互式探索与视觉语言推理的双流框架，有效解决了机器人处理模糊指令时的挑战，实现了高效的任务规划和执行能力。

Abstract: Enabling robots to explore and act in unfamiliar environments under ambiguous human instructions by interactively identifying task-relevant objects (e.g., identifying cups or beverages for "I'm thirsty") remains challenging for existing vision-language model (VLM)-based methods. This challenge stems from inefficient reasoning and the lack of environmental interaction, which hinder real-time task planning and execution. To address this, We propose Affordance-Aware Interactive Decision-Making and Execution for Ambiguous Instructions (AIDE), a dual-stream framework that integrates interactive exploration with vision-language reasoning, where Multi-Stage Inference (MSI) serves as the decision-making stream and Accelerated Decision-Making (ADM) as the execution stream, enabling zero-shot affordance analysis and interpretation of ambiguous instructions. Extensive experiments in simulation and real-world environments show that AIDE achieves the task planning success rate of over 80\% and more than 95\% accuracy in closed-loop continuous execution at 10 Hz, outperforming existing VLM-based methods in diverse open-world scenarios.

</details>


### [10] [Learning Soccer Skills for Humanoid Robots: A Progressive Perception-Action Framework](https://arxiv.org/abs/2602.05310)
*Jipeng Kong,Xinzhe Liu,Yuhang Lin,Jinrui Han,Sören Schwertfeger,Chenjia Bai,Xuelong Li*

Main category: cs.RO

TL;DR: PAiD提出了一种渐进式架构，将人形机器人足球技能学习分解为三个阶段：通过人体运动跟踪学习运动技能、轻量级感知-动作集成实现位置泛化、物理感知的仿真到现实迁移，在Unitree G1上实现了高保真的人形踢球能力。


<details>
  <summary>Details</summary>
Motivation: 足球对人形机器人提出了重大挑战，需要紧密集成的感知-动作能力来完成感知引导的踢球和全身平衡控制等任务。现有方法存在模块化管道中的模块间不稳定性或端到端框架中的训练目标冲突问题。

Method: 提出感知-动作集成决策（PAiD）架构，将足球技能获取分解为三个阶段：1）通过人体运动跟踪获取运动技能；2）轻量级感知-动作集成实现位置泛化；3）物理感知的仿真到现实迁移。这种分阶段分解建立了稳定的基础技能，避免了感知集成期间的奖励冲突，并最小化了仿真到现实的差距。

Result: 在Unitree G1机器人上的实验展示了高保真的人形踢球能力，在静态或滚动球、各种位置和干扰等多样化条件下表现出鲁棒性能，同时在室内和室外场景中保持一致的执行效果。

Conclusion: 这种分而治之的策略推进了鲁棒的人形机器人足球能力，并为复杂的具身技能获取提供了一个可扩展的框架。

Abstract: Soccer presents a significant challenge for humanoid robots, demanding tightly integrated perception-action capabilities for tasks like perception-guided kicking and whole-body balance control. Existing approaches suffer from inter-module instability in modular pipelines or conflicting training objectives in end-to-end frameworks. We propose Perception-Action integrated Decision-making (PAiD), a progressive architecture that decomposes soccer skill acquisition into three stages: motion-skill acquisition via human motion tracking, lightweight perception-action integration for positional generalization, and physics-aware sim-to-real transfer. This staged decomposition establishes stable foundational skills, avoids reward conflicts during perception integration, and minimizes sim-to-real gaps. Experiments on the Unitree G1 demonstrate high-fidelity human-like kicking with robust performance under diverse conditions-including static or rolling balls, various positions, and disturbances-while maintaining consistent execution across indoor and outdoor scenarios. Our divide-and-conquer strategy advances robust humanoid soccer capabilities and offers a scalable framework for complex embodied skill acquisition. The project page is available at https://soccer-humanoid.github.io/.

</details>


### [11] [RoboPaint: From Human Demonstration to Any Robot and Any View](https://arxiv.org/abs/2602.05325)
*Jiacheng Fan,Zhiyue Zhao,Yiqian Zhang,Chao Chen,Peide Wang,Hengdi Zhang,Zhengxue Cheng*

Main category: cs.RO

TL;DR: 提出Real-Sim-Real数据管道，通过人类演示生成机器人可执行训练数据，无需直接遥操作，解决VLA模型在灵巧操作中的数据瓶颈问题。


<details>
  <summary>Details</summary>
Motivation: 获取大规模高保真机器人演示数据是扩展视觉-语言-动作模型在灵巧操作中的关键瓶颈。传统遥操作方法成本高、可扩展性差，需要更高效的数据收集方案。

Method: 建立标准化数据采集室收集多模态人类演示数据，提出触觉感知重定向方法将人手状态映射到机器人灵巧手状态，在逼真仿真环境中渲染生成机器人训练数据。

Result: 重定向的灵巧手轨迹在10个多样化物体操作任务中达到84%成功率；基于生成数据训练的VLA策略在三个代表性任务中达到80%平均成功率。

Conclusion: 通过Real-Sim-Real数据管道可以从人类演示中高效"绘制"机器人训练数据，为复杂灵巧操作提供可扩展、经济高效的遥操作替代方案，性能损失最小。

Abstract: Acquiring large-scale, high-fidelity robot demonstration data remains a critical bottleneck for scaling Vision-Language-Action (VLA) models in dexterous manipulation. We propose a Real-Sim-Real data collection and data editing pipeline that transforms human demonstrations into robot-executable, environment-specific training data without direct robot teleoperation. Standardized data collection rooms are built to capture multimodal human demonstrations (synchronized 3 RGB-D videos, 11 RGB videos, 29-DoF glove joint angles, and 14-channel tactile signals). Based on these human demonstrations, we introduce a tactile-aware retargeting method that maps human hand states to robot dex-hand states via geometry and force-guided optimization. Then the retargeted robot trajectories are rendered in a photorealistic Isaac Sim environment to build robot training data. Real world experiments have demonstrated: (1) The retargeted dex-hand trajectories achieve an 84\% success rate across 10 diverse object manipulation tasks. (2) VLA policies (Pi0.5) trained exclusively on our generated data achieve 80\% average success rate on three representative tasks, i.e., pick-and-place, pushing and pouring. To conclude, robot training data can be efficiently "painted" from human demonstrations using our real-sim-real data pipeline. We offer a scalable, cost-effective alternative to teleoperation with minimal performance loss for complex dexterous manipulation.

</details>


### [12] [Benchmarking Affordance Generalization with BusyBox](https://arxiv.org/abs/2602.05441)
*Dean Fortier,Timothy Adamson,Tess Hellebrekers,Teresa LaScala,Kofi Ennin,Michael Murray,Andrey Kolobov,Galen Mullins*

Main category: cs.RO

TL;DR: BusyBox是一个用于评估视觉-语言-动作模型在物理操作中泛化能力的基准测试平台，包含6个可互换模块，挑战当前先进VLA模型的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 虽然VLA模型在视觉和语言空间的泛化能力很重要，但关键元技能是"可供性泛化"——即能够操作具有熟悉物理特征的新物体。当前缺乏系统评估VLA模型这种泛化能力的物理基准测试。

Method: 设计BusyBox物理基准测试平台，包含6个模块（开关、滑块、电线、按钮、显示屏、旋钮），这些模块可以互换和旋转以创建多种变体，具有不同视觉外观但相同可供性。提供3D打印文件和材料清单，便于在机器人实验室中构建。

Result: 实验表明，即使对于强大的开源VLA模型如π₀.₅和GR00T-N1.6，在BusyBox变体间的泛化仍然极具挑战性。团队还发布了使用Mobile Aloha机器人收集的语言标注演示数据集。

Conclusion: BusyBox为研究社区提供了一个系统评估VLA模型可供性泛化能力的物理基准测试平台，通过模块化设计和开源材料鼓励社区参与评估并提出新的泛化实验。

Abstract: Vision-Language-Action (VLA) models have been attracting the attention of researchers and practitioners thanks to their promise of generalization. Although single-task policies still offer competitive performance, VLAs are increasingly able to handle commands and environments unseen in their training set. While generalization in vision and language space is undoubtedly important for robust versatile behaviors, a key meta-skill VLAs need to possess is affordance generalization -- the ability to manipulate new objects with familiar physical features.
  In this work, we present BusyBox, a physical benchmark for systematic semi-automatic evaluation of VLAs' affordance generalization. BusyBox consists of 6 modules with switches, sliders, wires, buttons, a display, and a dial. The modules can be swapped and rotated to create a multitude of BusyBox variations with different visual appearances but the same set of affordances. We empirically demonstrate that generalization across BusyBox variants is highly challenging even for strong open-weights VLAs such as $π_{0.5}$ and GR00T-N1.6. To encourage the research community to evaluate their own VLAs on BusyBox and to propose new affordance generalization experiments, we have designed BusyBox to be easy to build in most robotics labs. We release the full set of CAD files for 3D-printing its parts as well as a bill of materials for (optionally) assembling its electronics. We also publish a dataset of language-annotated demonstrations that we collected using the common bimanual Mobile Aloha robot on the canonical BusyBox configuration. All of the released materials are available at https://microsoft.github.io/BusyBox.

</details>


### [13] [Ontology-Driven Robotic Specification Synthesis](https://arxiv.org/abs/2602.05456)
*Maksym Figat,Ryan M. Mackey,Michel D. Ingham*

Main category: cs.RO

TL;DR: RSTM2是一种本体驱动的分层方法，使用随机时间Petri网与资源，通过蒙特卡洛模拟支持机器人系统工程，特别适用于安全关键和任务关键应用。


<details>
  <summary>Details</summary>
Motivation: 解决安全关键和任务关键机器人应用中高层目标与形式化可执行规范之间的鸿沟，支持复杂多机器人系统的架构权衡、资源分配和不确定性下的性能分析。

Method: 提出RSTM2方法，基于本体驱动的分层方法，使用随机时间Petri网与资源建模，支持任务、系统和子系统级别的蒙特卡洛模拟，并利用本体概念实现可解释的AI辅助规范合成。

Result: 通过假设案例研究展示了RSTM2方法如何支持架构权衡、资源分配和不确定性下的性能分析，特别适用于NASA CADRE任务等复杂多机器人系统。

Conclusion: RSTM2方法为复杂多机器人系统提供了一种有效的系统工程方法，支持完全自主的规范合成，代表了未来去中心化、资源感知和自适应自主系统的发展方向。

Abstract: This paper addresses robotic system engineering for safety- and mission-critical applications by bridging the gap between high-level objectives and formal, executable specifications. The proposed method, Robotic System Task to Model Transformation Methodology (RSTM2) is an ontology-driven, hierarchical approach using stochastic timed Petri nets with resources, enabling Monte Carlo simulations at mission, system, and subsystem levels. A hypothetical case study demonstrates how the RSTM2 method supports architectural trades, resource allocation, and performance analysis under uncertainty. Ontological concepts further enable explainable AI-based assistants, facilitating fully autonomous specification synthesis. The methodology offers particular benefits to complex multi-robot systems, such as the NASA CADRE mission, representing decentralized, resource-aware, and adaptive autonomous systems of the future.

</details>


### [14] [TaSA: Two-Phased Deep Predictive Learning of Tactile Sensory Attenuation for Improving In-Grasp Manipulation](https://arxiv.org/abs/2602.05468)
*Pranav Ponnivalavan,Satoshi Funabashi,Alexander Schmitz,Tetsuya Ogata,Shigeki Sugano*

Main category: cs.RO

TL;DR: 提出TaSA框架，通过两阶段深度学习预测模型解决机器人手自接触与外部接触的触觉区分问题，提高灵巧操作成功率


<details>
  <summary>Details</summary>
Motivation: 人类能够进行复杂的多指接触操作，但机器人手在灵巧操作中难以区分自接触和外部接触的触觉信号，这限制了其在真实场景中的泛化能力

Method: 提出TaSA两阶段深度预测学习框架：第一阶段学习自接触动力学模型，预测机器人自身动作产生的触觉反馈；第二阶段将学习到的模型融入运动学习中，突出物体接触信号

Result: 在铅笔芯插入自动铅笔、硬币插入插槽、回形针固定在纸张等插入任务中，使用TaSA训练的策略比基线方法获得显著更高的成功率

Conclusion: 基于感觉衰减原理的结构化触觉感知对于机器人灵巧操作至关重要，TaSA框架通过区分自接触信号有效提升了复杂操作任务的性能

Abstract: Humans can achieve diverse in-hand manipulations, such as object pinching and tool use, which often involve simultaneous contact between the object and multiple fingers. This is still an open issue for robotic hands because such dexterous manipulation requires distinguishing between tactile sensations generated by their self-contact and those arising from external contact. Otherwise, object/robot breakage happens due to contacts/collisions. Indeed, most approaches ignore self-contact altogether, by constraining motion to avoid/ignore self-tactile information during contact. While this reduces complexity, it also limits generalization to real-world scenarios where self-contact is inevitable. Humans overcome this challenge through self-touch perception, using predictive mechanisms that anticipate the tactile consequences of their own motion, through a principle called sensory attenuation, where the nervous system differentiates predictable self-touch signals, allowing novel object stimuli to stand out as relevant. Deriving from this, we introduce TaSA, a two-phased deep predictive learning framework. In the first phase, TaSA explicitly learns self-touch dynamics, modeling how a robot's own actions generate tactile feedback. In the second phase, this learned model is incorporated into the motion learning phase, to emphasize object contact signals during manipulation. We evaluate TaSA on a set of insertion tasks, which demand fine tactile discrimination: inserting a pencil lead into a mechanical pencil, inserting coins into a slot, and fixing a paper clip onto a sheet of paper, with various orientations, positions, and sizes. Across all tasks, policies trained with TaSA achieve significantly higher success rates than baseline methods, demonstrating that structured tactile perception with self-touch based on sensory attenuation is critical for dexterous robotic manipulation.

</details>


### [15] [DECO: Decoupled Multimodal Diffusion Transformer for Bimanual Dexterous Manipulation with a Plugin Tactile Adapter](https://arxiv.org/abs/2602.05513)
*Xukun Li,Yu Sun,Lei Zhang,Bosheng Huang,Yibo Peng,Yuan Meng,Haojun Jiang,Shaoxuan Xie,Guacai Yao,Alois Knoll,Zhenshan Bing,Xinlong Wang,Zhenguo Sun*

Main category: cs.RO

TL;DR: DECO是一个基于DiT的策略框架，通过解耦多模态条件处理实现灵巧操作，并附带包含50小时数据的DECO-50双手灵巧操作数据集。


<details>
  <summary>Details</summary>
Motivation: 解决多模态条件（图像、动作、本体感知、触觉信号）在机器人灵巧操作中的有效整合问题，提高策略的适应性和性能。

Method: 基于DiT架构，通过联合自注意力处理图像和动作token，自适应层归一化注入本体感知状态，交叉注意力注入触觉信号，并使用轻量级LoRA适配器进行高效微调。

Result: 提出了DECO框架和DECO-50数据集，后者包含4个场景、28个子任务、50小时数据、约500万帧和8000条成功轨迹的双手灵巧操作数据。

Conclusion: DECO框架有效整合多模态信息，DECO-50数据集为灵巧操作研究提供了丰富的资源，两者共同推进了机器人灵巧操作技术的发展。

Abstract: Overview of the Proposed DECO Framework.} DECO is a DiT-based policy that decouples multimodal conditioning. Image and action tokens interact via joint self attention, while proprioceptive states and optional conditions are injected through adaptive layer normalization. Tactile signals are injected via cross attention, while a lightweight LoRA-based adapter is used to efficiently fine-tune the pretrained policy. DECO is also accompanied by DECO-50, a bimanual dexterous manipulation dataset with tactile sensing, consisting of 4 scenarios and 28 sub-tasks, covering more than 50 hours of data, approximately 5 million frames, and 8,000 successful trajectories.

</details>


### [16] [Virtual-Tube-Based Cooperative Transport Control for Multi-UAV Systems in Constrained Environments](https://arxiv.org/abs/2602.05516)
*Runxiao Liu,Pengda Mao,Xiangli Le,Shuang Gu,Yapeng Chen,Quan Quan*

Main category: cs.RO

TL;DR: 提出基于虚拟管理论和耗散系统理论的多无人机协同运输控制框架，适用于受限环境下的电缆悬挂负载运输，具有低计算开销和高稳定性


<details>
  <summary>Details</summary>
Motivation: 解决多无人机在受限环境中协同运输电缆悬挂负载的挑战，需要实现低计算开销的张力分配、协调运输，同时确保系统在复杂环境中的稳定性和鲁棒性

Method: 结合虚拟管理论和耗散系统理论，设计多无人机控制框架，动态调整无人机配置以适应障碍物布局，实现高效的协同导航

Result: 通过大量仿真验证了方法的有效性，展示了其在大规模多无人机系统中的可扩展性，并在户外场景中进行了实验验证，证明了实际可行性和现实条件下的鲁棒性

Conclusion: 该控制框架成功实现了多无人机在受限环境中的高效协同运输，具有低计算开销、高稳定性和实际可行性，为复杂环境下的多无人机协同操作提供了有效解决方案

Abstract: This paper proposes a novel control framework for cooperative transportation of cable-suspended loads by multiple unmanned aerial vehicles (UAVs) operating in constrained environments. Leveraging virtual tube theory and principles from dissipative systems theory, the framework facilitates efficient multi-UAV collaboration for navigating obstacle-rich areas. The proposed framework offers several key advantages. (1) It achieves tension distribution and coordinated transportation within the UAV-cable-load system with low computational overhead, dynamically adapting UAV configurations based on obstacle layouts to facilitate efficient navigation. (2) By integrating dissipative systems theory, the framework ensures high stability and robustness, essential for complex multi-UAV operations. The effectiveness of the proposed approach is validated through extensive simulations, demonstrating its scalability for large-scale multi-UAV systems. Furthermore, the method is experimentally validated in outdoor scenarios, showcasing its practical feasibility and robustness under real-world conditions.

</details>


### [17] [VLN-Pilot: Large Vision-Language Model as an Autonomous Indoor Drone Operator](https://arxiv.org/abs/2602.05552)
*Bessie Dominguez-Dager,Sergio Suescun-Ferrandiz,Felix Escalona,Francisco Gomez-Donoso,Miguel Cazorla*

Main category: cs.RO

TL;DR: VLN-Pilot是一个基于大型视觉语言模型(VLLM)的室内无人机导航框架，让VLLM充当人类飞行员角色，通过自然语言指令和视觉感知实现GPS拒止环境下的自主导航。


<details>
  <summary>Details</summary>
Motivation: 传统基于规则或几何路径规划的方法需要大量任务特定工程，缺乏语义理解和上下文感知能力。需要开发更智能、更人性化的无人机控制方法，减少操作员工作量，提高室内环境下的安全性和任务灵活性。

Method: 利用VLLM的多模态推理能力，将自由形式的自然语言指令与视觉观察相结合，进行空间关系推理、障碍物避让和动态事件响应，实现语义理解和上下文感知的飞行行为规划。

Result: 在定制的逼真室内仿真基准测试中，VLLM驱动的智能体在复杂指令跟随任务上取得了高成功率，包括具有多个语义目标的长时程导航任务。

Conclusion: VLLM为基础的飞行员有望替代远程无人机操作员，显著减少操作员工作量，同时提高室内受限环境下的安全性和任务灵活性，为检查、搜救、设施监控等任务提供可扩展、人性化的无人机控制方案。

Abstract: This paper introduces VLN-Pilot, a novel framework in which a large Vision-and-Language Model (VLLM) assumes the role of a human pilot for indoor drone navigation. By leveraging the multimodal reasoning abilities of VLLMs, VLN-Pilot interprets free-form natural language instructions and grounds them in visual observations to plan and execute drone trajectories in GPS-denied indoor environments. Unlike traditional rule-based or geometric path-planning approaches, our framework integrates language-driven semantic understanding with visual perception, enabling context-aware, high-level flight behaviors with minimal task-specific engineering. VLN-Pilot supports fully autonomous instruction-following for drones by reasoning about spatial relationships, obstacle avoidance, and dynamic reactivity to unforeseen events. We validate our framework on a custom photorealistic indoor simulation benchmark and demonstrate the ability of the VLLM-driven agent to achieve high success rates on complex instruction-following tasks, including long-horizon navigation with multiple semantic targets. Experimental results highlight the promise of replacing remote drone pilots with a language-guided autonomous agent, opening avenues for scalable, human-friendly control of indoor UAVs in tasks such as inspection, search-and-rescue, and facility monitoring. Our results suggest that VLLM-based pilots may dramatically reduce operator workload while improving safety and mission flexibility in constrained indoor environments.

</details>


### [18] [TOLEBI: Learning Fault-Tolerant Bipedal Locomotion via Online Status Estimation and Fallibility Rewards](https://arxiv.org/abs/2602.05596)
*Hokyun Lee,Woo-Jeong Baek,Junhyeok Cha,Jaeheung Park*

Main category: cs.RO

TL;DR: TOLEBI：首个基于学习的双足机器人容错框架，通过模拟故障训练和在线关节状态监测，实现硬件故障下的稳定行走


<details>
  <summary>Details</summary>
Motivation: 现有双足机器人强化学习研究主要关注正常状态下的行走性能，但忽视了实际应用中可能发生的硬件故障（如关节锁定、电源故障）和环境干扰。这些故障在现实世界中可能导致严重后果，因此需要开发能够处理运行时故障的容错方法。

Method: 提出TOLEBI框架：1）在仿真环境中注入关节锁定、电源故障和外部干扰等故障，训练能够容忍这些故障的行走策略；2）通过仿真到现实的迁移将策略部署到真实机器人；3）集成在线关节状态模块，根据运行时实际观测数据实时分类关节状态。

Result: 在人形机器人TOCABI上进行了仿真和真实世界验证实验，证明了所提方法的适用性。该框架能够有效处理硬件故障，维持双足行走的稳定性。

Conclusion: 这是首个基于学习的双足机器人容错框架，通过故障注入训练和在线状态监测，实现了硬件故障下的鲁棒行走，推动了该领域高效学习方法的发展。

Abstract: With the growing employment of learning algorithms in robotic applications, research on reinforcement learning for bipedal locomotion has become a central topic for humanoid robotics. While recently published contributions achieve high success rates in locomotion tasks, scarce attention has been devoted to the development of methods that enable to handle hardware faults that may occur during the locomotion process. However, in real-world settings, environmental disturbances or sudden occurrences of hardware faults might yield severe consequences. To address these issues, this paper presents TOLEBI (A faulT-tOlerant Learning framEwork for Bipedal locomotIon) that handles faults on the robot during operation. Specifically, joint locking, power loss and external disturbances are injected in simulation to learn fault-tolerant locomotion strategies. In addition to transferring the learned policy to the real robot via sim-to-real transfer, an online joint status module incorporated. This module enables to classify joint conditions by referring to the actual observations at runtime under real-world conditions. The validation experiments conducted both in real-world and simulation with the humanoid robot TOCABI highlight the applicability of the proposed approach. To our knowledge, this manuscript provides the first learning-based fault-tolerant framework for bipedal locomotion, thereby fostering the development of efficient learning methods in this field.

</details>


### [19] [HiCrowd: Hierarchical Crowd Flow Alignment for Dense Human Environments](https://arxiv.org/abs/2602.05608)
*Yufei Zhu,Shih-Min Yang,Martin Magnusson,Allan Wang*

Main category: cs.RO

TL;DR: HiCrowd：分层框架结合强化学习和模型预测控制，利用行人运动作为引导，解决机器人在密集人群中的冻结问题


<details>
  <summary>Details</summary>
Motivation: 解决机器人在密集人群中的导航挑战，特别是"冻结机器人问题"——机器人难以找到安全运动路径而被困在人群中

Method: 提出HiCrowd分层框架：高层RL策略生成跟随点，使机器人与合适的行人群体对齐；低层MPC安全跟踪引导，进行短期规划

Result: 在真实世界数据集和合成人群数据集上评估，离线（回放记录轨迹）和在线（模拟中行人响应机器人）设置中，在导航效率、安全性方面优于基准方法，减少冻结行为

Conclusion: 将人类运动作为引导而非仅视为动态障碍物，为机器人在人群中的安全高效导航提供了有力原则

Abstract: Navigating through dense human crowds remains a significant challenge for mobile robots. A key issue is the freezing robot problem, where the robot struggles to find safe motions and becomes stuck within the crowd. To address this, we propose HiCrowd, a hierarchical framework that integrates reinforcement learning (RL) with model predictive control (MPC). HiCrowd leverages surrounding pedestrian motion as guidance, enabling the robot to align with compatible crowd flows. A high-level RL policy generates a follow point to align the robot with a suitable pedestrian group, while a low-level MPC safely tracks this guidance with short horizon planning. The method combines long-term crowd aware decision making with safe short-term execution. We evaluate HiCrowd against reactive and learning-based baselines in offline setting (replaying recorded human trajectories) and online setting (human trajectories are updated to react to the robot in simulation). Experiments on a real-world dataset and a synthetic crowd dataset show that our method outperforms in navigation efficiency and safety, while reducing freezing behaviors. Our results suggest that leveraging human motion as guidance, rather than treating humans solely as dynamic obstacles, provides a powerful principle for safe and efficient robot navigation in crowds.

</details>


### [20] [From Vision to Decision: Neuromorphic Control for Autonomous Navigation and Tracking](https://arxiv.org/abs/2602.05683)
*Chuwei Wang,Eduardo Sebastián,Amanda Prorok,Anastasia Bizyaeva*

Main category: cs.RO

TL;DR: 提出一种简约的神经形态控制框架，通过动态神经元群体将视觉目标激发直接转换为自我中心运动指令，利用动态分岔机制解决目标对称性导致的决策困难问题。


<details>
  <summary>Details</summary>
Motivation: 机器人导航长期面临反应式传感器控制与模型规划器决策能力之间的协调难题，特别是在多个目标选项均等时，反应式系统难以打破对称性而不依赖计算密集的规划器。

Method: 采用神经形态控制框架，将机载摄像头图像像素编码为动态神经元群体的输入，直接将视觉目标激发转换为自我中心运动指令。引入动态分岔机制，延迟决策直到环境几何诱导的临界点出现。

Result: 在仿真环境和实验四旋翼平台上验证了该方法的有效性，实现了实时自主导航，计算负担小，参数少且可解释，并能与特定应用的图像处理流程无缝集成。

Conclusion: 该神经形态控制器成功弥合了反应式控制与模型规划之间的鸿沟，为视觉引导导航和跟踪提供了简约高效的解决方案，受动物认知和意见动态机制模型启发，具有实际应用价值。

Abstract: Robotic navigation has historically struggled to reconcile reactive, sensor-based control with the decisive capabilities of model-based planners. This duality becomes critical when the absence of a predominant option among goals leads to indecision, challenging reactive systems to break symmetries without computationally-intense planners. We propose a parsimonious neuromorphic control framework that bridges this gap for vision-guided navigation and tracking. Image pixels from an onboard camera are encoded as inputs to dynamic neuronal populations that directly transform visual target excitation into egocentric motion commands. A dynamic bifurcation mechanism resolves indecision by delaying commitment until a critical point induced by the environmental geometry. Inspired by recently proposed mechanistic models of animal cognition and opinion dynamics, the neuromorphic controller provides real-time autonomy with a minimal computational burden, a small number of interpretable parameters, and can be seamlessly integrated with application-specific image processing pipelines. We validate our approach in simulation environments as well as on an experimental quadrotor platform.

</details>


### [21] [Scalable and General Whole-Body Control for Cross-Humanoid Locomotion](https://arxiv.org/abs/2602.05791)
*Yufei Xue,YunFeng Lin,Wentao Dong,Yang Tang,Jingbo Wang,Jiangmiao Pang,Ming Zhou,Minghuan Liu,Weinan Zhang*

Main category: cs.RO

TL;DR: 提出XHugWBC框架，实现单一策略跨多种人形机器人设计的通用控制，无需为每个机器人单独训练


<details>
  <summary>Details</summary>
Motivation: 现有基于学习的全身控制器大多需要针对特定机器人进行训练，缺乏跨不同机器人设计的通用性

Method: 提出XHugWBC框架，包含：(1)物理一致的形态随机化，(2)跨不同人形机器人的语义对齐观测和动作空间，(3)建模形态和动力学特性的有效策略架构

Result: 在12个模拟人形机器人和7个真实机器人上实验，证明了所得通用控制器的强大泛化能力和鲁棒性

Conclusion: 通过一次性训练学习广泛形态和动力学分布，策略获得强大的结构偏置，支持对未见机器人的零样本迁移

Abstract: Learning-based whole-body controllers have become a key driver for humanoid robots, yet most existing approaches require robot-specific training. In this paper, we study the problem of cross-embodiment humanoid control and show that a single policy can robustly generalize across a wide range of humanoid robot designs with one-time training. We introduce XHugWBC, a novel cross-embodiment training framework that enables generalist humanoid control through: (1) physics-consistent morphological randomization, (2) semantically aligned observation and action spaces across diverse humanoid robots, and (3) effective policy architectures modeling morphological and dynamical properties. XHugWBC is not tied to any specific robot. Instead, it internalizes a broad distribution of morphological and dynamical characteristics during training. By learning motion priors from diverse randomized embodiments, the policy acquires a strong structural bias that supports zero-shot transfer to previously unseen robots. Experiments on twelve simulated humanoids and seven real-world robots demonstrate the strong generalization and robustness of the resulting universal controller.

</details>


### [22] [A Hybrid Autoencoder for Robust Heightmap Generation from Fused Lidar and Depth Data for Humanoid Robot Locomotion](https://arxiv.org/abs/2602.05855)
*Dennis Bank,Joost Cordes,Thomas Seel,Simon F. G. Ehlers*

Main category: cs.RO

TL;DR: 提出基于学习的框架，使用机器人中心高度图表示，通过混合编码器-解码器结构融合多模态传感器数据，提升地形感知精度和时序一致性


<details>
  <summary>Details</summary>
Motivation: 传统系统依赖手动设计的单传感器管道，在非结构化、以人为中心的环境中部署人形机器人需要可靠的地形感知

Method: 采用混合编码器-解码器结构，CNN提取空间特征，GRU核心保证时序一致性，融合Intel RealSense深度相机、LIVOX MID-360 LiDAR（通过球面投影处理）和IMU的多模态数据

Result: 多模态融合比仅使用深度相机提高7.2%重建精度，比仅使用LiDAR提高9.9%；集成3.2秒时序上下文减少建图漂移

Conclusion: 提出的学习框架通过多模态传感器融合和时序建模，显著提升了人形机器人在非结构化环境中的地形感知能力

Abstract: Reliable terrain perception is a critical prerequisite for the deployment of humanoid robots in unstructured, human-centric environments. While traditional systems often rely on manually engineered, single-sensor pipelines, this paper presents a learning-based framework that uses an intermediate, robot-centric heightmap representation. A hybrid Encoder-Decoder Structure (EDS) is introduced, utilizing a Convolutional Neural Network (CNN) for spatial feature extraction fused with a Gated Recurrent Unit (GRU) core for temporal consistency. The architecture integrates multimodal data from an Intel RealSense depth camera, a LIVOX MID-360 LiDAR processed via efficient spherical projection, and an onboard IMU. Quantitative results demonstrate that multimodal fusion improves reconstruction accuracy by 7.2% over depth-only and 9.9% over LiDAR-only configurations. Furthermore, the integration of a 3.2 s temporal context reduces mapping drift.

</details>


### [23] [Residual Reinforcement Learning for Waste-Container Lifting Using Large-Scale Cranes with Underactuated Tools](https://arxiv.org/abs/2602.05895)
*Qi Li,Karsten Berns*

Main category: cs.RO

TL;DR: 论文提出了一种残差强化学习方法，用于城市环境中液压装载起重机执行垃圾箱提升任务，结合名义控制器和学习的残差策略来提高精度和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 城市环境中垃圾箱回收任务的容器提升阶段需要高精度操作，起重机吊钩与容器环之间的几何公差很小，需要精确的轨迹跟踪和摆动抑制，但未建模的动力学和参数变化会影响性能。

Method: 采用残差强化学习方法，结合名义笛卡尔控制器（使用导纳控制进行轨迹跟踪和摆锤感知的摆动阻尼，配合阻尼最小二乘逆运动学）和PPO训练的残差策略，通过随机化初始化和域随机化增强泛化能力。

Result: 仿真结果显示，相比单独使用名义控制器，该方法提高了跟踪精度、减少了振荡、提升了提升成功率。

Conclusion: 残差强化学习方法能有效补偿未建模动力学和参数变化，在保持名义控制器稳定性的同时提高了垃圾箱提升任务的精度和鲁棒性。

Abstract: This paper studies the container lifting phase of a waste-container recycling task in urban environments, performed by a hydraulic loader crane equipped with an underactuated discharge unit, and proposes a residual reinforcement learning (RRL) approach that combines a nominal Cartesian controller with a learned residual policy. All experiments are conducted in simulation, where the task is characterized by tight geometric tolerances between the discharge-unit hooks and the container rings relative to the overall crane scale, making precise trajectory tracking and swing suppression essential. The nominal controller uses admittance control for trajectory tracking and pendulum-aware swing damping, followed by damped least-squares inverse kinematics with a nullspace posture term to generate joint velocity commands. A PPO-trained residual policy in Isaac Lab compensates for unmodeled dynamics and parameter variations, improving precision and robustness without requiring end-to-end learning from scratch. We further employ randomized episode initialization and domain randomization over payload properties, actuator gains, and passive joint parameters to enhance generalization. Simulation results demonstrate improved tracking accuracy, reduced oscillations, and higher lifting success rates compared to the nominal controller alone.

</details>


### [24] [From Bench to Flight: Translating Drone Impact Tests into Operational Safety Limits](https://arxiv.org/abs/2602.05922)
*Aziz Mohamed Mili,Louis Catar,Paul Gérard,Ilyass Tabiai,David St-Onge*

Main category: cs.RO

TL;DR: 开发了一个端到端的开源工具链，可将台式碰撞测试转化为可部署的无人机安全控制器，通过数据驱动模型将碰撞前速度映射到冲击力，在线执行速度限制以保障室内无人机在人类附近的安全操作。


<details>
  <summary>Details</summary>
Motivation: 室内微型飞行器(MAV)越来越多地用于需要接近人员的任务，但从业者缺乏基于实测碰撞风险调整运动限制的实用方法。需要建立从碰撞测试到运行时限制的实用桥梁。

Method: 1. 设计紧凑可复制的碰撞测试装置和协议，捕获不同无人机类别和接触表面的力-时间曲线；2. 提供数据驱动模型，将碰撞前速度映射到冲击力和接触持续时间；3. 发布脚本和ROS2节点，在线执行速度限制并记录合规性，支持特定设施策略。

Result: 在多个商用现成四旋翼无人机和代表性室内资产上验证了工作流程，证明推导的控制器在满足安全利益相关者指定的力约束的同时，保持了任务吞吐量。

Conclusion: 该研究提供了一个实用的桥梁，将实测碰撞转化为运行时限制，包含可共享的数据集、代码和可重复流程，团队可采用这些工具来认证室内MAV在人类附近的操作。

Abstract: Indoor micro-aerial vehicles (MAVs) are increasingly used for tasks that require close proximity to people, yet practitioners lack practical methods to tune motion limits based on measured impact risk. We present an end-to-end, open toolchain that converts benchtop impact tests into deployable safety governors for drones. First, we describe a compact and replicable impact rig and protocol for capturing force-time profiles across drone classes and contact surfaces. Second, we provide data-driven models that map pre-impact speed to impulse and contact duration, enabling direct computation of speed bounds for a target force limit. Third, we release scripts and a ROS2 node that enforce these bounds online and log compliance, with support for facility-specific policies. We validate the workflow on multiple commercial off-the-shelf quadrotors and representative indoor assets, demonstrating that the derived governors preserve task throughput while meeting force constraints specified by safety stakeholders. Our contribution is a practical bridge from measured impacts to runtime limits, with shareable datasets, code, and a repeatable process that teams can adopt to certify indoor MAV operations near humans.

</details>


### [25] [Visuo-Tactile World Models](https://arxiv.org/abs/2602.06001)
*Carolina Higuera,Sergio Arnaud,Byron Boots,Mustafa Mukadam,Francois Robert Hogan,Franziska Meier*

Main category: cs.RO

TL;DR: VT-WM通过结合视觉和触觉感知构建世界模型，在接触丰富的机器人操作任务中提升物理保真度，改善物体恒存性和运动规律遵循，并在零样本真实机器人实验中显著提高成功率。


<details>
  <summary>Details</summary>
Motivation: 纯视觉模型在遮挡或接触状态模糊时存在物体消失、瞬移或违反物理规律等失败模式，需要结合触觉感知来更好地理解机器人-物体交互的接触物理。

Method: 提出多任务视觉-触觉世界模型(VT-WM)，通过触觉推理捕捉接触物理，在接触丰富的操作任务集上进行训练，结合视觉和触觉感知来理解机器人-物体交互。

Result: VT-WM在想象中提升物理保真度：物体恒存性提高33%，运动规律遵循性提高29%；零样本真实机器人实验中成功率提高达35%，在接触丰富的多步任务中提升最大；展示下游任务适应性，仅需少量演示即可在新任务中实现可靠规划。

Conclusion: 结合视觉和触觉感知的世界模型能更好地捕捉接触物理，提升机器人操作的物理保真度和规划性能，在接触丰富的任务中表现优异，并具有良好的任务适应性。

Abstract: We introduce multi-task Visuo-Tactile World Models (VT-WM), which capture the physics of contact through touch reasoning. By complementing vision with tactile sensing, VT-WM better understands robot-object interactions in contact-rich tasks, avoiding common failure modes of vision-only models under occlusion or ambiguous contact states, such as objects disappearing, teleporting, or moving in ways that violate basic physics. Trained across a set of contact-rich manipulation tasks, VT-WM improves physical fidelity in imagination, achieving 33% better performance at maintaining object permanence and 29% better compliance with the laws of motion in autoregressive rollouts. Moreover, experiments show that grounding in contact dynamics also translates to planning. In zero-shot real-robot experiments, VT-WM achieves up to 35% higher success rates, with the largest gains in multi-step, contact-rich tasks. Finally, VT-WM demonstrates significant downstream versatility, effectively adapting its learned contact dynamics to a novel task and achieving reliable planning success with only a limited set of demonstrations.

</details>


### [26] [CommCP: Efficient Multi-Agent Coordination via LLM-Based Communication with Conformal Prediction](https://arxiv.org/abs/2602.06038)
*Xiaopan Zhang,Zejin Wang,Zhixu Li,Jianpeng Yao,Jiachen Li*

Main category: cs.RO

TL;DR: 该论文提出了CommCP框架，这是一个基于LLM的去中心化通信系统，用于解决多智能体多任务具身问答问题，通过共形预测校准消息来提升通信可靠性。


<details>
  <summary>Details</summary>
Motivation: 现实世界中机器人需要协作完成人类自然语言指令，这要求机器人能够解释命令、生成问题进行场景理解、并操作目标物体。多异构机器人协作需要有效的信息收集，而现有研究对多智能体多任务具身问答中的通信协调问题关注不足。

Method: 提出CommCP框架：1）将信息收集过程形式化为MM-EQA问题；2）采用基于LLM的去中心化通信架构；3）使用共形预测来校准生成的消息，减少接收者分心并提高通信可靠性。

Result: 实验结果表明，CommCP在任务成功率和探索效率方面显著优于基线方法。作者还创建了一个包含多样化、照片级真实家庭场景的MM-EQA基准数据集。

Conclusion: CommCP框架有效解决了多智能体协作中的通信协调问题，通过共形预测校准消息提升了通信可靠性，为机器人协作任务中的信息收集提供了有效解决方案。

Abstract: To complete assignments provided by humans in natural language, robots must interpret commands, generate and answer relevant questions for scene understanding, and manipulate target objects. Real-world deployments often require multiple heterogeneous robots with different manipulation capabilities to handle different assignments cooperatively. Beyond the need for specialized manipulation skills, effective information gathering is important in completing these assignments. To address this component of the problem, we formalize the information-gathering process in a fully cooperative setting as an underexplored multi-agent multi-task Embodied Question Answering (MM-EQA) problem, which is a novel extension of canonical Embodied Question Answering (EQA), where effective communication is crucial for coordinating efforts without redundancy. To address this problem, we propose CommCP, a novel LLM-based decentralized communication framework designed for MM-EQA. Our framework employs conformal prediction to calibrate the generated messages, thereby minimizing receiver distractions and enhancing communication reliability. To evaluate our framework, we introduce an MM-EQA benchmark featuring diverse, photo-realistic household scenarios with embodied questions. Experimental results demonstrate that CommCP significantly enhances the task success rate and exploration efficiency over baselines. The experiment videos, code, and dataset are available on our project website: https://comm-cp.github.io.

</details>
