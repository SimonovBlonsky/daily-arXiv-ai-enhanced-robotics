<div id=toc></div>

# Table of Contents

- [cs.RO](#cs.RO) [Total: 30]


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [1] [Beyond the Vehicle: Cooperative Localization by Fusing Point Clouds for GPS-Challenged Urban Scenarios](https://arxiv.org/abs/2602.03908)
*Kuo-Yi Chao,Ralph Rasshofer,Alois Christian Knoll*

Main category: cs.RO

TL;DR: 提出一种融合V2V和V2I数据的协同多传感器多模态定位方法，通过点云配准SLAM算法提升城市环境中GPS不可靠时的车辆定位精度


<details>
  <summary>Details</summary>
Motivation: 城市环境中GPS信号不可靠，需要解决车辆精确定位的挑战性问题

Method: 融合车辆间(V2V)和车辆与基础设施(V2I)的协同数据，结合点云配准SLAM算法，整合车载LiDAR、立体相机和路口部署传感器生成的点云数据

Result: 通过利用基础设施共享数据，在复杂GPS噪声城市场景中显著提高了定位精度和鲁棒性

Conclusion: 协同多传感器多模态定位方法能够有效解决城市环境中GPS不可靠时的车辆定位问题

Abstract: Accurate vehicle localization is a critical challenge in urban environments where GPS signals are often unreliable. This paper presents a cooperative multi-sensor and multi-modal localization approach to address this issue by fusing data from vehicle-to-vehicle (V2V) and vehicle-to-infrastructure (V2I) systems. Our approach integrates cooperative data with a point cloud registration-based simultaneous localization and mapping (SLAM) algorithm. The system processes point clouds generated from diverse sensor modalities, including vehicle-mounted LiDAR and stereo cameras, as well as sensors deployed at intersections. By leveraging shared data from infrastructure, our method significantly improves localization accuracy and robustness in complex, GPS-noisy urban scenarios.

</details>


### [2] [How Users Understand Robot Foundation Model Performance through Task Success Rates and Beyond](https://arxiv.org/abs/2602.03920)
*Isaac Sheidlower,Jindan Huang,James Staley,Bingyu Wu,Qicong Chen,Reuben Aronson,Elaine Short*

Main category: cs.RO

TL;DR: 研究非机器人专家如何理解机器人基础模型评估中的任务成功率信息，发现他们不仅正确使用该指标，还高度关注评估中通常不报告的其他信息类型。


<details>
  <summary>Details</summary>
Motivation: 随着机器人基础模型的发展，用户可能会要求机器人执行模型未训练或评估过的任务。在这种情况下，用户理解尝试新任务的风险至关重要，因为机器人失败的成本相对较高。了解用户如何解读RFM评估中的性能信息，特别是任务成功率这一主要指标，对于确保用户安全有效地使用机器人非常重要。

Method: 通过用户研究，让非机器人专家查看真实的RFM评估数据，包括任务成功率、失败案例描述和来自多个已发表RFM研究项目的视频。研究用户如何解读这些信息，特别是任务成功率的使用方式。

Result: 研究发现非专家不仅以与专家预期一致的方式使用任务成功率，还高度评价其他类型的信息，如RFM评估中通常不报告的失败案例。此外，用户希望同时获得RFM先前评估的真实数据和机器人对执行新任务表现的估计。

Conclusion: 机器人基础模型的评估报告应该包含更多类型的信息，特别是失败案例，以帮助非专家用户更好地理解机器人的能力和限制。用户需要综合的历史评估数据和当前任务表现预测来做出明智决策。

Abstract: Robot Foundation Models (RFMs) represent a promising approach to developing general-purpose home robots. Given the broad capabilities of RFMs, users will inevitably ask an RFM-based robot to perform tasks that the RFM was not trained or evaluated on. In these cases, it is crucial that users understand the risks associated with attempting novel tasks due to the relatively high cost of failure. Furthermore, an informed user who understands an RFM's capabilities will know what situations and tasks the robot can handle. In this paper, we study how non-roboticists interpret performance information from RFM evaluations. These evaluations typically report task success rate (TSR) as the primary performance metric. While TSR is intuitive to experts, it is necessary to validate whether novices also use this information as intended. Toward this end, we conducted a study in which users saw real evaluation data, including TSR, failure case descriptions, and videos from multiple published RFM research projects. The results highlight that non-experts not only use TSR in a manner consistent with expert expectations but also highly value other information types, such as failure cases that are not often reported in RFM evaluations. Furthermore, we find that users want access to both real data from previous evaluations of the RFM and estimates from the robot about how well it will do on a novel task.

</details>


### [3] [VLS: Steering Pretrained Robot Policies via Vision-Language Models](https://arxiv.org/abs/2602.03973)
*Shuo Liu,Ishneet Sukhvinder Singh,Yiqing Xu,Jiafei Duan,Ranjay Krishna*

Main category: cs.RO

TL;DR: VLS是一个无需训练、推理时适应的框架，通过视觉语言模型合成轨迹可微的奖励函数，引导预训练扩散或流匹配策略的采样过程，以应对测试时的空间和任务变化。


<details>
  <summary>Details</summary>
Motivation: 预训练的扩散或流匹配策略在面对障碍物、支撑面偏移或轻微杂乱等测试时分布偏移时容易失败，这些失败反映了模仿学习在训练-测试偏移下的局限性。重新训练或微调成本高昂且概念上不匹配，因为所需的行为已经存在但无法在测试时有选择地适应。

Method: 提出Vision-Language Steering (VLS)框架，将适应视为推理时的控制问题，通过视觉语言模型合成轨迹可微的奖励函数，在不修改策略参数的情况下，引导预训练扩散或流匹配策略的采样过程，使其响应分布外观察-语言输入。

Result: 在仿真和真实世界评估中，VLS始终优于先前的引导方法，在CALVIN上实现了31%的改进，在LIBERO-PRO上获得了13%的提升。在Franka机器人上的真实世界部署进一步展示了在测试时空间和语义偏移下的鲁棒推理时适应能力。

Conclusion: VLS提供了一种无需训练、推理时适应的方法，能够有效应对预训练生成机器人策略在测试时分布偏移下的失败问题，通过视觉语言引导实现了对空间配置和任务规范的灵活适应。

Abstract: Why do pretrained diffusion or flow-matching policies fail when the same task is performed near an obstacle, on a shifted support surface, or amid mild clutter? Such failures rarely reflect missing motor skills; instead, they expose a limitation of imitation learning under train-test shifts, where action generation is tightly coupled to training-specific spatial configurations and task specifications. Retraining or fine-tuning to address these failures is costly and conceptually misaligned, as the required behaviors already exist but cannot be selectively adapted at test time. We propose Vision-Language Steering (VLS), a training-free framework for inference-time adaptation of frozen generative robot policies. VLS treats adaptation as an inference-time control problem, steering the sampling process of a pretrained diffusion or flow-matching policy in response to out-of-distribution observation-language inputs without modifying policy parameters. By leveraging vision-language models to synthesize trajectory-differentiable reward functions, VLS guides denoising toward action trajectories that satisfy test-time spatial and task requirements. Across simulation and real-world evaluations, VLS consistently outperforms prior steering methods, achieving a 31% improvement on CALVIN and a 13% gain on LIBERO-PRO. Real-world deployment on a Franka robot further demonstrates robust inference-time adaptation under test-time spatial and semantic shifts. Project page: https://vision-language-steering.github.io/webpage/

</details>


### [4] [Efficient Long-Horizon Vision-Language-Action Models via Static-Dynamic Disentanglement](https://arxiv.org/abs/2602.03983)
*Weikang Qiu,Tinglin Huang,Aosong Feng,Rex Ying*

Main category: cs.RO

TL;DR: SD-VLA：通过将视觉输入解耦为静态和动态token来优化视觉-语言-动作模型，显著减少上下文长度并重用KV缓存，实现高效推理和长时程建模


<details>
  <summary>Details</summary>
Motivation: 现有VLA模型面临两个主要挑战：1）长时程上下文建模能力有限；2）由于二次注意力复杂性和大量参数导致的推理效率低下。观察到轨迹中大部分视觉信息在时间步之间保持静态（如背景），因此可以利用这一特性进行优化

Method: 提出SD-VLA框架，将视觉输入解耦为多级静态和动态token：1）跨帧保留静态token的单一副本以显著减少上下文长度；2）通过轻量级重缓存门仅在必要时更新静态token的KV缓存，实现高效多帧集成和推理加速

Result: 1）在新提出的长时程时序依赖建模基准上，SD-VLA比基线模型绝对成功率提升39.8%；2）在SimplerEnv基准上获得3.9%的性能增益；3）在相同基准上实现2.26倍推理加速

Conclusion: SD-VLA通过解耦静态和动态视觉token，有效解决了VLA模型的长时程建模和推理效率问题，在性能和速度方面均有显著提升，为实际部署提供了更实用的解决方案

Abstract: Vision-Language-Action (VLA) models have recently emerged as a promising paradigm for generalist robotic control. Built upon vision-language model (VLM) architectures, VLAs predict actions conditioned on visual observations and language instructions, achieving strong performance and generalization across tasks. However, VLAs face two major challenges: limited long-horizon context and inefficient inference due to the quadratic attention complexity and large parameter counts. Our work is motivated by the observation that much of the visual information in a trajectory remains static across timesteps (e.g., the background). Leveraging this property, we propose SD-VLA, a framework that disentangles visual inputs into multi-level static and dynamic tokens, which enables (1) retaining a single copy of static tokens across frames to significantly reduce context length, and (2) reusing the key-value (KV) cache of static tokens through a lightweight recache gate that updates only when necessary. This design enables efficient multi-frame integration and efficient inference. In addition, we introduce a new benchmark that more effectively evaluates the long-horizon temporal dependency modeling ability of VLAs. Experimental results show that our approach outperforms baselines on this benchmark by 39.8% absolute improvement in success rate, and achieves a 3.9% gain on the SimplerEnv benchmark. Moreover, SD-VLA delivers a 2.26x inference speedup over the base VLA model on the same benchmark, enabling faster and more practical real-world deployment.

</details>


### [5] [An Anatomy-specific Guidewire Shaping Robot for Improved Vascular Navigation](https://arxiv.org/abs/2602.04050)
*Aabha Tamhankar,Jay Patil,Giovanni Pittiglio*

Main category: cs.RO

TL;DR: 开发了一种用于神经血管介入手术的导丝成形机器人系统，能够根据导航需求自动成形导丝尖端，提高手术标准化和可重复性。


<details>
  <summary>Details</summary>
Motivation: 神经血管介入手术中，导丝成形依赖外科医生的经验和手工操作，在复杂解剖结构中尤其困难且结果不一致。需要开发标准化、自动化的导丝成形技术来减少对专家经验的依赖。

Method: 开发了一个台式导丝成形机器人系统，能够根据目标形状生成机器人动作指令。通过实验数据校准模型，将期望的导丝形状映射为机器人动作。系统能够产生临床常见的尖端几何形状（C形、S形、角度形、钩形）。

Result: 机器人能够成功产生临床常见的导丝尖端几何形状，模型预测形状与实验结果相比的均方根误差为0.56mm。展示了3D尖端成形能力，以及从岩骨段颈内动脉到后交通动脉的复杂腔内导航能力。

Conclusion: 该机器人系统能够实现标准化、自主的导丝成形，减少对专家经验的依赖，提高神经血管介入手术的可重复性和安全性，特别是在复杂解剖结构中。

Abstract: Neuroendovascular access often relies on passive microwires that are hand-shaped at the back table and then used to track a microcatheter to the target. Neuroendovascular surgeons determine the shape of the wire by examining the patient pre-operative images and using their experience to identify anatomy specific shapes of the wire that would facilitate reaching the target. This procedure is particularly complex in convoluted anatomical structures and is heavily dependent on the level of expertise of the surgeon. Towards enabling standardized autonomous shaping, we present a bench-top guidewire shaping robot capable of producing navigation-specific desired wire configurations. We present a model that can map the desired wire shape into robot actions, calibrated using experimental data. We show that the robot can produce clinically common tip geometries (C, S, Angled, Hook) and validate them with respect to the model-predicted shapes in 2D. Our model predicts the shape with a Root Mean Square (RMS) error of 0.56mm across all shapes when compared to the experimental results. We also demonstrate 3D tip shaping capabilities and the ability to traverse complex endoluminal navigation from the petrous Internal Carotid Artery (ICA) to the Posterior Communicating Artery (PComm).

</details>


### [6] [Control and State Estimation of Vehicle-Mounted Aerial Systems in GPS-Denied, Non-Inertial Environments](https://arxiv.org/abs/2602.04057)
*Riming Xu,Obadah Wali,Yasmine Marani,Eric Feron*

Main category: cs.RO

TL;DR: 提出了一种在GNSS拒止、非惯性环境中运行的无人机鲁棒控制与估计框架，使用外部位置测量结合EKF-UI估计平台运动，无需依赖IMU和GNSS。


<details>
  <summary>Details</summary>
Motivation: 在GNSS拒止、非惯性环境中，传统惯性传感器（如IMU）因平台诱导加速度而不可靠，导致无法区分加速度来源，造成漂移和控制性能下降。

Method: 采用基于外部位置测量的EKF-UI（带未知输入的扩展卡尔曼滤波器）来估计平台运动，结合级联PID控制器实现完整3D跟踪，使用高精度运动捕捉系统进行测试。

Result: 实验验证表明，相比标准EKF，该方法在X轴和Y轴平移扰动下显著提高了稳定性和轨迹跟踪性能，无需惯性反馈即可在移动平台上部署。

Conclusion: 该方法为在卡车、电梯等移动平台上部署无人机提供了实用解决方案，能够在非惯性环境中实现鲁棒控制，克服了传统方法对IMU和GNSS的依赖。

Abstract: We present a robust control and estimation framework for quadrotors operating in Global Navigation Satellite System(GNSS)-denied, non-inertial environments where inertial sensors such as Inertial Measurement Units (IMUs) become unreliable due to platform-induced accelerations. In such settings, conventional estimators fail to distinguish whether the measured accelerations arise from the quadrotor itself or from the non-inertial platform, leading to drift and control degradation. Unlike conventional approaches that depend heavily on IMU and GNSS, our method relies exclusively on external position measurements combined with a Extended Kalman Filter with Unknown Inputs (EKF-UI) to account for platform motion. The estimator is paired with a cascaded PID controller for full 3D tracking. To isolate estimator performance from localization errors, all tests are conducted using high-precision motion capture systems. Experimental results in a moving-cart testbed validate our approach under both translational in X-axis and Y-axis dissonance. Compared to standard EKF, the proposed method significantly improves stability and trajectory tracking without requiring inertial feedback, enabling practical deployment on moving platforms such as trucks or elevators.

</details>


### [7] [Comparative Analysis of Autonomous Robotic and Manual Techniques for Ultrasonic Sacral Osteotomy: A Preliminary Study](https://arxiv.org/abs/2602.04076)
*Daniyal Maroufi,Yash Kulkarni,Justin E. Bird,Jeffrey H. Siewerdsen,Farshid Alambeigi*

Main category: cs.RO

TL;DR: 本文介绍了一种自主超声骶骨截骨机器人系统，通过对比手动与机器人操作，证明机器人系统在轨迹精度和深度控制方面显著优于手动操作。


<details>
  <summary>Details</summary>
Motivation: 手动骶骨截骨手术存在轨迹精度不足和深度控制困难的问题，可能导致过度穿透等安全风险。需要开发更精确、安全的机器人辅助系统来克服这些限制。

Method: 开发了集成超声骨刀、七自由度机械臂和光学跟踪系统的自主机器人系统。在相同截骨条件下，使用Sawbones模型对比手动超声骶骨截骨（MUSO）和机器人超声骶骨截骨（RUSO）的多方向控制性能。

Result: RUSO系统轨迹精度达到亚毫米级（0.11 mm RMSE），比MUSO（1.10 mm RMSE）提高一个数量级。深度控制方面，MUSO出现显著过度穿透（16.0 mm vs 8.0 mm目标），而RUSO保持精确控制（8.1 mm）。

Conclusion: 机器人手术能有效克服手动截骨的关键限制，为更安全、更精确的骶骨切除手术奠定了基础。

Abstract: In this paper, we introduce an autonomous Ultrasonic Sacral Osteotomy (USO) robotic system that integrates an ultrasonic osteotome with a seven-degree-of-freedom (DoF) robotic manipulator guided by an optical tracking system. To assess multi-directional control along both the surface trajectory and cutting depth of this system, we conducted quantitative comparisons between manual USO (MUSO) and robotic USO (RUSO) in Sawbones phantoms under identical osteotomy conditions. The RUSO system achieved sub-millimeter trajectory accuracy (0.11 mm RMSE), an order of magnitude improvement over MUSO (1.10 mm RMSE). Moreover, MUSO trials showed substantial over-penetration (16.0 mm achieved vs. 8.0 mm target), whereas the RUSO system maintained precise depth control (8.1 mm). These results demonstrate that robotic procedures can effectively overcome the critical limitations of manual osteotomy, establishing a foundation for safer and more precise sacral resections.

</details>


### [8] [KGLAMP: Knowledge Graph-guided Language model for Adaptive Multi-robot Planning and Replanning](https://arxiv.org/abs/2602.04129)
*Chak Lam Shek,Faizan M. Tariq,Sangjae Bae,David Isele,Piyush Gupta*

Main category: cs.RO

TL;DR: KGLAMP是一个基于知识图谱引导的LLM规划框架，用于异构多机器人团队的协调规划，通过结构化知识图谱提高PDDL问题规范的准确性，并在动态环境中保持计划一致性。


<details>
  <summary>Details</summary>
Motivation: 异构多机器人系统在长期任务中需要协调，但现有规划方法难以构建准确的符号表示并在动态环境中保持计划一致性。传统PDDL规划器需要手动构建符号模型，而基于LLM的规划器往往忽略机器人异构性和环境不确定性。

Method: 提出KGLAMP框架，维护一个结构化知识图谱，编码对象关系、空间可达性和机器人能力。该知识图谱作为持久化、动态更新的记忆，指导LLM生成准确的PDDL问题规范，并在检测到不一致时触发重新规划。

Result: 在MAT-THOR基准测试中，KGLAMP相比纯LLM和基于PDDL的变体，性能提升至少25.5%。

Conclusion: KGLAMP通过知识图谱引导的LLM规划，有效解决了异构多机器人系统在动态环境中的规划问题，提高了规划准确性和适应性。

Abstract: Heterogeneous multi-robot systems are increasingly deployed in long-horizon missions that require coordination among robots with diverse capabilities. However, existing planning approaches struggle to construct accurate symbolic representations and maintain plan consistency in dynamic environments. Classical PDDL planners require manually crafted symbolic models, while LLM-based planners often ignore agent heterogeneity and environmental uncertainty. We introduce KGLAMP, a knowledge-graph-guided LLM planning framework for heterogeneous multi-robot teams. The framework maintains a structured knowledge graph encoding object relations, spatial reachability, and robot capabilities, which guides the LLM in generating accurate PDDL problem specifications. The knowledge graph serves as a persistent, dynamically updated memory that incorporates new observations and triggers replanning upon detecting inconsistencies, enabling symbolic plans to adapt to evolving world states. Experiments on the MAT-THOR benchmark show that KGLAMP improves performance by at least 25.5% over both LLM-only and PDDL-based variants.

</details>


### [9] [Shaping Expressiveness in Robotics: The Role of Design Tools in Crafting Embodied Robot Movements](https://arxiv.org/abs/2602.04137)
*Elisabetta Zibetti,Alexandra Mercader,Hélène Duval,Florent Levillain,Audrey Rochette,David St-Onge*

Main category: cs.RO

TL;DR: 本文提出了一种面向机器人手臂表达性运动的设计教学法，通过跨学科工作坊和工具集，帮助工程师设计更具表现力和互动性的机器人动作。


<details>
  <summary>Details</summary>
Motivation: 随着机器人越来越多地进入人类共享空间，其运动需要超越基本功能，融入表达性品质以增强互动和沟通能力。当前工程师在设计表达性机器人运动方面缺乏有效的方法和工具。

Method: 采用基于跨学科方法的实践性互动工作坊，整合舞蹈分析框架，开发了定制的手动遥控器和专用动画软件，支持实时操控、可视化、详细运动序列和精确参数控制。

Result: 定性分析表明，提出的"工具箱"有效弥合了人类意图与机器人表达性之间的鸿沟，产生了更直观、更具吸引力的表达性机器人手臂运动。

Conclusion: 该运动中心设计教学法为工程师提供了创建表达性机器人运动的方法和工具，通过跨学科整合和交互式设计过程，显著提升了机器人运动的表达性和互动性。

Abstract: As robots increasingly become part of shared human spaces, their movements must transcend basic functionality by incorporating expressive qualities to enhance engagement and communication. This paper introduces a movement-centered design pedagogy designed to support engineers in creating expressive robotic arm movements. Through a hands-on interactive workshop informed by interdisciplinary methodologies, participants explored various creative possibilities, generating valuable insights into expressive motion design. The iterative approach proposed integrates analytical frameworks from dance, enabling designers to examine motion through dynamic and embodied dimensions. A custom manual remote controller facilitates interactive, real-time manipulation of the robotic arm, while dedicated animation software supports visualization, detailed motion sequencing, and precise parameter control. Qualitative analysis of this interactive design process reveals that the proposed "toolbox" effectively bridges the gap between human intent and robotic expressiveness resulting in more intuitive and engaging expressive robotic arm movements.

</details>


### [10] [MA3DSG: Multi-Agent 3D Scene Graph Generation for Large-Scale Indoor Environments](https://arxiv.org/abs/2602.04152)
*Yirum Kim,Jaewoo Kim,Ue-Hwan Kim*

Main category: cs.RO

TL;DR: 提出了首个多智能体3D场景图生成框架MA3DSG，通过无训练图对齐算法合并多个智能体的局部查询图，并创建了支持多样化配置的基准测试MA3DSG-Bench。


<details>
  <summary>Details</summary>
Motivation: 当前3D场景图生成方法严重依赖单智能体假设和小规模环境，在扩展到真实世界场景时存在局限性。需要解决多智能体协作下的可扩展性问题。

Method: 开发了无训练图对齐算法，能够高效地将各个智能体的部分查询图合并为统一的全局场景图。该方法使传统单智能体系统能够协作工作，无需任何可学习参数。

Result: 提出了MA3DSG模型和MA3DSG-Bench基准测试，支持多样化的智能体配置、领域大小和环境条件，为可扩展的多智能体3D场景图生成研究奠定了基础。

Conclusion: 这项工作为可扩展的多智能体3D场景图生成研究奠定了坚实基础，通过多智能体协作和无训练图对齐算法解决了现有方法的可扩展性限制。

Abstract: Current 3D scene graph generation (3DSGG) approaches heavily rely on a single-agent assumption and small-scale environments, exhibiting limited scalability to real-world scenarios. In this work, we introduce Multi-Agent 3D Scene Graph Generation (MA3DSG) model, the first framework designed to tackle this scalability challenge using multiple agents. We develop a training-free graph alignment algorithm that efficiently merges partial query graphs from individual agents into a unified global scene graph. Leveraging extensive analysis and empirical insights, our approach enables conventional single-agent systems to operate collaboratively without requiring any learnable parameters. To rigorously evaluate 3DSGG performance, we propose MA3DSG-Bench-a benchmark that supports diverse agent configurations, domain sizes, and environmental conditions-providing a more general and extensible evaluation framework. This work lays a solid foundation for scalable, multi-agent 3DSGG research.

</details>


### [11] [A Modern System Recipe for Situated Embodied Human-Robot Conversation with Real-Time Multimodal LLMs and Tool-Calling](https://arxiv.org/abs/2602.04157)
*Dong Won Lee,Sarah Gillet,Louis-Philippe Morency,Cynthia Breazeal,Hae Won Park*

Main category: cs.RO

TL;DR: 提出一个简单的最小系统配方，将实时多模态语言模型与注意力工具接口配对，用于需要频繁注意力转移的居家场景中的具身对话


<details>
  <summary>Details</summary>
Motivation: 具身对话需要机器人在严格延迟约束下，将实时对话与主动感知（决定看什么、何时看、说什么）交织进行，这是一个具有挑战性的问题

Method: 采用一个简单的系统配方，将实时多模态语言模型与一小套用于注意力和主动感知的工具接口配对，研究六个需要频繁注意力转移和增加感知范围的居家场景

Result: 评估了四个系统变体，在回合级工具决策正确性方面与人类标注进行比较，并收集交互质量的主观评分，结果表明实时多模态大语言模型和用于主动感知的工具使用是一个有前景的方向

Conclusion: 实时多模态大语言模型与主动感知工具使用的结合，为实用的具身对话系统提供了一个有前景的研究方向

Abstract: Situated embodied conversation requires robots to interleave real-time dialogue with active perception: deciding what to look at, when to look, and what to say under tight latency constraints. We present a simple, minimal system recipe that pairs a real-time multimodal language model with a small set of tool interfaces for attention and active perception. We study six home-style scenarios that require frequent attention shifts and increasing perceptual scope. Across four system variants, we evaluate turn-level tool-decision correctness against human annotations and collect subjective ratings of interaction quality. Results indicate that real-time multimodal large language models and tool use for active perception is a promising direction for practical situated embodied conversation.

</details>


### [12] [SCALE: Self-uncertainty Conditioned Adaptive Looking and Execution for Vision-Language-Action Models](https://arxiv.org/abs/2602.04208)
*Hyeonbeom Choi,Daechul Ahn,Youhan Lee,Taewook Kang,Seongwon Cho,Jonghyun Choi*

Main category: cs.RO

TL;DR: SCALE是一种无需额外训练、验证器或多次前向传播的推理策略，通过"自我不确定性"联合调制视觉感知和动作，在单次前向传播中实现自适应执行，提升VLA模型的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有VLA模型的测试时扩展方法需要额外训练、验证器和多次前向传播，不实用且仅在动作解码阶段干预，无法处理感知模糊性。需要一种更简单有效的方法来同时调整感知和动作。

Method: 提出SCALE推理策略，基于主动推理理论中的不确定性驱动探索思想，利用"自我不确定性"联合调制视觉感知和动作。无需额外训练、验证器，仅需单次前向传播。在高不确定性时扩大感知和动作探索，在置信时聚焦利用。

Result: 在模拟和真实世界基准测试中，SCALE提升了最先进的VLA模型性能，优于现有测试时扩展方法，同时保持单次前向传播的效率。

Conclusion: SCALE为VLA模型提供了一种简单有效的推理策略，通过联合调制感知和动作来应对不确定性，实现了实用部署和自适应执行。

Abstract: Vision-Language-Action (VLA) models have emerged as a promising paradigm for general-purpose robotic control, with test-time scaling (TTS) gaining attention to enhance robustness beyond training. However, existing TTS methods for VLAs require additional training, verifiers, and multiple forward passes, making them impractical for deployment. Moreover, they intervene only at action decoding while keeping visual representations fixed-insufficient under perceptual ambiguity, where reconsidering how to perceive is as important as deciding what to do. To address these limitations, we propose SCALE, a simple inference strategy that jointly modulates visual perception and action based on 'self-uncertainty', inspired by uncertainty-driven exploration in Active Inference theory-requiring no additional training, no verifier, and only a single forward pass. SCALE broadens exploration in both perception and action under high uncertainty, while focusing on exploitation when confident-enabling adaptive execution across varying conditions. Experiments on simulated and real-world benchmarks demonstrate that SCALE improves state-of-the-art VLAs and outperforms existing TTS methods while maintaining single-pass efficiency.

</details>


### [13] [ALORE: Autonomous Large-Object Rearrangement with a Legged Manipulator](https://arxiv.org/abs/2602.04214)
*Zhihai Bi,Yushan Zhang,Kai Chen,Guoyang Zhao,Yulin Li,Jun Ma*

Main category: cs.RO

TL;DR: ALORE是一个用于腿式机器人的自主大物体重排系统，能够高效重排各种大型物体，通过分层强化学习、统一交互配置表示和任务-运动规划框架实现多物体环境中的稳定操作。


<details>
  <summary>Details</summary>
Motivation: 让机器人能够重排各种大型重型物体（如家具）可以大大减轻人类的工作负担，但这一任务极具挑战性，因为需要与多样物体交互、在复杂环境中高效重排多个物体，同时确保无碰撞的全身操作。

Method: 系统采用分层强化学习训练管道，包含高层物体速度控制器和低层全身控制器；引入统一交互配置表示和物体速度估计器模块；开发任务-运动规划框架，联合优化物体访问顺序和物体-目标分配。

Result: 与强基线相比，在策略泛化、物体速度跟踪精度和多物体重排效率方面表现一致优越。成功完成8个连续循环重排32把椅子近40分钟无失败，执行约40米长距离自主重排。

Conclusion: ALORE系统通过分层学习、统一表示和联合规划，实现了腿式机器人在复杂环境中对各种大型物体的高效、稳定自主重排，展示了强大的鲁棒性和实际应用潜力。

Abstract: Endowing robots with the ability to rearrange various large and heavy objects, such as furniture, can substantially alleviate human workload. However, this task is extremely challenging due to the need to interact with diverse objects and efficiently rearrange multiple objects in complex environments while ensuring collision-free loco-manipulation. In this work, we present ALORE, an autonomous large-object rearrangement system for a legged manipulator that can rearrange various large objects across diverse scenarios. The proposed system is characterized by three main features: (i) a hierarchical reinforcement learning training pipeline for multi-object environment learning, where a high-level object velocity controller is trained on top of a low-level whole-body controller to achieve efficient and stable joint learning across multiple objects; (ii) two key modules, a unified interaction configuration representation and an object velocity estimator, that allow a single policy to regulate planar velocity of diverse objects accurately; and (iii) a task-and-motion planning framework that jointly optimizes object visitation order and object-to-target assignment, improving task efficiency while enabling online replanning. Comparisons against strong baselines show consistent superiority in policy generalization, object-velocity tracking accuracy, and multi-object rearrangement efficiency. Key modules are systematically evaluated, and extensive simulations and real-world experiments are conducted to validate the robustness and effectiveness of the entire system, which successfully completes 8 continuous loops to rearrange 32 chairs over nearly 40 minutes without a single failure, and executes long-distance autonomous rearrangement over an approximately 40 m route. The open-source packages are available at https://zhihaibi.github.io/Alore/.

</details>


### [14] [OAT: Ordered Action Tokenization](https://arxiv.org/abs/2602.04215)
*Chaoqi Liu,Xiaoshen Han,Jiawei Gao,Yue Zhao,Haonan Chen,Yilun Du*

Main category: cs.RO

TL;DR: 提出Ordered Action Tokenization (OAT)方法，为连续机器人动作提供高效的自回归建模tokenization方案


<details>
  <summary>Details</summary>
Motivation: 自回归策略为机器人学习提供了可扩展的基础，但应用于连续动作时需要有效的动作tokenization方案。现有方法要么产生过长的token序列，要么缺乏结构，限制了与下一token预测的兼容性。

Method: 提出OAT方法，通过transformer with registers、有限标量量化和排序诱导训练机制，将动作块离散化为有序的token序列，满足高压缩率、完全可解码性和左到右因果有序token空间三个要求。

Result: 在超过20个任务（涵盖四个仿真基准和真实世界场景）中，配备OAT的自回归策略始终优于先前的tokenization方案和基于扩散的基线方法，同时在推理时提供更大的灵活性。

Conclusion: OAT为连续机器人动作提供了一种有效的tokenization方案，使自回归策略能够更好地平衡推理成本和动作保真度，在多种任务中表现出优越性能。

Abstract: Autoregressive policies offer a compelling foundation for scalable robot learning by enabling discrete abstraction, token-level reasoning, and flexible inference. However, applying autoregressive modeling to continuous robot actions requires an effective action tokenization scheme. Existing approaches either rely on analytical discretization methods that produce prohibitively long token sequences, or learned latent tokenizers that lack structure, limiting their compatibility with next-token prediction. In this work, we identify three desiderata for action tokenization - high compression, total decodability, and a left-to-right causally ordered token space - and introduce Ordered Action Tokenization (OAT), a learned action tokenizer that satisfies all three. OAT discretizes action chunks into an ordered sequence of tokens using transformer with registers, finite scalar quantization, and ordering-inducing training mechanisms. The resulting token space aligns naturally with autoregressive generation and enables prefix-based detokenization, yielding an anytime trade-off between inference cost and action fidelity. Across more than 20 tasks spanning four simulation benchmarks and real-world settings, autoregressive policies equipped with OAT consistently outperform prior tokenization schemes and diffusion-based baselines, while offering significantly greater flexibility at inference time.

</details>


### [15] [Reshaping Action Error Distributions for Reliable Vision-Language-Action Models](https://arxiv.org/abs/2602.04228)
*Shuanghao Bai,Dakai Wang,Cheng Chi,Wanqi Zhou,Jing Lyu,Xiaoguang Zhao,Pengwei Wang,Zhongyuan Wang,Lei Xing,Shanghang Zhang,Badong Chen*

Main category: cs.RO

TL;DR: 该论文提出在连续动作的视觉-语言-动作模型中引入最小误差熵目标，替代传统的均方误差回归，以改善动作误差分布并提升模型性能


<details>
  <summary>Details</summary>
Motivation: 现有VLA模型主要使用交叉熵和均方误差等监督目标，这些方法对单个预测施加了强点约束。研究者希望超越传统的MSE回归，通过重塑训练过程中的动作误差分布来改进连续动作VLA模型

Method: 基于信息论原理，将最小误差熵引入现代VLA架构，提出了轨迹级MEE目标及其两种加权变体，并与MSE结合用于连续动作VLA训练

Result: 在标准、少样本和噪声设置下，对多个代表性VLA架构进行评估，使用LIBERO和SimplerEnv等仿真基准以及真实世界机器人操作任务。实验结果显示在成功率、鲁棒性方面获得一致改进，在数据不平衡情况下增益持续存在，且训练成本增加可忽略，不影响推理效率

Conclusion: MEE监督在连续动作VLA模型中有效，提供了理论分析解释其有效性并描述其实际适用范围，为VLA训练提供了新的监督范式

Abstract: In robotic manipulation, vision-language-action (VLA) models have emerged as a promising paradigm for learning generalizable and scalable robot policies. Most existing VLA frameworks rely on standard supervised objectives, typically cross-entropy for discrete actions and mean squared error (MSE) for continuous action regression, which impose strong pointwise constraints on individual predictions. In this work, we focus on continuous-action VLA models and move beyond conventional MSE-based regression by reshaping action error distributions during training. Drawing on information-theoretic principles, we introduce Minimum Error Entropy (MEE) into modern VLA architectures and propose a trajectory-level MEE objective, together with two weighted variants, combined with MSE for continuous-action VLA training. We evaluate our approaches across standard, few-shot, and noisy settings on multiple representative VLA architectures, using simulation benchmarks such as LIBERO and SimplerEnv as well as real-world robotic manipulation tasks. Experimental results demonstrate consistent improvements in success rates and robustness across these settings. Under imbalanced data regimes, the gains persist within a well-characterized operating range, while incurring negligible additional training cost and no impact on inference efficiency. We further provide theoretical analyses that explain why MEE-based supervision is effective and characterize its practical range. Project Page: https://cognition2actionlab.github.io/VLA-TMEE.github.io/

</details>


### [16] [GeoLanG: Geometry-Aware Language-Guided Grasping with Unified RGB-D Multimodal Learning](https://arxiv.org/abs/2602.04231)
*Rui Tang,Guankun Wang,Long Bai,Huxin Gao,Jiewen Lai,Chi Kit Ng,Jiazheng Wang,Fan Zhang,Hongliang Ren*

Main category: cs.RO

TL;DR: GeoLanG是一个基于CLIP架构的端到端多任务框架，通过深度引导几何模块和自适应密集通道集成，在复杂遮挡和低纹理场景中实现鲁棒的语言引导抓取。


<details>
  <summary>Details</summary>
Motivation: 现有语言引导抓取方法通常采用多阶段流程，分离物体感知和抓取，导致跨模态融合有限、计算冗余，在杂乱、遮挡或低纹理场景中泛化能力差。

Method: 1. 基于CLIP架构构建端到端多任务框架，将视觉和语言输入统一到共享表示空间；2. 深度引导几何模块将深度信息转换为显式几何先验并注入注意力机制；3. 自适应密集通道集成平衡多层特征的贡献，生成更具区分性和泛化能力的视觉表示。

Result: 在OCID-VLG数据集以及仿真和真实硬件上的大量实验表明，GeoLanG能够在复杂杂乱环境中实现精确鲁棒的语言引导抓取。

Conclusion: GeoLanG为现实世界以人为中心的场景中实现更可靠的多模态机器人操作铺平了道路。

Abstract: Language-guided grasping has emerged as a promising paradigm for enabling robots to identify and manipulate target objects through natural language instructions, yet it remains highly challenging in cluttered or occluded scenes. Existing methods often rely on multi-stage pipelines that separate object perception and grasping, which leads to limited cross-modal fusion, redundant computation, and poor generalization in cluttered, occluded, or low-texture scenes. To address these limitations, we propose GeoLanG, an end-to-end multi-task framework built upon the CLIP architecture that unifies visual and linguistic inputs into a shared representation space for robust semantic alignment and improved generalization. To enhance target discrimination under occlusion and low-texture conditions, we explore a more effective use of depth information through the Depth-guided Geometric Module (DGGM), which converts depth into explicit geometric priors and injects them into the attention mechanism without additional computational overhead. In addition, we propose Adaptive Dense Channel Integration, which adaptively balances the contributions of multi-layer features to produce more discriminative and generalizable visual representations. Extensive experiments on the OCID-VLG dataset, as well as in both simulation and real-world hardware, demonstrate that GeoLanG enables precise and robust language-guided grasping in complex, cluttered environments, paving the way toward more reliable multimodal robotic manipulation in real-world human-centric settings.

</details>


### [17] [Viewpoint Matters: Dynamically Optimizing Viewpoints with Masked Autoencoder for Visual Manipulation](https://arxiv.org/abs/2602.04243)
*Pengfei Yi,Yifan Han,Junyan Li,Litao Liu,Wenzhao Lian*

Main category: cs.RO

TL;DR: MAE-Select：基于预训练多视角掩码自编码器表示的单摄像头机器人主动视角选择框架，无需标注视角数据即可动态选择信息最丰富的视角


<details>
  <summary>Details</summary>
Motivation: 当前模仿学习方法依赖固定摄像头设置，限制了系统的适应性和覆盖范围。受人类主动感知启发，需要动态调整视角以获取最相关、噪声最少的信息

Method: 提出MAE-Select框架，利用预训练的多视角掩码自编码器表示，在每个时间块动态选择下一个信息最丰富的视角，无需标注视角数据

Result: 实验表明MAE-Select提升了单摄像头系统的能力，在某些情况下甚至超越了多摄像头设置

Conclusion: MAE-Select为单摄像头机器人系统提供了一种有效的主动视角选择方法，提高了系统的适应性和性能

Abstract: Robotic manipulation continues to be a challenge, and imitation learning (IL) enables robots to learn tasks from expert demonstrations. Current IL methods typically rely on fixed camera setups, where cameras are manually positioned in static locations, imposing significant limitations on adaptability and coverage. Inspired by human active perception, where humans dynamically adjust their viewpoint to capture the most relevant and least noisy information, we propose MAE-Select, a novel framework for active viewpoint selection in single-camera robotic systems. MAE-Select fully leverages pre-trained multi-view masked autoencoder representations and dynamically selects the next most informative viewpoint at each time chunk without requiring labeled viewpoints. Extensive experiments demonstrate that MAE-Select improves the capabilities of single-camera systems and, in some cases, even surpasses multi-camera setups. The project will be available at https://mae-select.github.io.

</details>


### [18] [Towards Next-Generation SLAM: A Survey on 3DGS-SLAM Focusing on Performance, Robustness, and Future Directions](https://arxiv.org/abs/2602.04251)
*Li Wang,Ruixuan Gong,Yumo Han,Lei Yang,Lu Yang,Ying Li,Bin Xu,Huaping Liu,Rong Fu*

Main category: cs.RO

TL;DR: 这篇综述系统回顾了将3D高斯泼溅(3DGS)与SLAM技术融合的关键方法，分析了在渲染质量、跟踪精度、重建速度和内存消耗四个维度的性能优化，并探讨了在复杂环境下的鲁棒性增强技术。


<details>
  <summary>Details</summary>
Motivation: 传统SLAM系统存在渲染质量粗糙、场景细节恢复不足、动态环境下鲁棒性差等局限性。3DGS凭借其高效的显式表示和高质量渲染能力，为SLAM提供了新的重建范式，需要系统梳理相关技术进展。

Method: 通过全面综述将3DGS与SLAM集成的关键技术方法，从四个关键维度分析代表性方法的性能优化：渲染质量、跟踪精度、重建速度和内存消耗，深入探讨其设计原理和突破点。同时研究在运动模糊和动态环境等复杂场景下增强3DGS-SLAM鲁棒性的方法。

Result: 系统梳理了3DGS-SLAM领域的技术发展现状，分析了各种方法的性能特点和优化策略，为研究人员提供了全面的技术参考，推动了高保真、高效、鲁棒的下一代SLAM系统的发展。

Conclusion: 3DGS为SLAM系统带来了新的重建范式，通过综述现有技术方法，分析了性能优化和鲁棒性增强策略，为未来高保真、高效、鲁棒的下一代SLAM系统发展提供了技术参考和发展方向。

Abstract: Traditional Simultaneous Localization and Mapping (SLAM) systems often face limitations including coarse rendering quality, insufficient recovery of scene details, and poor robustness in dynamic environments. 3D Gaussian Splatting (3DGS), with its efficient explicit representation and high-quality rendering capabilities, offers a new reconstruction paradigm for SLAM. This survey comprehensively reviews key technical approaches for integrating 3DGS with SLAM. We analyze performance optimization of representative methods across four critical dimensions: rendering quality, tracking accuracy, reconstruction speed, and memory consumption, delving into their design principles and breakthroughs. Furthermore, we examine methods for enhancing the robustness of 3DGS-SLAM in complex environments such as motion blur and dynamic environments. Finally, we discuss future challenges and development trends in this area. This survey aims to provide a technical reference for researchers and foster the development of next-generation SLAM systems characterized by high fidelity, efficiency, and robustness.

</details>


### [19] [AppleVLM: End-to-end Autonomous Driving with Advanced Perception and Planning-Enhanced Vision-Language Models](https://arxiv.org/abs/2602.04256)
*Yuxuan Han,Kunyuan Wu,Qianyi Shao,Renxiang Xiao,Zilu Wang,Cansen Jiang,Yi Xiao,Liang Hu,Yunjiang Lou*

Main category: cs.RO

TL;DR: AppleVLM是一个用于端到端自动驾驶的先进视觉语言模型，通过改进感知和规划模块来解决现有VLM方法的局限性，在CARLA基准测试中达到SOTA性能，并在真实AGV平台上成功部署。


<details>
  <summary>Details</summary>
Motivation: 现有基于视觉语言模型的端到端自动驾驶方法面临车道感知不理想、语言理解偏差以及难以处理极端情况等挑战，需要更鲁棒的感知和规划能力来应对多样化和未见过的驾驶场景。

Method: 提出AppleVLM模型，包含三个核心创新：1) 使用可变形transformer融合多视角多时间步的空间-时间信息的视觉编码器；2) 引入专门的规划模态编码鸟瞰图空间信息；3) 通过分层思维链微调的VLM解码器集成视觉、语言和规划特征输出驾驶路径点。

Result: 在CARLA基准测试的闭环实验中达到最先进的驾驶性能，并在AGV平台上成功展示了复杂户外环境中的真实端到端自动驾驶能力。

Conclusion: AppleVLM通过改进的感知和规划增强机制，有效解决了现有VLM方法的局限性，为端到端自动驾驶提供了更鲁棒和可泛化的解决方案，并在模拟和真实环境中都验证了其有效性。

Abstract: End-to-end autonomous driving has emerged as a promising paradigm integrating perception, decision-making, and control within a unified learning framework. Recently, Vision-Language Models (VLMs) have gained significant attention for their potential to enhance the robustness and generalization of end-to-end driving models in diverse and unseen scenarios. However, existing VLM-based approaches still face challenges, including suboptimal lane perception, language understanding biases, and difficulties in handling corner cases. To address these issues, we propose AppleVLM, an advanced perception and planning-enhanced VLM model for robust end-to-end driving. AppleVLM introduces a novel vision encoder and a planning strategy encoder to improve perception and decision-making. Firstly, the vision encoder fuses spatial-temporal information from multi-view images across multiple timesteps using a deformable transformer mechanism, enhancing robustness to camera variations and facilitating scalable deployment across different vehicle platforms. Secondly, unlike traditional VLM-based approaches, AppleVLM introduces a dedicated planning modality that encodes explicit Bird's-Eye-View spatial information, mitigating language biases in navigation instructions. Finally, a VLM decoder fine-tuned by a hierarchical Chain-of-Thought integrates vision, language, and planning features to output robust driving waypoints. We evaluate AppleVLM in closed-loop experiments on two CARLA benchmarks, achieving state-of-the-art driving performance. Furthermore, we deploy AppleVLM on an AGV platform and successfully showcase real-world end-to-end autonomous driving in complex outdoor environments.

</details>


### [20] [Quantile Transfer for Reliable Operating Point Selection in Visual Place Recognition](https://arxiv.org/abs/2602.04401)
*Dhyey Manish Rajani,Michael Milford,Tobias Fischer*

Main category: cs.RO

TL;DR: 提出一种自动选择视觉地点识别系统阈值的方法，通过量化归一化技术将校准阈值转移到部署环境，在满足用户定义精度要求下最大化召回率。


<details>
  <summary>Details</summary>
Motivation: 传统视觉地点识别系统需要手动调整图像匹配阈值，且阈值在部署时固定不变，导致环境变化时性能下降。需要一种能自动适应新环境并满足特定精度要求的方法。

Method: 使用带有已知对应关系的小型校准遍历数据，通过量化归一化技术将相似度得分分布的阈值从校准环境转移到部署环境，确保阈值在不同校准规模和查询子集下的稳定性。

Result: 在多个先进VPR技术和数据集上的实验表明，该方法在保持高精度操作机制下，召回率比现有方法提高达25%，且能适应新环境和泛化到不同操作条件。

Conclusion: 该方法通过自动阈值选择消除了手动调参需求，在满足用户精度要求的同时最大化召回率，提高了视觉地点识别系统在环境变化下的鲁棒性和性能。

Abstract: Visual Place Recognition (VPR) is a key component for localisation in GNSS-denied environments, but its performance critically depends on selecting an image matching threshold (operating point) that balances precision and recall. Thresholds are typically hand-tuned offline for a specific environment and fixed during deployment, leading to degraded performance under environmental change. We propose a method that, given a user-defined precision requirement, automatically selects the operating point of a VPR system to maximise recall. The method uses a small calibration traversal with known correspondences and transfers thresholds to deployment via quantile normalisation of similarity score distributions. This quantile transfer ensures that thresholds remain stable across calibration sizes and query subsets, making the method robust to sampling variability. Experiments with multiple state-of-the-art VPR techniques and datasets show that the proposed approach consistently outperforms the state-of-the-art, delivering up to 25% higher recall in high-precision operating regimes. The method eliminates manual tuning by adapting to new environments and generalising across operating conditions. Our code will be released upon acceptance.

</details>


### [21] [HoRD: Robust Humanoid Control via History-Conditioned Reinforcement Learning and Online Distillation](https://arxiv.org/abs/2602.04412)
*Puyue Wang,Jiawei Hu,Yan Gao,Junyan Wang,Yu Zhang,Gillian Dobbie,Tao Gu,Wafa Johal,Ting Dang,Hong Jia*

Main category: cs.RO

TL;DR: HoRD是一个两阶段学习框架，通过历史条件强化学习训练教师策略，再通过在线蒸馏将鲁棒控制能力转移到基于Transformer的学生策略中，实现人形机器人在未见域上的零样本适应。


<details>
  <summary>Details</summary>
Motivation: 人形机器人在动力学、任务规范或环境设置发生微小变化时，性能会显著下降。现有方法通常需要针对每个新领域重新训练，缺乏对未见域的鲁棒适应能力。

Method: 1. 训练高性能教师策略：使用历史条件强化学习，策略从最近的状态-动作轨迹推断潜在动力学上下文，在线适应多样随机化动力学。
2. 在线蒸馏：将教师的鲁棒控制能力转移到基于Transformer的学生策略中，学生策略基于稀疏的根相对3D关节关键点轨迹运行。

Result: HoRD在鲁棒性和迁移性方面优于强基线方法，特别是在未见域和外部扰动情况下。该框架使单个策略能够零样本适应未见域，无需针对每个领域重新训练。

Conclusion: HoRD通过结合历史条件适应和在线蒸馏，为人形机器人控制提供了一个鲁棒的框架，能够在未见域上实现零样本适应，提高了人形机器人在动态变化环境中的实用性和可靠性。

Abstract: Humanoid robots can suffer significant performance drops under small changes in dynamics, task specifications, or environment setup. We propose HoRD, a two-stage learning framework for robust humanoid control under domain shift. First, we train a high-performance teacher policy via history-conditioned reinforcement learning, where the policy infers latent dynamics context from recent state--action trajectories to adapt online to diverse randomized dynamics. Second, we perform online distillation to transfer the teacher's robust control capabilities into a transformer-based student policy that operates on sparse root-relative 3D joint keypoint trajectories. By combining history-conditioned adaptation with online distillation, HoRD enables a single policy to adapt zero-shot to unseen domains without per-domain retraining. Extensive experiments show HoRD outperforms strong baselines in robustness and transfer, especially under unseen domains and external perturbations. Code and project page are available at \href{https://tonywang-0517.github.io/hord/}{https://tonywang-0517.github.io/hord/}.

</details>


### [22] [Integrated Exploration and Sequential Manipulation on Scene Graph with LLM-based Situated Replanning](https://arxiv.org/abs/2602.04419)
*Heqing Yang,Ziyuan Jiao,Shu Wang,Yida Niu,Si Liu,Hangxin Liu*

Main category: cs.RO

TL;DR: EPoG是一个基于场景图的探索式顺序操作规划框架，通过结合图全局规划和LLM局部规划，在部分已知环境中实现高效的机器人探索与任务执行。


<details>
  <summary>Details</summary>
Motivation: 在部分已知环境中，机器人需要同时进行探索获取信息和任务规划以实现高效执行。现有方法难以无缝结合探索与顺序操作规划。

Method: 提出EPoG框架：1）使用图全局规划器与基于LLM的局部规划器；2）通过观察和LLM预测持续更新信念图表示已知和未知物体；3）通过计算目标图与信念图之间的图编辑操作生成动作序列，按时间依赖性和移动成本排序。

Result: 在46个真实家庭场景和5个长时程日常物体运输任务中，EPoG达到91.3%的成功率，平均减少36.1%的移动距离。物理移动机械臂在未知和动态环境中成功执行复杂任务。

Conclusion: EPoG框架能够无缝结合探索与顺序操作规划，在部分已知环境中实现高效的任务执行，展示了在现实世界应用中的潜力。

Abstract: In partially known environments, robots must combine exploration to gather information with task planning for efficient execution. To address this challenge, we propose EPoG, an Exploration-based sequential manipulation Planning framework on Scene Graphs. EPoG integrates a graph-based global planner with a Large Language Model (LLM)-based situated local planner, continuously updating a belief graph using observations and LLM predictions to represent known and unknown objects. Action sequences are generated by computing graph edit operations between the goal and belief graphs, ordered by temporal dependencies and movement costs. This approach seamlessly combines exploration and sequential manipulation planning. In ablation studies across 46 realistic household scenes and 5 long-horizon daily object transportation tasks, EPoG achieved a success rate of 91.3%, reducing travel distance by 36.1% on average. Furthermore, a physical mobile manipulator successfully executed complex tasks in unknown and dynamic environments, demonstrating EPoG's potential for real-world applications.

</details>


### [23] [Gust Estimation and Rejection with a Disturbance Observer for Proprioceptive Underwater Soft Morphing Wings](https://arxiv.org/abs/2602.04438)
*Tobias Cook,Leo Micklem,Huazhi Dong,Yunjie Yang,Michael Mistry,Francesco Giorgio Serchi*

Main category: cs.RO

TL;DR: 该论文提出了一种受海洋生物启发的软变形翼，通过本体感知传感和扰动观测器来抵消水下环境扰动，提高无人水下航行器在浅水区的稳定性。


<details>
  <summary>Details</summary>
Motivation: 无人水下航行器在浅水区作业时，常受到波浪、水流和湍流等非定常流体动力扰动的影响，导致方向和速度快速变化，损害航行器的稳定性和机动性。海洋生物通过结合本体感知反馈与柔性鳍尾来应对此类扰动，这启发了研究团队开发类似的软变形翼系统。

Method: 1. 开发并实验验证了液压驱动的软变形翼动态模型，该翼具有可控的弯度；2. 利用翼的连续变形作为本体感知信号，通过曲率传感准确估计攻角扰动；3. 设计扰动观测器实时重构流动参数；4. 开发基于本体感知估计的控制器来抑制软翼升力响应中的扰动。

Result: 1. 成功开发并验证了液压驱动软变形翼的动态模型；2. 曲率传感能够准确估计攻角扰动；3. 基于本体感知估计的控制器能够有效抑制软翼升力响应中的环境扰动。

Conclusion: 通过结合本体感知传感和扰动观测器，该技术模仿了生物应对环境扰动的策略，为软体水下航行器在危险环境中保持稳定性提供了一条有效途径。这种方法将软变形翼的连续变形转化为感知信号，实现了对环境扰动的实时估计和主动抑制。

Abstract: Unmanned underwater vehicles are increasingly employed for maintenance and surveying tasks at sea, but their operation in shallow waters is often hindered by hydrodynamic disturbances such as waves, currents, and turbulence. These unsteady flows can induce rapid changes in direction and speed, compromising vehicle stability and manoeuvrability. Marine organisms contend with such conditions by combining proprioceptive feedback with flexible fins and tails to reject disturbances. Inspired by this strategy, we propose soft morphing wings endowed with proprioceptive sensing to mitigate environmental perturbations. The wing's continuous deformation provides a natural means to infer dynamic disturbances: sudden changes in camber directly reflect variations in the oncoming flow. By interpreting this proprioceptive signal, a disturbance observer can reconstruct flow parameters in real time. To enable this, we develop and experimentally validate a dynamic model of a hydraulically actuated soft wing with controllable camber. We then show that curvature-based sensing allows accurate estimation of disturbances in the angle of attack. Finally, we demonstrate that a controller leveraging these proprioceptive estimates can reject disturbances in the lift response of the soft wing. By combining proprioceptive sensing with a disturbance observer, this technique mirrors biological strategies and provides a pathway for soft underwater vehicles to maintain stability in hazardous environments.

</details>


### [24] [EgoActor: Grounding Task Planning into Spatial-aware Egocentric Actions for Humanoid Robots via Visual-Language Models](https://arxiv.org/abs/2602.04515)
*Yu Bai,MingMing Yu,Chaojie Li,Ziyi Bai,Xinlong Wang,Börje F. Karlsson*

Main category: cs.RO

TL;DR: 提出EgoActing任务和EgoActor模型，通过统一的视觉语言模型将高层指令直接映射到精确的空间感知人形机器人动作，实现感知、运动、操作的实时协调。


<details>
  <summary>Details</summary>
Motivation: 在现实世界中部署人形机器人面临根本性挑战，需要在部分信息观察和动态变化环境下紧密集成感知、运动和操作，并能在不同类型子任务间稳健过渡。

Method: 提出EgoActing任务，并构建EgoActor统一可扩展的视觉语言模型，预测运动基元、头部运动、操作命令和人机交互。利用真实世界第一人称RGB数据监督、空间推理问答和模拟环境演示进行训练。

Result: EgoActor能够做出稳健的上下文感知决策，在1秒内执行流畅的动作推理，8B和4B参数模型均表现良好。在模拟和真实环境中的广泛评估表明，该模型能有效桥接抽象任务规划和具体运动执行，并在多样化任务和未见环境中具有良好泛化能力。

Conclusion: EgoActor通过统一的视觉语言模型成功解决了人形机器人在现实世界部署中的核心挑战，实现了高层指令到精确动作的直接映射，为感知、运动、操作的实时协调提供了有效解决方案。

Abstract: Deploying humanoid robots in real-world settings is fundamentally challenging, as it demands tight integration of perception, locomotion, and manipulation under partial-information observations and dynamically changing environments. As well as transitioning robustly between sub-tasks of different types. Towards addressing these challenges, we propose a novel task - EgoActing, which requires directly grounding high-level instructions into various, precise, spatially aware humanoid actions. We further instantiate this task by introducing EgoActor, a unified and scalable vision-language model (VLM) that can predict locomotion primitives (e.g., walk, turn, move sideways, change height), head movements, manipulation commands, and human-robot interactions to coordinate perception and execution in real-time. We leverage broad supervision over egocentric RGB-only data from real-world demonstrations, spatial reasoning question-answering, and simulated environment demonstrations, enabling EgoActor to make robust, context-aware decisions and perform fluent action inference (under 1s) with both 8B and 4B parameter models. Extensive evaluations in both simulated and real-world environments demonstrate that EgoActor effectively bridges abstract task planning and concrete motor execution, while generalizing across diverse tasks and unseen environments.

</details>


### [25] [TACO: Temporal Consensus Optimization for Continual Neural Mapping](https://arxiv.org/abs/2602.04516)
*Xunlan Zhou,Hongrui Zhao,Negar Mehr*

Main category: cs.RO

TL;DR: TACO是一个无需回放的持续神经建图框架，通过时间共识优化解决动态环境下的机器人建图问题，在内存和计算受限条件下实现场景自适应。


<details>
  <summary>Details</summary>
Motivation: 现有神经隐式建图方法无法在内存和计算受限条件下适应动态环境变化，大多依赖历史观测回放并假设静态场景，无法满足真实机器人部署的持续学习需求。

Method: 将建图重新定义为时间共识优化问题，将过去模型快照视为时间邻居，通过加权共识约束当前地图更新，允许可靠历史几何约束优化同时更新过时区域。

Result: TACO在模拟和真实世界实验中稳健适应场景变化，在持续学习基准测试中持续优于其他方法，实现了内存效率与适应性的平衡。

Conclusion: TACO框架通过时间共识优化解决了持续神经建图的关键挑战，为动态环境下的机器人导航和场景理解提供了高效实用的解决方案。

Abstract: Neural implicit mapping has emerged as a powerful paradigm for robotic navigation and scene understanding. However, real-world robotic deployment requires continual adaptation to changing environments under strict memory and computation constraints, which existing mapping systems fail to support. Most prior methods rely on replaying historical observations to preserve consistency and assume static scenes. As a result, they cannot adapt to continual learning in dynamic robotic settings. To address these challenges, we propose TACO (TemporAl Consensus Optimization), a replay-free framework for continual neural mapping. We reformulate mapping as a temporal consensus optimization problem, where we treat past model snapshots as temporal neighbors. Intuitively, our approach resembles a model consulting its own past knowledge. We update the current map by enforcing weighted consensus with historical representations. Our method allows reliable past geometry to constrain optimization while permitting unreliable or outdated regions to be revised in response to new observations. TACO achieves a balance between memory efficiency and adaptability without storing or replaying previous data. Through extensive simulated and real-world experiments, we show that TACO robustly adapts to scene changes, and consistently outperforms other continual learning baselines.

</details>


### [26] [A Unified Complementarity-based Approach for Rigid-Body Manipulation and Motion Prediction](https://arxiv.org/abs/2602.04522)
*Bingkun Huang,Xin Ma,Nilanjan Chakraborty,Riddhiman Laha*

Main category: cs.RO

TL;DR: 提出Unicomp统一离散时间建模框架，将自由空间运动和摩擦接触统一在互补性刚性体动力学中，通过线性/非线性互补问题处理接触模式转换，适用于实时优化规划。


<details>
  <summary>Details</summary>
Motivation: 现有规划框架通常将自由空间运动和接触分离处理，或依赖简化的接触表示，这限制了接触模式转换的保真度，阻碍了接触丰富行为在实时环境中的鲁棒执行。

Method: 基于互补性刚性体动力学，将自由空间运动和接触交互建模为耦合的线性和非线性互补问题；针对平面面接触，从最大功率耗散原理推导摩擦接触模型，用椭球极限表面表示可接受接触力矩集合。

Result: 提出的方法在交互速度下实现了稳定、物理一致的行为，适用于从平面推送到接触丰富的全身操作等多种任务。

Conclusion: Unicomp框架为机器人操作提供了统一的离散时间建模方法，能够一致地捕捉自由运动和摩擦接触，支持接触模式的无缝转换，适用于实时优化规划。

Abstract: Robotic manipulation in unstructured environments requires planners to reason jointly about free-space motion and sustained, frictional contact with the environment. Existing (local) planning and simulation frameworks typically separate these regimes or rely on simplified contact representations, particularly when modeling non-convex or distributed contact patches. Such approximations limit the fidelity of contact-mode transitions and hinder the robust execution of contact-rich behaviors in real time. This paper presents a unified discrete-time modeling framework for robotic manipulation that consistently captures both free motion and frictional contact within a single mathematical formalism (Unicomp). Building on complementarity-based rigid-body dynamics, we formulate free-space motion and contact interactions as coupled linear and nonlinear complementarity problems, enabling principled transitions between contact modes without enforcing fixed-contact assumptions. For planar patch contact, we derive a frictional contact model from the maximum power dissipation principle in which the set of admissible contact wrenches is represented by an ellipsoidal limit surface. This representation captures coupled force-moment effects, including torsional friction, while remaining agnostic to the underlying pressure distribution across the contact patch. The resulting formulation yields a discrete-time predictive model that relates generalized velocities and contact wrenches through quadratic constraints and is suitable for real-time optimization-based planning. Experimental results show that the proposed approach enables stable, physically consistent behavior at interactive speeds across tasks, from planar pushing to contact-rich whole-body maneuvers.

</details>


### [27] [Act, Sense, Act: Learning Non-Markovian Active Perception Strategies from Large-Scale Egocentric Human Data](https://arxiv.org/abs/2602.04600)
*Jialiang Li,Yi Qiao,Yunhan Guo,Changwen Chen,Wenzhao Lian*

Main category: cs.RO

TL;DR: 提出CoMe-VLA框架，通过认知记忆感知的视觉-语言-动作系统，利用人类第一人称数据学习主动感知和操作先验，解决非结构化环境中机器人信息不确定性问题。


<details>
  <summary>Details</summary>
Motivation: 在非约束环境中实现通用化操作需要机器人主动解决信息不确定性（主动感知能力）。现有方法通常局限于有限的感知行为类型，难以适应复杂环境。

Method: 将主动感知形式化为由信息增益和决策分支驱动的非马尔可夫过程，提出CoMe-VLA框架：包含认知辅助头实现自主子任务转换，双轨记忆系统融合本体感觉和视觉时序上下文，在统一的第一人称动作空间中对齐人类和机器人手眼协调行为，分三阶段渐进训练。

Result: 在轮式人形机器人上进行广泛实验，证明该方法在跨越多种主动感知场景的多样长时程任务中具有强大的鲁棒性和适应性。

Conclusion: 通过结构化主动感知范式、认知记忆感知框架和人类第一人称数据对齐，实现了在复杂环境中具有强适应性的机器人主动感知和操作能力。

Abstract: Achieving generalizable manipulation in unconstrained environments requires the robot to proactively resolve information uncertainty, i.e., the capability of active perception. However, existing methods are often confined in limited types of sensing behaviors, restricting their applicability to complex environments. In this work, we formalize active perception as a non-Markovian process driven by information gain and decision branching, providing a structured categorization of visual active perception paradigms. Building on this perspective, we introduce CoMe-VLA, a cognitive and memory-aware vision-language-action (VLA) framework that leverages large-scale human egocentric data to learn versatile exploration and manipulation priors. Our framework integrates a cognitive auxiliary head for autonomous sub-task transitions and a dual-track memory system to maintain consistent self and environmental awareness by fusing proprioceptive and visual temporal contexts. By aligning human and robot hand-eye coordination behaviors in a unified egocentric action space, we train the model progressively in three stages. Extensive experiments on a wheel-based humanoid have demonstrated strong robustness and adaptability of our proposed method across diverse long-horizon tasks spanning multiple active perception scenarios.

</details>


### [28] [Radar-Inertial Odometry For Computationally Constrained Aerial Navigation](https://arxiv.org/abs/2602.04631)
*Jan Michalczyk*

Main category: cs.RO

TL;DR: 该论文提出了一种雷达-惯性里程计(RIO)算法，用于在资源受限的嵌入式设备上实时融合IMU和低成本FMCW雷达数据，以估计无人机导航状态，特别是在恶劣环境条件下。


<details>
  <summary>Details</summary>
Motivation: 传统外感知传感器（如LiDAR、相机）在极端光照、烟雾等恶劣环境下容易失效，而雷达因其电磁波特性对这些环境因素具有较强抗干扰能力。需要开发能够在资源受限设备上实时运行的雷达-惯性融合算法。

Method: 提出了基于多状态紧耦合扩展卡尔曼滤波(EKF)和因子图(FG)的雷达-惯性里程计算法，融合低成本FMCW雷达提供的3D点瞬时速度和距离信息与IMU读数。同时利用深度学习从稀疏噪声雷达点云中提取3D点对应关系。

Result: 开发了能够在便携式资源受限嵌入式计算机上实时运行的RIO算法，使用低成本消费级传感器，能够在极端环境条件下估计无人机导航状态。

Conclusion: 雷达-惯性融合为恶劣环境下的机器人自主导航提供了可靠解决方案，特别是结合深度学习技术处理稀疏噪声雷达数据，为资源受限平台的实时导航开辟了新途径。

Abstract: Recently, the progress in the radar sensing technology consisting in the miniaturization of the packages and increase in measuring precision has drawn the interest of the robotics research community. Indeed, a crucial task enabling autonomy in robotics is to precisely determine the pose of the robot in space. To fulfill this task sensor fusion algorithms are often used, in which data from one or several exteroceptive sensors like, for example, LiDAR, camera, laser ranging sensor or GNSS are fused together with the Inertial Measurement Unit (IMU) measurements to obtain an estimate of the navigation states of the robot. Nonetheless, owing to their particular sensing principles, some exteroceptive sensors are often incapacitated in extreme environmental conditions, like extreme illumination or presence of fine particles in the environment like smoke or fog. Radars are largely immune to aforementioned factors thanks to the characteristics of electromagnetic waves they use. In this thesis, we present Radar-Inertial Odometry (RIO) algorithms to fuse the information from IMU and radar in order to estimate the navigation states of a (Uncrewed Aerial Vehicle) UAV capable of running on a portable resource-constrained embedded computer in real-time and making use of inexpensive, consumer-grade sensors. We present novel RIO approaches relying on the multi-state tightly-coupled Extended Kalman Filter (EKF) and Factor Graphs (FG) fusing instantaneous velocities of and distances to 3D points delivered by a lightweight, low-cost, off-the-shelf Frequency Modulated Continuous Wave (FMCW) radar with IMU readings. We also show a novel way to exploit advances in deep learning to retrieve 3D point correspondences in sparse and noisy radar point clouds.

</details>


### [29] [Dull, Dirty, Dangerous: Understanding the Past, Present, and Future of a Key Motivation for Robotics](https://arxiv.org/abs/2602.04746)
*Nozomi Nakajima,Pedro Reynolds-Cuéllar,Caitrin Lynch,Kate Darling*

Main category: cs.RO

TL;DR: 对1980-2024年机器人学文献中"枯燥、肮脏、危险"（DDD）概念使用的实证分析，发现仅有少量文献明确定义或提供具体DDD任务案例，提出基于社会科学文献的DDD定义框架


<details>
  <summary>Details</summary>
Motivation: 机器人学领域长期使用"枯燥、肮脏、危险"（DDD）工作作为机器人应用的主要动机，但缺乏对这一概念的明确定义和系统性分析，需要更严谨地理解DDD概念以指导机器人技术对人类劳动的影响评估

Method: 1）对1980-2024年机器人学文献进行实证分析，统计DDD概念的提及、定义和具体案例；2）回顾社会科学文献，为"枯燥"、"肮脏"、"危险"工作提供定义和概念化指导；3）提出帮助机器人学界考虑工作背景的框架

Result: 实证分析显示：仅2.7%的文献明确定义DDD，仅8.7%的文献提供具体DDD任务或工作案例，表明机器人学界对DDD概念的使用缺乏严谨性和具体性

Conclusion: 需要更严谨地概念化DDD工作，提出的框架有助于机器人学界更全面地考虑工作背景，从而更明智地评估机器人技术对人类劳动的影响

Abstract: In robotics, the concept of "dull, dirty, and dangerous" (DDD) work has been used to motivate where robots might be useful. In this paper, we conduct an empirical analysis of robotics publications between 1980 and 2024 that mention DDD, and find that only 2.7% of publications define DDD and 8.7% of publications provide concrete examples of tasks or jobs that are DDD. We then review the social science literature on "dull," "dirty," and "dangerous" work to provide definitions and guidance on how to conceptualize DDD for robotics. Finally, we propose a framework that helps the robotics community consider the job context for our technology, encouraging a more informed perspective on how robotics may impact human labor.

</details>


### [30] [PDF-HR: Pose Distance Fields for Humanoid Robots](https://arxiv.org/abs/2602.04851)
*Yi Gu,Yukang Gao,Yangchen Zhou,Xingyu Chen,Yixiao Feng,Mingle Zhao,Yunyang Mo,Zhaorui Wang,Lixin Xu,Renjing Xu*

Main category: cs.RO

TL;DR: PDF-HR是一个轻量级的人形机器人姿态先验模型，将机器人姿态分布表示为连续可微的流形，通过预测任意姿态到大规模重定向机器人姿态语料库的距离来评估姿态合理性。


<details>
  <summary>Details</summary>
Motivation: 姿态和运动先验在人形机器人中至关重要，虽然这些先验在人体运动恢复领域已有广泛研究，但由于缺乏高质量的人形机器人运动数据，在人形机器人中的应用仍然有限。

Method: 提出Pose Distance Fields for Humanoid Robots (PDF-HR)，这是一个轻量级先验模型，将机器人姿态分布表示为连续可微的流形。给定任意姿态，PDF-HR预测其到大规模重定向机器人姿态语料库的距离，产生平滑的姿态合理性度量。

Result: PDF-HR在各种人形机器人任务中进行了评估，包括单轨迹运动跟踪、通用运动跟踪、基于风格的运动模仿和通用运动重定向。实验表明，这个即插即用的先验模型能够一致且显著地增强强基线方法。

Conclusion: PDF-HR作为一个轻量级、连续可微的姿态先验模型，能够有效评估人形机器人姿态的合理性，可作为奖励塑造项、正则化器或独立的合理性评分器集成到多种管道中，显著提升各种人形机器人任务的性能。

Abstract: Pose and motion priors play a crucial role in humanoid robotics. Although such priors have been widely studied in human motion recovery (HMR) domain with a range of models, their adoption for humanoid robots remains limited, largely due to the scarcity of high-quality humanoid motion data. In this work, we introduce Pose Distance Fields for Humanoid Robots (PDF-HR), a lightweight prior that represents the robot pose distribution as a continuous and differentiable manifold. Given an arbitrary pose, PDF-HR predicts its distance to a large corpus of retargeted robot poses, yielding a smooth measure of pose plausibility that is well suited for optimization and control. PDF-HR can be integrated as a reward shaping term, a regularizer, or a standalone plausibility scorer across diverse pipelines. We evaluate PDF-HR on various humanoid tasks, including single-trajectory motion tracking, general motion tracking, style-based motion mimicry, and general motion retargeting. Experiments show that this plug-and-play prior consistently and substantially strengthens strong baselines. Code and models will be released.

</details>
