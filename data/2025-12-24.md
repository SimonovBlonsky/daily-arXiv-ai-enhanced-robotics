<div id=toc></div>

# Table of Contents

- [cs.RO](#cs.RO) [Total: 11]


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [1] [Gaussian Variational Inference with Non-Gaussian Factors for State Estimation: A UWB Localization Case Study](https://arxiv.org/abs/2512.19855)
*Andrew Stirling,Mykola Lukashchuk,Dmitry Bagaev,Wouter Kouw,James R. Forbes*

Main category: cs.RO

TL;DR: 该论文将精确稀疏高斯变分推断（ESGVI）算法扩展到矩阵李群上，并引入因子处理重尾和偏斜噪声分布，应用于UWB定位中的NLOS场景，提高了估计精度。


<details>
  <summary>Details</summary>
Motivation: 现有的ESGVI算法在处理包含方向分量的状态估计时，未能充分利用矩阵李群的几何结构；同时，在UWB定位等实际应用中，NLOS和多径效应导致噪声呈现重尾和偏斜分布，需要更鲁棒的噪声模型。

Method: 1. 将ESGVI推广到矩阵李群上，使算法能够处理包含方向的状态估计并保持群结构；2. 引入能够处理重尾和偏斜噪声分布的因子模型；3. 在UWB定位实验中验证方法，并在因子图估计框架中提供开源Python实现。

Result: 在富含NLOS测量的UWB定位实验中，所提方法表现出更高的精度和相当的稳定性，证明了扩展ESGVI在处理复杂几何结构和非高斯噪声方面的有效性。

Conclusion: 该工作成功地将ESGVI扩展到矩阵李群并增强了其对非高斯噪声的鲁棒性，为状态估计提供了更通用的框架，特别是在UWB定位等实际应用中具有重要价值。开源实现促进了更广泛的研究应用。

Abstract: This letter extends the exactly sparse Gaussian variational inference (ESGVI) algorithm for state estimation in two complementary directions. First, ESGVI is generalized to operate on matrix Lie groups, enabling the estimation of states with orientation components while respecting the underlying group structure. Second, factors are introduced to accommodate heavy-tailed and skewed noise distributions, as commonly encountered in ultra-wideband (UWB) localization due to non-line-of-sight (NLOS) and multipath effects. Both extensions are shown to integrate naturally within the ESGVI framework while preserving its sparse and derivative-free structure. The proposed approach is validated in a UWB localization experiment with NLOS-rich measurements, demonstrating improved accuracy and comparable consistency. Finally, a Python implementation within a factor-graph-based estimation framework is made open-source (https://github.com/decargroup/gvi_ws) to support broader research use.

</details>


### [2] [A Time-efficient Prioritised Scheduling Algorithm to Optimise Initial Flock Formation of Drones](https://arxiv.org/abs/2512.19914)
*Sujan Warnakulasooriya,Andreas Willig,Xiaobing Wu*

Main category: cs.RO

TL;DR: 提出了一种基于优先级调度的时间高效算法，用于改进无人机群初始编队过程，通过优先级分配和延迟计算确保无碰撞轨迹，支持多达5000架无人机


<details>
  <summary>Details</summary>
Motivation: 无人机应用不断扩展，集群编队增强了协同能力，但初始编队面临效率低、可扩展性差的问题，现有算法在避免碰撞时往往导致次优轨迹

Method: 基于优先级调度的时间高效算法：根据无人机潜在碰撞数量和到达目标位置而不永久阻碍其他无人机的可能性分配优先级，利用这种层次结构计算适当延迟以确保无碰撞路径

Result: 仿真结果显示，该算法成功为多达5000架无人机群生成无碰撞轨迹，在性能和计算效率上都优于基于耦合度的启发式优先级规划方法（CDH-PP）

Conclusion: 提出的优先级调度算法有效解决了无人机群初始编队中的效率和可扩展性问题，为大规模无人机集群应用提供了实用的解决方案

Abstract: Drone applications continue to expand across various domains, with flocking offering enhanced cooperative capabilities but introducing significant challenges during initial formation. Existing flocking algorithms often struggle with efficiency and scalability, particularly when potential collisions force drones into suboptimal trajectories. This paper presents a time-efficient prioritised scheduling algorithm that improves the initial formation process of drone flocks. The method assigns each drone a priority based on its number of potential collisions and its likelihood of reaching its target position without permanently obstructing other drones. Using this hierarchy, each drone computes an appropriate delay to ensure a collision-free path. Simulation results show that the proposed algorithm successfully generates collision-free trajectories for flocks of up to 5000 drones and outperforms the coupling-degree-based heuristic prioritised planning method (CDH-PP) in both performance and computational efficiency.

</details>


### [3] [Bring My Cup! Personalizing Vision-Language-Action Models with Visual Attentive Prompting](https://arxiv.org/abs/2512.20014)
*Sangoh Lee,Sangwoo Mo,Wook-Shin Han*

Main category: cs.RO

TL;DR: VAP是一种无需训练的感知适配器，通过视觉注意力提示让冻结的VLA模型能够处理个性化对象操作任务，如"拿我的杯子"。


<details>
  <summary>Details</summary>
Motivation: 当前视觉-语言-动作模型在通用指令上表现良好，但在处理个性化命令时存在困难，例如需要从视觉相似对象中识别并操作特定用户物品的情况。

Method: 提出视觉注意力提示方法，将参考图像作为非参数视觉记忆，通过开放词汇检测和嵌入匹配定位个性化对象，然后通过高亮对象和重写指令将其作为视觉提示注入模型。

Result: 在两个仿真基准和真实世界桌面基准上的实验表明，VAP在成功率和正确对象操作方面始终优于通用策略和基于令牌学习的方法。

Conclusion: VAP有助于弥合语义理解和实例级控制之间的差距，为个性化对象操作提供了一种简单有效的解决方案。

Abstract: While Vision-Language-Action (VLA) models generalize well to generic instructions, they struggle with personalized commands such as "bring my cup", where the robot must act on one specific instance among visually similar objects. We study this setting of manipulating personal objects, in which a VLA must identify and control a user-specific object unseen during training using only a few reference images. To address this challenge, we propose Visual Attentive Prompting (VAP), a simple-yet-effective training-free perceptual adapter that equips frozen VLAs with top-down selective attention. VAP treats the reference images as a non-parametric visual memory, grounds the personal object in the scene through open-vocabulary detection and embedding-based matching, and then injects this grounding as a visual prompt by highlighting the object and rewriting the instruction. We construct two simulation benchmarks, Personalized-SIMPLER and Personalized-VLABench, and a real-world tabletop benchmark to evaluate personalized manipulation across multiple robots and tasks. Experiments show that VAP consistently outperforms generic policies and token-learning baselines in both success rate and correct-object manipulation, helping to bridge the gap between semantic understanding and instance-level control.

</details>


### [4] [LoLA: Long Horizon Latent Action Learning for General Robot Manipulation](https://arxiv.org/abs/2512.20166)
*Xiaofan Wang,Xingyu Gao,Jianlong Fu,Zuolei Li,Dean Fortier,Galen Mullins,Andrey Kolobov,Baining Guo*

Main category: cs.RO

TL;DR: LoLA框架通过整合长期多视角观测和机器人本体感知，实现多步推理和动作生成，在长视野机器人操作任务中显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言-动作模型在处理长视野、语言引导的机器人操作任务时，往往忽视了利用历史信息和生成连贯动作序列的能力，这限制了机器人在复杂操作任务中的表现。

Method: 提出LoLA框架：1) 使用视觉语言模型编码历史序列和多视角观测的上下文特征；2) 引入状态感知潜在重表示模块，将视觉输入和语言指令转换为可执行的机器人运动空间；3) 通过可学习的"具身锚定"潜在空间，利用机器人状态将视觉-语言表示显式地锚定在物理尺度上。

Result: 在SIMPLER和LIBERO仿真基准测试以及Franka和Bi-Manual Aloha机器人的真实世界任务中，LoLA显著优于现有最先进方法（如pi0），特别是在长视野操作任务中表现突出。

Conclusion: LoLA框架通过整合长期多视角观测和机器人本体感知，并引入状态感知潜在重表示模块，有效解决了长视野机器人操作任务中的历史信息利用和连贯动作生成问题，为复杂机器人操作提供了新的解决方案。

Abstract: The capability of performing long-horizon, language-guided robotic manipulation tasks critically relies on leveraging historical information and generating coherent action sequences. However, such capabilities are often overlooked by existing Vision-Language-Action (VLA) models. To solve this challenge, we propose LoLA (Long Horizon Latent Action Learning), a framework designed for robot manipulation that integrates long-term multi-view observations and robot proprioception to enable multi-step reasoning and action generation. We first employ Vision-Language Models to encode rich contextual features from historical sequences and multi-view observations. We further introduces a key module, State-Aware Latent Re-representation, which transforms visual inputs and language commands into actionable robot motion space. Unlike existing VLA approaches that merely concatenate robot proprioception (e.g., joint angles) with VL embeddings, this module leverages such robot states to explicitly ground VL representations in physical scale through a learnable "embodiment-anchored" latent space. We trained LoLA on diverse robotic pre-training datasets and conducted extensive evaluations on simulation benchmarks (SIMPLER and LIBERO), as well as two real-world tasks on Franka and Bi-Manual Aloha robots. Results show that LoLA significantly outperforms prior state-of-the-art methods (e.g., pi0), particularly in long-horizon manipulation tasks.

</details>


### [5] [Asynchronous Fast-Slow Vision-Language-Action Policies for Whole-Body Robotic Manipulation](https://arxiv.org/abs/2512.20188)
*Teqiang Zou,Hongliang Zeng,Yuxuan Nong,Yifan Li,Kehui Liu,Haotian Yang,Xinyang Ling,Xin Li,Lianyang Ma*

Main category: cs.RO

TL;DR: DuoCore-FS提出异步快速-慢速VLA框架，通过分离高频动作生成和慢速VLM推理，实现30Hz全身动作生成，比同步VLA模型快约3倍。


<details>
  <summary>Details</summary>
Motivation: 传统VLA系统采用统一频率运行，受限于大型VLM的低推理速度，导致控制稳定性和实时性能受限，特别是在涉及更多关节、更大运动空间和动态变化视角的全身机器人操作中。

Method: 1) 异步快速-慢速框架：快速路径用于高频动作生成，慢速路径用于丰富的VLM推理；2) 潜在表示缓冲区：桥接慢速和快速系统，存储指令语义和与场景-指令上下文对齐的动作推理表示；3) 全身动作标记器：提供紧凑统一的全身动作表示；4) 端到端联合训练：保持统一策略学习的同时支持异步执行。

Result: 支持30亿参数VLM的同时实现30Hz全身动作块生成，比同类规模VLA模型快约3倍。真实世界全身操作实验显示任务成功率提高，响应性显著增强。

Conclusion: DuoCore-FS通过异步架构解决了同步VLA系统的性能瓶颈，在保持端到端训练的同时显著提升了实时性和控制稳定性，为商业机器人平台提供了实用的解决方案。

Abstract: Most Vision-Language-Action (VLA) systems integrate a Vision-Language Model (VLM) for semantic reasoning with an action expert generating continuous action signals, yet both typically run at a single unified frequency. As a result, policy performance is constrained by the low inference speed of large VLMs. This mandatory synchronous execution severely limits control stability and real-time performance in whole-body robotic manipulation, which involves more joints, larger motion spaces, and dynamically changing views. We introduce a truly asynchronous Fast-Slow VLA framework (DuoCore-FS), organizing the system into a fast pathway for high-frequency action generation and a slow pathway for rich VLM reasoning. The system is characterized by two key features. First, a latent representation buffer bridges the slow and fast systems. It stores instruction semantics and action-reasoning representation aligned with the scene-instruction context, providing high-level guidance to the fast pathway. Second, a whole-body action tokenizer provides a compact, unified representation of whole-body actions. Importantly, the VLM and action expert are still jointly trained end-to-end, preserving unified policy learning while enabling asynchronous execution. DuoCore-FS supports a 3B-parameter VLM while achieving 30 Hz whole-body action-chunk generation, approximately three times as fast as prior VLA models with comparable model sizes. Real-world whole-body manipulation experiments demonstrate improved task success rates and significantly enhanced responsiveness compared to synchronous Fast-Slow VLA baselines. The implementation of DuoCore-FS, including training, inference, and deployment, is provided to commercial users by Astribot as part of the Astribot robotic platform.

</details>


### [6] [UrbanV2X: A Multisensory Vehicle-Infrastructure Dataset for Cooperative Navigation in Urban Areas](https://arxiv.org/abs/2512.20224)
*Qijun Qin,Ziqi Zhang,Yihan Zhong,Feng Huang,Xikun Liu,Runzhi Hu,Hang Chen,Wei Hu,Dongzhe Su,Jun Zhang,Hoi-Fung Ng,Weisong Wen*

Main category: cs.RO

TL;DR: UrbanV2X是一个用于车路协同导航的多传感器数据集，包含车辆和路侧基础设施在密集城市环境中的同步数据，支持智能交通应用研究。


<details>
  <summary>Details</summary>
Motivation: 由于单车自动驾驶的局限性，C-V2X技术通过传感器信息共享为实现完全自动驾驶提供了新途径。然而，支持复杂城市环境中车路协同导航的真实数据集仍然稀缺。

Method: 在香港C-V2X测试场收集车辆和路侧基础设施的多传感器数据，包括工业相机、LiDAR、4D雷达、UWB、IMU和高精度GNSS-RTK/INS导航系统，使用PTP协议进行同步，并提供传感器标定数据。

Result: 创建了UrbanV2X数据集，包含车辆和路侧基础设施的同步多传感器数据，并对各种导航算法进行了基准测试，数据集已公开可用。

Conclusion: UrbanV2X数据集填补了复杂城市环境中车路协同导航数据集的空白，为智能交通应用研究提供了有价值的资源，并展示了C-V2X技术在密集城市区域的实际应用潜力。

Abstract: Due to the limitations of a single autonomous vehicle, Cellular Vehicle-to-Everything (C-V2X) technology opens a new window for achieving fully autonomous driving through sensor information sharing. However, real-world datasets supporting vehicle-infrastructure cooperative navigation in complex urban environments remain rare. To address this gap, we present UrbanV2X, a comprehensive multisensory dataset collected from vehicles and roadside infrastructure in the Hong Kong C-V2X testbed, designed to support research on smart mobility applications in dense urban areas. Our onboard platform provides synchronized data from multiple industrial cameras, LiDARs, 4D radar, ultra-wideband (UWB), IMU, and high-precision GNSS-RTK/INS navigation systems. Meanwhile, our roadside infrastructure provides LiDAR, GNSS, and UWB measurements. The entire vehicle-infrastructure platform is synchronized using the Precision Time Protocol (PTP), with sensor calibration data provided. We also benchmark various navigation algorithms to evaluate the collected cooperative data. The dataset is publicly available at https://polyu-taslab.github.io/UrbanV2X/.

</details>


### [7] [KnowVal: A Knowledge-Augmented and Value-Guided Autonomous Driving System](https://arxiv.org/abs/2512.20299)
*Zhongyu Xia,Wenhao Chen,Yongtao Wang,Ming-Hsuan Yang*

Main category: cs.RO

TL;DR: KnowVal是一个通过开放世界感知与知识检索协同整合实现视觉语言推理的自动驾驶系统，显著提升了规划性能并保持与现有架构的兼容性。


<details>
  <summary>Details</summary>
Motivation: 现有自动驾驶方法主要依赖数据驱动学习，难以通过模仿或有限的强化奖励捕捉决策背后的复杂逻辑，需要解决视觉语言推理、驾驶知识和价值对齐等关键问题。

Method: 构建编码交通法规、防御性驾驶原则和伦理规范的全面驾驶知识图谱，开发基于LLM的驾驶场景检索机制，创建人类偏好数据集并训练价值模型来指导可解释的价值对齐轨迹评估。

Result: 在nuScenes数据集上实现了最低碰撞率，在Bench2Drive基准测试中取得了最先进的结果，显著提升了规划性能。

Conclusion: KnowVal通过整合开放世界感知、知识检索和价值对齐，有效解决了自动驾驶中的复杂逻辑推理问题，为高级自动驾驶系统提供了新的解决方案。

Abstract: Visual-language reasoning, driving knowledge, and value alignment are essential for advanced autonomous driving systems. However, existing approaches largely rely on data-driven learning, making it difficult to capture the complex logic underlying decision-making through imitation or limited reinforcement rewards. To address this, we propose KnowVal, a new autonomous driving system that enables visual-language reasoning through the synergistic integration of open-world perception and knowledge retrieval. Specifically, we construct a comprehensive driving knowledge graph that encodes traffic laws, defensive driving principles, and ethical norms, complemented by an efficient LLM-based retrieval mechanism tailored for driving scenarios. Furthermore, we develop a human-preference dataset and train a Value Model to guide interpretable, value-aligned trajectory assessment. Experimental results show that our method substantially improves planning performance while remaining compatible with existing architectures. Notably, KnowVal achieves the lowest collision rate on nuScenes and state-of-the-art results on Bench2Drive.

</details>


### [8] [Pneumatic bladder links with wide range of motion joints for articulated inflatable robots](https://arxiv.org/abs/2512.20322)
*Katsu Uchiyama,Ryuma Niiyama*

Main category: cs.RO

TL;DR: 提出了一种由多个气动囊体连杆通过滚动接触关节连接的可充气机器人，具有大运动范围和高负载能力，并展示了在机械臂和腿式运动中的应用。


<details>
  <summary>Details</summary>
Motivation: 探索可充气机器人的各种应用是当前研究的前沿，需要开发具有大运动范围和高负载能力的新型关节结构。

Method: 设计了一种由多个气动囊体连杆组成的关节式机器人，连杆采用双层结构（防水布和聚氨酯片），通过新型Hillberry滚动接触关节连接，实现±150°的大范围运动。

Result: 3自由度机械臂可移动500g负载，2自由度和1自由度机械臂分别可举起3.4kg和5kg负载；3自由度充气腿安装在推车上展示了腿式运动能力。

Conclusion: 提出的Hillberry关节集成到可充气机器人中是创新方法，滚动接触关节提供了传统充气关节中最大的运动范围，证明了该结构在机械臂和腿式运动中的有效性。

Abstract: Exploration of various applications is the frontier of research on inflatable robots. We proposed an articulated robots consisting of multiple pneumatic bladder links connected by rolling contact joints called Hillberry joints. The bladder link is made of a double-layered structure of tarpaulin sheet and polyurethane sheet, which is both airtight and flexible in shape. The integration of the Hilberry joint into an inflatable robot is also a new approach. The rolling contact joint allows wide range of motion of $\pm 150 ^{\circ}$, the largest among the conventional inflatable joints. Using the proposed mechanism for inflatable robots, we demonstrated moving a 500 g payload with a 3-DoF arm and lifting 3.4 kg and 5 kg payloads with 2-DoF and 1-DoF arms, respectively. We also experimented with a single 3-DoF inflatable leg attached to a dolly to show that the proposed structure worked for legged locomotion.

</details>


### [9] [FAR-AVIO: Fast and Robust Schur-Complement Based Acoustic-Visual-Inertial Fusion Odometry with Sensor Calibration](https://arxiv.org/abs/2512.20355)
*Hao Wei,Peiji Wang,Qianhao Wang,Tong Qin,Fei Gao,Yulin Si*

Main category: cs.RO

TL;DR: FAR-AVIO：一种基于舒尔补的紧耦合声学-视觉-惯性里程计框架，专为水下机器人设计，通过EKF嵌入舒尔补实现联合位姿-地标优化，保持恒定时间更新，并引入自适应传感器健康评估模块


<details>
  <summary>Details</summary>
Motivation: 水下环境对视觉惯性里程计系统带来严峻挑战：强光衰减、海洋雪、浑浊度以及弱激励运动导致惯性可观测性下降和频繁跟踪失败。虽然紧耦合声学-视觉-惯性融合（通常通过声学多普勒速度计与视觉惯性测量集成）能提供准确状态估计，但基于图的优化计算量大，难以在资源受限平台上实时部署。

Method: FAR-AVIO将舒尔补公式嵌入扩展卡尔曼滤波器(EKF)，实现联合位姿-地标优化，同时通过高效边缘化地标状态保持恒定时间更新。在此基础上引入自适应权重调整与可靠性评估(AWARE)模块，在线评估视觉、惯性和DVL测量的可靠性并自适应调节其权重，同时开发高效的在线标定方案联合估计DVL-IMU外参，无需专用标定操作。

Result: 数值仿真和真实水下实验一致表明，FAR-AVIO在定位精度和计算效率方面均优于最先进的水下SLAM基线方法，能够在低功耗嵌入式平台上实现鲁棒运行。

Conclusion: FAR-AVIO为水下机器人提供了一种高效、准确的声学-视觉-惯性里程计解决方案，通过舒尔补EKF框架实现计算效率与精度的平衡，结合自适应传感器健康评估和在线标定功能，显著提升了水下环境中的鲁棒性和实用性。

Abstract: Underwater environments impose severe challenges to visual-inertial odometry systems, as strong light attenuation, marine snow and turbidity, together with weakly exciting motions, degrade inertial observability and cause frequent tracking failures over long-term operation. While tightly coupled acoustic-visual-inertial fusion, typically implemented through an acoustic Doppler Velocity Log (DVL) integrated with visual-inertial measurements, can provide accurate state estimation, the associated graph-based optimization is often computationally prohibitive for real-time deployment on resource-constrained platforms. Here we present FAR-AVIO, a Schur-Complement based, tightly coupled acoustic-visual-inertial odometry framework tailored for underwater robots. FAR-AVIO embeds a Schur complement formulation into an Extended Kalman Filter(EKF), enabling joint pose-landmark optimization for accuracy while maintaining constant-time updates by efficiently marginalizing landmark states. On top of this backbone, we introduce Adaptive Weight Adjustment and Reliability Evaluation(AWARE), an online sensor health module that continuously assesses the reliability of visual, inertial and DVL measurements and adaptively regulates their sigma weights, and we develop an efficient online calibration scheme that jointly estimates DVL-IMU extrinsics, without dedicated calibration manoeuvres. Numerical simulations and real-world underwater experiments consistently show that FAR-AVIO outperforms state-of-the-art underwater SLAM baselines in both localization accuracy and computational efficiency, enabling robust operation on low-power embedded platforms. Our implementation has been released as open source software at https://far-vido.gitbook.io/far-vido-docs.

</details>


### [10] [Drift-Corrected Monocular VIO and Perception-Aware Planning for Autonomous Drone Racing](https://arxiv.org/abs/2512.20475)
*Maulana Bisyir Azhari,Donghun Han,Je In You,Sungjun Park,David Hyunchul Shim*

Main category: cs.RO

TL;DR: 本文介绍了为阿布扎比自主赛车联盟(A2RL) x 无人机冠军联赛(DCL)开发的单目视觉自主无人机竞速系统，该系统通过卡尔曼滤波器融合视觉惯性里程计和基于YOLO的门检测器，结合感知感知规划器，在比赛中获得多项优异成绩。


<details>
  <summary>Details</summary>
Motivation: A2RL x DCL竞赛要求仅使用单摄像头和低质量IMU进行高速自主无人机竞速，这种最小传感器配置容易导致视觉惯性里程计在长时间高速飞行和激进机动时产生漂移，需要开发有效的漂移校正系统。

Method: 1. 使用卡尔曼滤波器融合VIO输出和基于YOLO的门检测器获得的全局位置测量，校正VIO漂移；2. 开发感知感知规划器，生成平衡速度和门可见性的轨迹；3. 构建完整的单目视觉自主无人机飞行架构。

Result: 系统在比赛中表现优异：AI Grand Challenge第三名（最高速度43.2 km/h），AI Drag Race第二名（速度超过59 km/h），AI Multi-Drone Race第二名。系统展示了单目视觉自主无人机飞行的高性能。

Conclusion: 本文详细介绍了成功的单目视觉自主无人机竞速系统架构，通过融合VIO和门检测器校正漂移，结合感知感知规划，在竞赛中取得优异成绩，为基于单目视觉的自主无人机飞行提供了有价值的见解。

Abstract: The Abu Dhabi Autonomous Racing League(A2RL) x Drone Champions League competition(DCL) requires teams to perform high-speed autonomous drone racing using only a single camera and a low-quality inertial measurement unit -- a minimal sensor set that mirrors expert human drone racing pilots. This sensor limitation makes the system susceptible to drift from Visual-Inertial Odometry (VIO), particularly during long and fast flights with aggressive maneuvers. This paper presents the system developed for the championship, which achieved a competitive performance. Our approach corrected VIO drift by fusing its output with global position measurements derived from a YOLO-based gate detector using a Kalman filter. A perception-aware planner generated trajectories that balance speed with the need to keep gates visible for the perception system. The system demonstrated high performance, securing podium finishes across multiple categories: third place in the AI Grand Challenge with top speed of 43.2 km/h, second place in the AI Drag Race with over 59 km/h, and second place in the AI Multi-Drone Race. We detail the complete architecture and present a performance analysis based on experimental data from the competition, contributing our insights on building a successful system for monocular vision-based autonomous drone flight.

</details>


### [11] [LightTact: A Visual-Tactile Fingertip Sensor for Deformation-Independent Contact Sensing](https://arxiv.org/abs/2512.20591)
*Changyi Lin,Boda Huo,Mingyang Yu,Emily Ruppel,Bingqing Chen,Jonathan Francis,Ding Zhao*

Main category: cs.RO

TL;DR: LightTact是一种新型视觉触觉指尖传感器，通过光学原理直接可视化接触，无需依赖表面变形，能够感知液体、半液体和超软材料的轻接触


<details>
  <summary>Details</summary>
Motivation: 现有触觉传感器大多依赖表面变形来推断接触，难以可靠感知与液体、半液体或超软材料的轻接触交互

Method: 采用环境光阻挡光学配置，抑制非接触区域的外部光和内部照明，仅传输真实接触处产生的漫射光，实现高对比度原始图像

Result: LightTact产生高对比度原始图像，非接触像素保持近黑色，接触像素保留接触表面的自然外观，实现像素级接触分割，对材料特性、接触力、表面外观和环境光照具有鲁棒性

Conclusion: LightTact能够实现极轻接触驱动的操作行为，其空间对齐的视觉触觉图像可直接由现有视觉语言模型解释，为机器人感知和操作提供了新方法

Abstract: Contact often occurs without macroscopic surface deformation, such as during interaction with liquids, semi-liquids, or ultra-soft materials. Most existing tactile sensors rely on deformation to infer contact, making such light-contact interactions difficult to perceive robustly. To address this, we present LightTact, a visual-tactile fingertip sensor that makes contact directly visible via a deformation-independent, optics-based principle. LightTact uses an ambient-blocking optical configuration that suppresses both external light and internal illumination at non-contact regions, while transmitting only the diffuse light generated at true contacts. As a result, LightTact produces high-contrast raw images in which non-contact pixels remain near-black (mean gray value < 3) and contact pixels preserve the natural appearance of the contacting surface. Built on this, LightTact achieves accurate pixel-level contact segmentation that is robust to material properties, contact force, surface appearance, and environmental lighting. We further integrate LightTact on a robotic arm and demonstrate manipulation behaviors driven by extremely light contact, including water spreading, facial-cream dipping, and thin-film interaction. Finally, we show that LightTact's spatially aligned visual-tactile images can be directly interpreted by existing vision-language models, enabling resistor value reasoning for robotic sorting.

</details>
