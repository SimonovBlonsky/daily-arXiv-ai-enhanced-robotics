{"id": "2602.17737", "categories": ["cs.RO", "cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2602.17737", "abs": "https://arxiv.org/abs/2602.17737", "authors": ["Upasana Biswas", "Durgesh Kalwar", "Subbarao Kambhampati", "Sarath Sreedharan"], "title": "Nested Training for Mutual Adaptation in Human-AI Teaming", "comment": null, "summary": "Mutual adaptation is a central challenge in human--AI teaming, as humans naturally adjust their strategies in response to a robot's policy. Existing approaches aim to improve diversity in training partners to approximate human behavior, but these partners are static and fail to capture adaptive behavior of humans. Exposing robots to adaptive behaviors is critical, yet when both agents learn simultaneously in a multi-agent setting, they often converge to opaque implicit coordination strategies that only work with the agents they were co-trained with. Such agents fail to generalize when paired with new partners. In order to capture the adaptive behavior of humans, we model the human-robot teaming scenario as an Interactive Partially Observable Markov Decision Process (I-POMDP), explicitly modeling human adaptation as part of the state. We propose a nested training regime to approximately learn the solution to a finite-level I-POMDP. In this framework, agents at each level are trained against adaptive agents from the level below. This ensures that the ego agent is exposed to adaptive behavior during training while avoiding the emergence of implicit coordination strategies, since the training partners are not themselves learning. We train our method in a multi-episode, required cooperation setup in the Overcooked domain, comparing it against several baseline agents designed for human-robot teaming. We evaluate the performance of our agent when paired with adaptive partners that were not seen during training. Our results demonstrate that our agent not only achieves higher task performance with these adaptive partners but also exhibits significantly greater adaptability during team interactions.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8eI-POMDP\u7684\u5d4c\u5957\u8bad\u7ec3\u65b9\u6cd5\uff0c\u89e3\u51b3\u4eba\u673a\u534f\u4f5c\u4e2d\u4eba\u7c7b\u9002\u5e94\u6027\u884c\u4e3a\u5efa\u6a21\u95ee\u9898\uff0c\u907f\u514d\u9690\u5f0f\u534f\u8c03\u7b56\u7565\uff0c\u63d0\u9ad8\u4e0e\u672a\u89c1\u8fc7\u7684\u81ea\u9002\u5e94\u4f19\u4f34\u7684\u534f\u4f5c\u6027\u80fd", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f7f\u7528\u9759\u6001\u8bad\u7ec3\u4f19\u4f34\u65e0\u6cd5\u6355\u6349\u4eba\u7c7b\u7684\u9002\u5e94\u6027\u884c\u4e3a\uff0c\u800c\u540c\u65f6\u5b66\u4e60\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4f1a\u4ea7\u751f\u4ec5\u9002\u7528\u4e8e\u7279\u5b9a\u4f19\u4f34\u7684\u9690\u5f0f\u534f\u8c03\u7b56\u7565\uff0c\u7f3a\u4e4f\u6cdb\u5316\u80fd\u529b", "method": "\u5c06\u4eba\u673a\u534f\u4f5c\u5efa\u6a21\u4e3aI-POMDP\uff0c\u5c06\u4eba\u7c7b\u9002\u5e94\u884c\u4e3a\u4f5c\u4e3a\u72b6\u6001\u7684\u4e00\u90e8\u5206\uff0c\u91c7\u7528\u5d4c\u5957\u8bad\u7ec3\u673a\u5236\u8fd1\u4f3c\u5b66\u4e60\u6709\u9650\u5c42\u7ea7I-POMDP\u7684\u89e3\uff0c\u6bcf\u5c42\u667a\u80fd\u4f53\u4e0e\u4e0b\u4e00\u5c42\u7684\u81ea\u9002\u5e94\u667a\u80fd\u4f53\u8bad\u7ec3", "result": "\u5728Overcooked\u9886\u57df\u8fdb\u884c\u591a\u56de\u5408\u5fc5\u9700\u5408\u4f5c\u5b9e\u9a8c\uff0c\u4e0e\u591a\u79cd\u57fa\u7ebf\u65b9\u6cd5\u6bd4\u8f83\uff0c\u7ed3\u679c\u663e\u793a\u8be5\u65b9\u6cd5\u4e0e\u672a\u89c1\u8fc7\u7684\u81ea\u9002\u5e94\u4f19\u4f34\u534f\u4f5c\u65f6\u83b7\u5f97\u66f4\u9ad8\u4efb\u52a1\u6027\u80fd\uff0c\u8868\u73b0\u51fa\u66f4\u5f3a\u7684\u9002\u5e94\u6027", "conclusion": "\u901a\u8fc7I-POMDP\u5efa\u6a21\u4eba\u7c7b\u9002\u5e94\u6027\u548c\u5d4c\u5957\u8bad\u7ec3\u673a\u5236\uff0c\u80fd\u591f\u6709\u6548\u89e3\u51b3\u4eba\u673a\u534f\u4f5c\u4e2d\u7684\u4e92\u9002\u5e94\u95ee\u9898\uff0c\u63d0\u9ad8\u667a\u80fd\u4f53\u4e0e\u4e0d\u540c\u81ea\u9002\u5e94\u4f19\u4f34\u7684\u6cdb\u5316\u80fd\u529b\u548c\u534f\u4f5c\u6027\u80fd"}}
{"id": "2602.17908", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.17908", "abs": "https://arxiv.org/abs/2602.17908", "authors": ["Mingzhang Zhu", "Alvin Zhu", "Jose Victor S. H. Ramos", "Beom Jun Kim", "Yike Shi", "Yufeng Wu", "Ruochen Hou", "Quanyou Wang", "Eric Song", "Tony Fan", "Yuchen Cui", "Dennis W. Hong"], "title": "WHED: A Wearable Hand Exoskeleton for Natural, High-Quality Demonstration Collection", "comment": "7 pages, 9 figures, submitted to IEEE UR", "summary": "Scalable learning of dexterous manipulation remains bottlenecked by the difficulty of collecting natural, high-fidelity human demonstrations of multi-finger hands due to occlusion, complex hand kinematics, and contact-rich interactions. We present WHED, a wearable hand-exoskeleton system designed for in-the-wild demonstration capture, guided by two principles: wearability-first operation for extended use and a pose-tolerant, free-to-move thumb coupling that preserves natural thumb behaviors while maintaining a consistent mapping to the target robot thumb degrees of freedom. WHED integrates a linkage-driven finger interface with passive fit accommodation, a modified passive hand with robust proprioceptive sensing, and an onboard sensing/power module. We also provide an end-to-end data pipeline that synchronizes joint encoders, AR-based end-effector pose, and wrist-mounted visual observations, and supports post-processing for time alignment and replay. We demonstrate feasibility on representative grasping and manipulation sequences spanning precision pinch and full-hand enclosure grasps, and show qualitative consistency between collected demonstrations and replayed executions.", "AI": {"tldr": "WHED\u662f\u4e00\u4e2a\u53ef\u7a7f\u6234\u624b\u90e8\u5916\u9aa8\u9abc\u7cfb\u7edf\uff0c\u7528\u4e8e\u5728\u81ea\u7136\u73af\u5883\u4e2d\u6355\u6349\u4eba\u7c7b\u624b\u90e8\u64cd\u4f5c\u6f14\u793a\uff0c\u89e3\u51b3\u4e86\u591a\u6307\u624b\u6f14\u793a\u6570\u636e\u6536\u96c6\u4e2d\u7684\u906e\u6321\u3001\u590d\u6742\u8fd0\u52a8\u5b66\u548c\u63a5\u89e6\u4e30\u5bcc\u4ea4\u4e92\u7b49\u96be\u9898\u3002", "motivation": "\u7075\u5de7\u64cd\u4f5c\u7684\u53ef\u6269\u5c55\u5b66\u4e60\u9762\u4e34\u74f6\u9888\uff0c\u56e0\u4e3a\u96be\u4ee5\u6536\u96c6\u81ea\u7136\u3001\u9ad8\u4fdd\u771f\u7684\u4eba\u7c7b\u591a\u6307\u624b\u6f14\u793a\u6570\u636e\uff0c\u8fd9\u4e3b\u8981\u662f\u7531\u4e8e\u906e\u6321\u3001\u590d\u6742\u7684\u624b\u90e8\u8fd0\u52a8\u5b66\u4ee5\u53ca\u63a5\u89e6\u4e30\u5bcc\u7684\u4ea4\u4e92\u9020\u6210\u7684\u3002", "method": "\u5f00\u53d1\u4e86WHED\u53ef\u7a7f\u6234\u624b\u90e8\u5916\u9aa8\u9abc\u7cfb\u7edf\uff0c\u91c7\u7528\u53ef\u7a7f\u6234\u4f18\u5148\u8bbe\u8ba1\u539f\u5219\uff0c\u5177\u6709\u59ff\u6001\u5bb9\u5fcd\u3001\u81ea\u7531\u79fb\u52a8\u7684\u62c7\u6307\u8026\u5408\u673a\u5236\uff0c\u7ed3\u5408\u8fde\u6746\u9a71\u52a8\u7684\u624b\u6307\u63a5\u53e3\u3001\u88ab\u52a8\u9002\u5e94\u88c5\u7f6e\u3001\u6539\u8fdb\u7684\u88ab\u52a8\u624b\u90e8\u4ee5\u53ca\u672c\u4f53\u611f\u89c9\u4f20\u611f\uff0c\u5e76\u914d\u5907\u4e86\u7aef\u5230\u7aef\u6570\u636e\u7ba1\u9053\u3002", "result": "\u5728\u4ee3\u8868\u6027\u6293\u53d6\u548c\u64cd\u4f5c\u5e8f\u5217\u4e0a\u5c55\u793a\u4e86\u53ef\u884c\u6027\uff0c\u5305\u62ec\u7cbe\u786e\u634f\u53d6\u548c\u5168\u624b\u5305\u88f9\u6293\u53d6\uff0c\u5e76\u663e\u793a\u4e86\u6536\u96c6\u7684\u6f14\u793a\u4e0e\u56de\u653e\u6267\u884c\u4e4b\u95f4\u7684\u5b9a\u6027\u4e00\u81f4\u6027\u3002", "conclusion": "WHED\u7cfb\u7edf\u4e3a\u5728\u81ea\u7136\u73af\u5883\u4e2d\u6355\u6349\u4eba\u7c7b\u624b\u90e8\u64cd\u4f5c\u6f14\u793a\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u6709\u52a9\u4e8e\u89e3\u51b3\u7075\u5de7\u64cd\u4f5c\u5b66\u4e60\u4e2d\u7684\u6570\u636e\u6536\u96c6\u96be\u9898\u3002"}}
{"id": "2602.17921", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.17921", "abs": "https://arxiv.org/abs/2602.17921", "authors": ["Kei Ikemura", "Yifei Dong", "Florian T. Pokorny"], "title": "Latent Diffeomorphic Co-Design of End-Effectors for Deformable and Fragile Object Manipulation", "comment": null, "summary": "Manipulating deformable and fragile objects remains a fundamental challenge in robotics due to complex contact dynamics and strict requirements on object integrity. Existing approaches typically optimize either end-effector design or control strategies in isolation, limiting achievable performance. In this work, we present the first co-design framework that jointly optimizes end-effector morphology and manipulation control for deformable and fragile object manipulation. We introduce (1) a latent diffeomorphic shape parameterization enabling expressive yet tractable end-effector geometry optimization, (2) a stress-aware bi-level co-design pipeline coupling morphology and control optimization, and (3) a privileged-to-pointcloud policy distillation scheme for zero-shot real-world deployment. We evaluate our approach on challenging food manipulation tasks, including grasping and pushing jelly and scooping fillets. Simulation and real-world experiments demonstrate the effectiveness of the proposed method.", "AI": {"tldr": "\u9996\u4e2a\u540c\u65f6\u4f18\u5316\u672b\u7aef\u6267\u884c\u5668\u5f62\u6001\u548c\u64cd\u63a7\u7b56\u7565\u7684\u534f\u540c\u8bbe\u8ba1\u6846\u67b6\uff0c\u7528\u4e8e\u5904\u7406\u53ef\u53d8\u5f62\u548c\u6613\u788e\u7269\u4f53\u7684\u64cd\u4f5c\u4efb\u52a1\u3002", "motivation": "\u53ef\u53d8\u5f62\u548c\u6613\u788e\u7269\u4f53\u7684\u64cd\u4f5c\u662f\u673a\u5668\u4eba\u5b66\u4e2d\u7684\u57fa\u672c\u6311\u6218\uff0c\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u5355\u72ec\u4f18\u5316\u672b\u7aef\u6267\u884c\u5668\u8bbe\u8ba1\u6216\u63a7\u5236\u7b56\u7565\uff0c\u9650\u5236\u4e86\u6027\u80fd\u63d0\u5347\u3002", "method": "\u63d0\u51fa\u534f\u540c\u8bbe\u8ba1\u6846\u67b6\uff1a1) \u6f5c\u5728\u5fae\u5206\u540c\u80da\u5f62\u72b6\u53c2\u6570\u5316\u5b9e\u73b0\u672b\u7aef\u6267\u884c\u5668\u51e0\u4f55\u4f18\u5316\uff1b2) \u5e94\u529b\u611f\u77e5\u7684\u53cc\u5c42\u534f\u540c\u8bbe\u8ba1\u7ba1\u9053\u8026\u5408\u5f62\u6001\u548c\u63a7\u5236\u4f18\u5316\uff1b3) \u7279\u6743\u4fe1\u606f\u5230\u70b9\u4e91\u7684\u7b56\u7565\u84b8\u998f\u65b9\u6848\u5b9e\u73b0\u96f6\u6837\u672c\u771f\u5b9e\u4e16\u754c\u90e8\u7f72\u3002", "result": "\u5728\u5177\u6709\u6311\u6218\u6027\u7684\u98df\u7269\u64cd\u4f5c\u4efb\u52a1\uff08\u5305\u62ec\u6293\u53d6\u548c\u63a8\u52a8\u679c\u51bb\u3001\u8200\u53d6\u9c7c\u7247\uff09\u4e0a\u8fdb\u884c\u8bc4\u4f30\uff0c\u4eff\u771f\u548c\u771f\u5b9e\u4e16\u754c\u5b9e\u9a8c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u534f\u540c\u8bbe\u8ba1\u6846\u67b6\u901a\u8fc7\u8054\u5408\u4f18\u5316\u672b\u7aef\u6267\u884c\u5668\u5f62\u6001\u548c\u64cd\u63a7\u7b56\u7565\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u53ef\u53d8\u5f62\u548c\u6613\u788e\u7269\u4f53\u7684\u64cd\u4f5c\u96be\u9898\u3002"}}
{"id": "2602.17926", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.17926", "abs": "https://arxiv.org/abs/2602.17926", "authors": ["Jennifer Wakulicz", "Ki Myung Brian Lee", "Teresa Vidal-Calleja", "Robert Fitch"], "title": "Homotopic information gain for sparse active target tracking", "comment": "12 pages, 12 figures, accepted to Transactions on Robotics", "summary": "The problem of planning sensing trajectories for a mobile robot to collect observations of a target and predict its future trajectory is known as active target tracking. Enabled by probabilistic motion models, one may solve this problem by exploring the belief space of all trajectory predictions given future sensing actions to maximise information gain. However, for multi-modal motion models the notion of information gain is often ill-defined. This paper proposes a planning approach designed around maximising information regarding the target's homotopy class, or high-level motion. We introduce homotopic information gain, a measure of the expected high-level trajectory information given by a measurement. We show that homotopic information gain is a lower bound for metric or low-level information gain, and is as sparsely distributed in the environment as obstacles are. Planning sensing trajectories to maximise homotopic information results in highly accurate trajectory estimates with fewer measurements than a metric information approach, as supported by our empirical evaluation on real and simulated pedestrian data.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u4e3b\u52a8\u76ee\u6807\u8ddf\u8e2a\u7684\u89c4\u5212\u65b9\u6cd5\uff0c\u901a\u8fc7\u6700\u5927\u5316\u76ee\u6807\u540c\u4f26\u7c7b\u4fe1\u606f\u6765\u4f18\u5316\u4f20\u611f\u5668\u8f68\u8ff9\u89c4\u5212\uff0c\u76f8\u6bd4\u4f20\u7edf\u5ea6\u91cf\u4fe1\u606f\u65b9\u6cd5\u80fd\u66f4\u6709\u6548\u5730\u9884\u6d4b\u76ee\u6807\u8f68\u8ff9\u3002", "motivation": "\u5728\u591a\u6a21\u6001\u8fd0\u52a8\u6a21\u578b\u4e2d\uff0c\u4fe1\u606f\u589e\u76ca\u7684\u6982\u5ff5\u5e38\u5e38\u5b9a\u4e49\u4e0d\u6e05\u3002\u4f20\u7edf\u65b9\u6cd5\u63a2\u7d22\u6240\u6709\u8f68\u8ff9\u9884\u6d4b\u7684\u4fe1\u5ff5\u7a7a\u95f4\u4ee5\u6700\u5927\u5316\u4fe1\u606f\u589e\u76ca\uff0c\u4f46\u5bf9\u4e8e\u591a\u6a21\u6001\u8fd0\u52a8\u6a21\u578b\uff0c\u8fd9\u79cd\u5ea6\u91cf\u4fe1\u606f\u589e\u76ca\u7684\u65b9\u6cd5\u5b58\u5728\u5c40\u9650\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u89c4\u5212\u65b9\u6cd5\uff0c\u56f4\u7ed5\u6700\u5927\u5316\u76ee\u6807\u540c\u4f26\u7c7b\uff08\u9ad8\u5c42\u6b21\u8fd0\u52a8\uff09\u4fe1\u606f\u8fdb\u884c\u8bbe\u8ba1\u3002\u5f15\u5165\u4e86\u540c\u4f26\u4fe1\u606f\u589e\u76ca\u7684\u6982\u5ff5\uff0c\u4f5c\u4e3a\u7ed9\u5b9a\u6d4b\u91cf\u6240\u671f\u671b\u7684\u9ad8\u5c42\u6b21\u8f68\u8ff9\u4fe1\u606f\u7684\u5ea6\u91cf\u3002\u8bc1\u660e\u4e86\u540c\u4f26\u4fe1\u606f\u589e\u76ca\u662f\u5ea6\u91cf\u6216\u4f4e\u5c42\u6b21\u4fe1\u606f\u589e\u76ca\u7684\u4e0b\u754c\uff0c\u5e76\u4e14\u5728\u73af\u5883\u4e2d\u5206\u5e03\u7a00\u758f\u5982\u969c\u788d\u7269\u3002", "result": "\u901a\u8fc7\u771f\u5b9e\u548c\u6a21\u62df\u7684\u884c\u4eba\u6570\u636e\u5b9e\u8bc1\u8bc4\u4f30\u8868\u660e\uff0c\u89c4\u5212\u4f20\u611f\u5668\u8f68\u8ff9\u4ee5\u6700\u5927\u5316\u540c\u4f26\u4fe1\u606f\u589e\u76ca\uff0c\u76f8\u6bd4\u5ea6\u91cf\u4fe1\u606f\u65b9\u6cd5\uff0c\u80fd\u591f\u4ee5\u66f4\u5c11\u7684\u6d4b\u91cf\u83b7\u5f97\u9ad8\u5ea6\u51c6\u786e\u7684\u8f68\u8ff9\u4f30\u8ba1\u3002", "conclusion": "\u540c\u4f26\u4fe1\u606f\u589e\u76ca\u4e3a\u591a\u6a21\u6001\u8fd0\u52a8\u6a21\u578b\u4e0b\u7684\u4e3b\u52a8\u76ee\u6807\u8ddf\u8e2a\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u89c4\u5212\u65b9\u6cd5\uff0c\u80fd\u591f\u66f4\u9ad8\u6548\u5730\u9884\u6d4b\u76ee\u6807\u7684\u9ad8\u5c42\u6b21\u8fd0\u52a8\u8f68\u8ff9\u3002"}}
{"id": "2602.18014", "categories": ["cs.RO", "eess.SY", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.18014", "abs": "https://arxiv.org/abs/2602.18014", "authors": ["Unnati Nigam", "Radhendushka Srivastava", "Faezeh Marzbanrad", "Michael Burke"], "title": "Quasi-Periodic Gaussian Process Predictive Iterative Learning Control", "comment": null, "summary": "Repetitive motion tasks are common in robotics, but performance can degrade over time due to environmental changes and robot wear and tear. Iterative learning control (ILC) improves performance by using information from previous iterations to compensate for expected errors in future iterations. This work incorporates the use of Quasi-Periodic Gaussian Processes (QPGPs) into a predictive ILC framework to model and forecast disturbances and drift across iterations. Using a recent structural equation formulation of QPGPs, the proposed approach enables efficient inference with complexity $\\mathcal{O}(p^3)$ instead of $\\mathcal{O}(i^2p^3)$, where $p$ denotes the number of points within an iteration and $i$ represents the total number of iterations, specially for larger $i$. This formulation also enables parameter estimation without loss of information, making continual GP learning computationally feasible within the control loop. By predicting next-iteration error profiles rather than relying only on past errors, the controller achieves faster convergence and maintains this under time-varying disturbances. We benchmark the method against both standard ILC and conventional Gaussian Process (GP)-based predictive ILC on three tasks, autonomous vehicle trajectory tracking, a three-link robotic manipulator, and a real-world Stretch robot experiment. Across all cases, the proposed approach converges faster and remains robust under injected and natural disturbances while reducing computational cost. This highlights its practicality across a range of repetitive dynamical systems.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5c06\u51c6\u5468\u671f\u9ad8\u65af\u8fc7\u7a0b\uff08QPGP\uff09\u878d\u5165\u9884\u6d4b\u6027\u8fed\u4ee3\u5b66\u4e60\u63a7\u5236\uff08ILC\uff09\u6846\u67b6\uff0c\u7528\u4e8e\u5efa\u6a21\u548c\u9884\u6d4b\u91cd\u590d\u8fd0\u52a8\u4efb\u52a1\u4e2d\u7684\u6270\u52a8\u548c\u6f02\u79fb\uff0c\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\u5e76\u5b9e\u73b0\u66f4\u5feb\u7684\u6536\u655b\u3002", "motivation": "\u673a\u5668\u4eba\u91cd\u590d\u8fd0\u52a8\u4efb\u52a1\u4e2d\uff0c\u73af\u5883\u53d8\u5316\u548c\u673a\u5668\u4eba\u78e8\u635f\u4f1a\u5bfc\u81f4\u6027\u80fd\u968f\u65f6\u95f4\u4e0b\u964d\u3002\u4f20\u7edf\u8fed\u4ee3\u5b66\u4e60\u63a7\u5236\uff08ILC\uff09\u867d\u7136\u80fd\u5229\u7528\u5148\u524d\u8fed\u4ee3\u4fe1\u606f\u8865\u507f\u9884\u671f\u8bef\u5dee\uff0c\u4f46\u5904\u7406\u65f6\u53d8\u6270\u52a8\u548c\u8ba1\u7b97\u6548\u7387\u65b9\u9762\u4ecd\u6709\u5c40\u9650\u3002", "method": "\u91c7\u7528\u51c6\u5468\u671f\u9ad8\u65af\u8fc7\u7a0b\uff08QPGP\uff09\u7684\u7ed3\u6784\u65b9\u7a0b\u516c\u5f0f\u5316\uff0c\u5c06\u5176\u878d\u5165\u9884\u6d4b\u6027ILC\u6846\u67b6\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7QPGP\u5efa\u6a21\u548c\u9884\u6d4b\u8de8\u8fed\u4ee3\u7684\u6270\u52a8\u548c\u6f02\u79fb\uff0c\u8ba1\u7b97\u590d\u6742\u5ea6\u4eceO(i\u00b2p\u00b3)\u964d\u81f3O(p\u00b3)\uff0c\u5176\u4e2dp\u4e3a\u5355\u8fed\u4ee3\u5185\u70b9\u6570\uff0ci\u4e3a\u603b\u8fed\u4ee3\u6570\u3002\u540c\u65f6\u5b9e\u73b0\u65e0\u4fe1\u606f\u635f\u5931\u7684\u53c2\u6570\u4f30\u8ba1\uff0c\u4f7f\u6301\u7eedGP\u5b66\u4e60\u5728\u63a7\u5236\u56de\u8def\u4e2d\u8ba1\u7b97\u53ef\u884c\u3002", "result": "\u5728\u4e09\u4e2a\u4efb\u52a1\uff08\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u8f68\u8ff9\u8ddf\u8e2a\u3001\u4e09\u8fde\u6746\u673a\u68b0\u81c2\u3001\u771f\u5b9eStretch\u673a\u5668\u4eba\u5b9e\u9a8c\uff09\u4e2d\uff0c\u8be5\u65b9\u6cd5\u76f8\u6bd4\u6807\u51c6ILC\u548c\u4f20\u7edfGP\u9884\u6d4b\u6027ILC\u6536\u655b\u66f4\u5feb\uff0c\u5728\u6ce8\u5165\u548c\u81ea\u7136\u6270\u52a8\u4e0b\u4fdd\u6301\u9c81\u68d2\u6027\uff0c\u540c\u65f6\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u3002", "conclusion": "QPGP\u9884\u6d4b\u6027ILC\u65b9\u6cd5\u4e3a\u91cd\u590d\u52a8\u529b\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u9ad8\u6548\u5efa\u6a21\u65f6\u53d8\u6270\u52a8\u5b9e\u73b0\u66f4\u5feb\u6536\u655b\u548c\u9c81\u68d2\u6027\u80fd\uff0c\u8ba1\u7b97\u6548\u7387\u63d0\u5347\u4f7f\u5176\u9002\u7528\u4e8e\u5b9e\u9645\u5e94\u7528\u573a\u666f\u3002"}}
{"id": "2602.18071", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.18071", "abs": "https://arxiv.org/abs/2602.18071", "authors": ["Boyuan An", "Zhexiong Wang", "Yipeng Wang", "Jiaqi Li", "Sihang Li", "Jing Zhang", "Chen Feng"], "title": "EgoPush: Learning End-to-End Egocentric Multi-Object Rearrangement for Mobile Robots", "comment": "18 pages, 13 figures. Project page: https://ai4ce.github.io/EgoPush/", "summary": "Humans can rearrange objects in cluttered environments using egocentric perception, navigating occlusions without global coordinates. Inspired by this capability, we study long-horizon multi-object non-prehensile rearrangement for mobile robots using a single egocentric camera. We introduce EgoPush, a policy learning framework that enables egocentric, perception-driven rearrangement without relying on explicit global state estimation that often fails in dynamic scenes. EgoPush designs an object-centric latent space to encode relative spatial relations among objects, rather than absolute poses. This design enables a privileged reinforcement-learning (RL) teacher to jointly learn latent states and mobile actions from sparse keypoints, which is then distilled into a purely visual student policy. To reduce the supervision gap between the omniscient teacher and the partially observed student, we restrict the teacher's observations to visually accessible cues. This induces active perception behaviors that are recoverable from the student's viewpoint. To address long-horizon credit assignment, we decompose rearrangement into stage-level subproblems using temporally decayed, stage-local completion rewards. Extensive simulation experiments demonstrate that EgoPush significantly outperforms end-to-end RL baselines in success rate, with ablation studies validating each design choice. We further demonstrate zero-shot sim-to-real transfer on a mobile platform in the real world. Code and videos are available at https://ai4ce.github.io/EgoPush/.", "AI": {"tldr": "EgoPush\u662f\u4e00\u4e2a\u7528\u4e8e\u79fb\u52a8\u673a\u5668\u4eba\u591a\u7269\u4f53\u975e\u6293\u53d6\u91cd\u6392\u7684\u6846\u67b6\uff0c\u4f7f\u7528\u5355\u76ee\u81ea\u6211\u4e2d\u5fc3\u76f8\u673a\uff0c\u901a\u8fc7\u7269\u4f53\u4e2d\u5fc3\u6f5c\u5728\u7a7a\u95f4\u7f16\u7801\u76f8\u5bf9\u7a7a\u95f4\u5173\u7cfb\uff0c\u907f\u514d\u4e86\u5168\u5c40\u72b6\u6001\u4f30\u8ba1\u7684\u4f9d\u8d56\u3002", "motivation": "\u53d7\u4eba\u7c7b\u5728\u6742\u4e71\u73af\u5883\u4e2d\u4ec5\u51ed\u81ea\u6211\u4e2d\u5fc3\u611f\u77e5\u5c31\u80fd\u91cd\u6392\u7269\u4f53\u7684\u80fd\u529b\u542f\u53d1\uff0c\u7814\u7a76\u79fb\u52a8\u673a\u5668\u4eba\u5728\u4e0d\u4f7f\u7528\u5168\u5c40\u5750\u6807\u7684\u60c5\u51b5\u4e0b\u8fdb\u884c\u957f\u65f6\u7a0b\u591a\u7269\u4f53\u975e\u6293\u53d6\u91cd\u6392\u7684\u95ee\u9898\u3002", "method": "\u8bbe\u8ba1\u7269\u4f53\u4e2d\u5fc3\u6f5c\u5728\u7a7a\u95f4\u7f16\u7801\u7269\u4f53\u95f4\u76f8\u5bf9\u7a7a\u95f4\u5173\u7cfb\uff1b\u4f7f\u7528\u7279\u6743\u5f3a\u5316\u5b66\u4e60\u6559\u5e08\u8054\u5408\u5b66\u4e60\u6f5c\u5728\u72b6\u6001\u548c\u79fb\u52a8\u52a8\u4f5c\uff0c\u7136\u540e\u84b8\u998f\u5230\u7eaf\u89c6\u89c9\u5b66\u751f\u7b56\u7565\uff1b\u9650\u5236\u6559\u5e08\u89c2\u5bdf\u4e3a\u89c6\u89c9\u53ef\u8bbf\u95ee\u7ebf\u7d22\u4ee5\u51cf\u5c11\u76d1\u7763\u5dee\u8ddd\uff1b\u4f7f\u7528\u65f6\u95f4\u8870\u51cf\u7684\u9636\u6bb5\u5c40\u90e8\u5b8c\u6210\u5956\u52b1\u5206\u89e3\u957f\u65f6\u7a0b\u4efb\u52a1\u3002", "result": "\u5927\u91cf\u4eff\u771f\u5b9e\u9a8c\u8868\u660eEgoPush\u5728\u6210\u529f\u7387\u4e0a\u663e\u8457\u4f18\u4e8e\u7aef\u5230\u7aef\u5f3a\u5316\u5b66\u4e60\u57fa\u7ebf\uff0c\u6d88\u878d\u7814\u7a76\u9a8c\u8bc1\u4e86\u6bcf\u4e2a\u8bbe\u8ba1\u9009\u62e9\u7684\u6709\u6548\u6027\uff0c\u5e76\u5c55\u793a\u4e86\u5728\u771f\u5b9e\u79fb\u52a8\u5e73\u53f0\u4e0a\u7684\u96f6\u6837\u672c\u4eff\u771f\u5230\u73b0\u5b9e\u8fc1\u79fb\u80fd\u529b\u3002", "conclusion": "EgoPush\u6846\u67b6\u6210\u529f\u5b9e\u73b0\u4e86\u57fa\u4e8e\u81ea\u6211\u4e2d\u5fc3\u611f\u77e5\u7684\u79fb\u52a8\u673a\u5668\u4eba\u591a\u7269\u4f53\u91cd\u6392\uff0c\u65e0\u9700\u4f9d\u8d56\u5bb9\u6613\u5728\u52a8\u6001\u573a\u666f\u4e2d\u5931\u6548\u7684\u5168\u5c40\u72b6\u6001\u4f30\u8ba1\uff0c\u4e3a\u673a\u5668\u4eba\u64cd\u4f5c\u63d0\u4f9b\u4e86\u65b0\u7684\u611f\u77e5\u9a71\u52a8\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.18097", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.18097", "abs": "https://arxiv.org/abs/2602.18097", "authors": ["Aarati Andrea Noronha", "Jean Oh"], "title": "Interacting safely with cyclists using Hamilton-Jacobi reachability and reinforcement learning", "comment": "7 pages. This manuscript was completed in 2020 as part of the first author's graduate thesis at Carnegie Mellon University", "summary": "In this paper, we present a framework for enabling autonomous vehicles to interact with cyclists in a manner that balances safety and optimality. The approach integrates Hamilton-Jacobi reachability analysis with deep Q-learning to jointly address safety guarantees and time-efficient navigation. A value function is computed as the solution to a time-dependent Hamilton-Jacobi-Bellman inequality, providing a quantitative measure of safety for each system state. This safety metric is incorporated as a structured reward signal within a reinforcement learning framework. The method further models the cyclist's latent response to the vehicle, allowing disturbance inputs to reflect human comfort and behavioral adaptation. The proposed framework is evaluated through simulation and comparison with human driving behavior and an existing state-of-the-art method.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u7ed3\u5408\u54c8\u5bc6\u987f-\u96c5\u53ef\u6bd4\u53ef\u8fbe\u6027\u5206\u6790\u4e0e\u6df1\u5ea6Q\u5b66\u4e60\u7684\u6846\u67b6\uff0c\u4f7f\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u80fd\u591f\u4ee5\u5e73\u8861\u5b89\u5168\u6027\u548c\u6700\u4f18\u6027\u7684\u65b9\u5f0f\u4e0e\u9a91\u884c\u8005\u4ea4\u4e92\u3002", "motivation": "\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u9700\u8981\u4e0e\u9a91\u884c\u8005\u5b89\u5168\u4ea4\u4e92\uff0c\u540c\u65f6\u4fdd\u6301\u884c\u9a76\u6548\u7387\u3002\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u540c\u65f6\u4fdd\u8bc1\u5b89\u5168\u6027\u548c\u65f6\u95f4\u6700\u4f18\u6027\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u5e73\u8861\u4e24\u8005\u7684\u6846\u67b6\u3002", "method": "\u6574\u5408\u54c8\u5bc6\u987f-\u96c5\u53ef\u6bd4\u53ef\u8fbe\u6027\u5206\u6790\u4e0e\u6df1\u5ea6Q\u5b66\u4e60\uff1a1) \u901a\u8fc7\u6c42\u89e3\u65f6\u95f4\u76f8\u5173\u7684\u54c8\u5bc6\u987f-\u96c5\u53ef\u6bd4-\u8d1d\u5c14\u66fc\u4e0d\u7b49\u5f0f\u8ba1\u7b97\u503c\u51fd\u6570\uff0c\u4e3a\u6bcf\u4e2a\u7cfb\u7edf\u72b6\u6001\u63d0\u4f9b\u5b89\u5168\u5ea6\u91cf\uff1b2) \u5c06\u5b89\u5168\u5ea6\u91cf\u4f5c\u4e3a\u7ed3\u6784\u5316\u5956\u52b1\u4fe1\u53f7\u878d\u5165\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff1b3) \u5efa\u6a21\u9a91\u884c\u8005\u5bf9\u8f66\u8f86\u7684\u6f5c\u5728\u54cd\u5e94\uff0c\u4f7f\u6270\u52a8\u8f93\u5165\u53cd\u6620\u4eba\u7c7b\u8212\u9002\u5ea6\u548c\u884c\u4e3a\u9002\u5e94\u3002", "result": "\u901a\u8fc7\u4eff\u771f\u8bc4\u4f30\uff0c\u5e76\u4e0e\u4eba\u7c7b\u9a7e\u9a76\u884c\u4e3a\u548c\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u8fdb\u884c\u6bd4\u8f83\uff0c\u9a8c\u8bc1\u4e86\u6240\u63d0\u6846\u67b6\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u6210\u529f\u5b9e\u73b0\u4e86\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u4e0e\u9a91\u884c\u8005\u4ea4\u4e92\u4e2d\u5b89\u5168\u6027\u4e0e\u6700\u4f18\u6027\u7684\u5e73\u8861\uff0c\u4e3a\u590d\u6742\u4ea4\u901a\u73af\u5883\u4e2d\u7684\u8f66\u8f86-\u9a91\u884c\u8005\u4ea4\u4e92\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.18164", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.18164", "abs": "https://arxiv.org/abs/2602.18164", "authors": ["Jonas Frey", "Turcan Tuna", "Frank Fu", "Katharine Patterson", "Tianao Xu", "Maurice Fallon", "Cesar Cadena", "Marco Hutter"], "title": "GrandTour: A Legged Robotics Dataset in the Wild for Multi-Modal Perception and State Estimation", "comment": "Jonas Frey and Turcan Tuna contributed equally. Submitted to Sage The International Journal of Robotics Research", "summary": "Accurate state estimation and multi-modal perception are prerequisites for autonomous legged robots in complex, large-scale environments. To date, no large-scale public legged-robot dataset captures the real-world conditions needed to develop and benchmark algorithms for legged-robot state estimation, perception, and navigation. To address this, we introduce the GrandTour dataset, a multi-modal legged-robotics dataset collected across challenging outdoor and indoor environments, featuring an ANYbotics ANYmal-D quadruped equipped with the \\boxi multi-modal sensor payload. GrandTour spans a broad range of environments and operational scenarios across distinct test sites, ranging from alpine scenery and forests to demolished buildings and urban areas, and covers a wide variation in scale, complexity, illumination, and weather conditions. The dataset provides time-synchronized sensor data from spinning LiDARs, multiple RGB cameras with complementary characteristics, proprioceptive sensors, and stereo depth cameras. Moreover, it includes high-precision ground-truth trajectories from satellite-based RTK-GNSS and a Leica Geosystems total station. This dataset supports research in SLAM, high-precision state estimation, and multi-modal learning, enabling rigorous evaluation and development of new approaches to sensor fusion in legged robotic systems. With its extensive scope, GrandTour represents the largest open-access legged-robotics dataset to date. The dataset is available at https://grand-tour.leggedrobotics.com, on HuggingFace (ROS-independent), and in ROS formats, along with tools and demo resources.", "AI": {"tldr": "GrandTour\u662f\u4e00\u4e2a\u5927\u89c4\u6a21\u591a\u6a21\u6001\u56db\u8db3\u673a\u5668\u4eba\u6570\u636e\u96c6\uff0c\u5305\u542b\u5728\u5404\u79cd\u590d\u6742\u5ba4\u5185\u5916\u73af\u5883\u4e2d\u6536\u96c6\u7684\u540c\u6b65\u4f20\u611f\u5668\u6570\u636e\uff0c\u652f\u6301SLAM\u3001\u72b6\u6001\u4f30\u8ba1\u548c\u591a\u6a21\u6001\u5b66\u4e60\u7814\u7a76\u3002", "motivation": "\u76ee\u524d\u7f3a\u4e4f\u5927\u89c4\u6a21\u516c\u5f00\u7684\u56db\u8db3\u673a\u5668\u4eba\u6570\u636e\u96c6\u6765\u5f00\u53d1\u548c\u8bc4\u4f30\u5728\u590d\u6742\u3001\u5927\u89c4\u6a21\u73af\u5883\u4e2d\u7684\u72b6\u6001\u4f30\u8ba1\u3001\u611f\u77e5\u548c\u5bfc\u822a\u7b97\u6cd5\u3002", "method": "\u4f7f\u7528ANYmal-D\u56db\u8db3\u673a\u5668\u4eba\u914d\u5907\u591a\u6a21\u6001\u4f20\u611f\u5668\u8f7d\u8377\uff0c\u5728\u591a\u79cd\u6311\u6218\u6027\u73af\u5883\u4e2d\u6536\u96c6\u6570\u636e\uff0c\u5305\u62ec\u9ad8\u5c71\u3001\u68ee\u6797\u3001\u62c6\u9664\u5efa\u7b51\u548c\u57ce\u5e02\u533a\u57df\uff0c\u6db5\u76d6\u4e0d\u540c\u5c3a\u5ea6\u3001\u590d\u6742\u6027\u3001\u5149\u7167\u548c\u5929\u6c14\u6761\u4ef6\u3002", "result": "\u521b\u5efa\u4e86\u5305\u542b\u65f6\u95f4\u540c\u6b65\u7684\u6fc0\u5149\u96f7\u8fbe\u3001\u591a\u53f0RGB\u76f8\u673a\u3001\u672c\u4f53\u4f20\u611f\u5668\u3001\u7acb\u4f53\u6df1\u5ea6\u76f8\u673a\u6570\u636e\u7684\u6570\u636e\u96c6\uff0c\u5e76\u63d0\u4f9b\u9ad8\u7cbe\u5ea6\u5730\u9762\u771f\u503c\u8f68\u8ff9\uff08RTK-GNSS\u548c\u5168\u7ad9\u4eea\uff09\u3002", "conclusion": "GrandTour\u662f\u76ee\u524d\u6700\u5927\u7684\u5f00\u6e90\u56db\u8db3\u673a\u5668\u4eba\u6570\u636e\u96c6\uff0c\u652f\u6301SLAM\u3001\u9ad8\u7cbe\u5ea6\u72b6\u6001\u4f30\u8ba1\u548c\u591a\u6a21\u6001\u5b66\u4e60\u7814\u7a76\uff0c\u4e3a\u817f\u5f0f\u673a\u5668\u4eba\u7cfb\u7edf\u7684\u4f20\u611f\u5668\u878d\u5408\u65b9\u6cd5\u63d0\u4f9b\u4e86\u4e25\u683c\u7684\u8bc4\u4f30\u548c\u5f00\u53d1\u5e73\u53f0\u3002"}}
{"id": "2602.18174", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.18174", "abs": "https://arxiv.org/abs/2602.18174", "authors": ["Hyoseok Ju", "Bokeon Suh", "Giseop Kim"], "title": "Have We Mastered Scale in Deep Monocular Visual SLAM? The ScaleMaster Dataset and Benchmark", "comment": "8 pages, 9 figures, accepted to ICRA 2026", "summary": "Recent advances in deep monocular visual Simultaneous Localization and Mapping (SLAM) have achieved impressive accuracy and dense reconstruction capabilities, yet their robustness to scale inconsistency in large-scale indoor environments remains largely unexplored. Existing benchmarks are limited to room-scale or structurally simple settings, leaving critical issues of intra-session scale drift and inter-session scale ambiguity insufficiently addressed. To fill this gap, we introduce the ScaleMaster Dataset, the first benchmark explicitly designed to evaluate scale consistency under challenging scenarios such as multi-floor structures, long trajectories, repetitive views, and low-texture regions. We systematically analyze the vulnerability of state-of-the-art deep monocular visual SLAM systems to scale inconsistency, providing both quantitative and qualitative evaluations. Crucially, our analysis extends beyond traditional trajectory metrics to include a direct map-to-map quality assessment using metrics like Chamfer distance against high-fidelity 3D ground truth. Our results reveal that while recent deep monocular visual SLAM systems demonstrate strong performance on existing benchmarks, they suffer from severe scale-related failures in realistic, large-scale indoor environments. By releasing the ScaleMaster dataset and baseline results, we aim to establish a foundation for future research toward developing scale-consistent and reliable visual SLAM systems.", "AI": {"tldr": "\u8be5\u8bba\u6587\u4ecb\u7ecd\u4e86ScaleMaster\u6570\u636e\u96c6\uff0c\u8fd9\u662f\u9996\u4e2a\u4e13\u95e8\u8bc4\u4f30\u5927\u89c4\u6a21\u5ba4\u5185\u73af\u5883\u4e2d\u6df1\u5ea6\u5355\u76ee\u89c6\u89c9SLAM\u7cfb\u7edf\u5c3a\u5ea6\u4e00\u81f4\u6027\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u63ed\u793a\u4e86\u73b0\u6709\u7cfb\u7edf\u5728\u73b0\u5b9e\u573a\u666f\u4e2d\u7684\u4e25\u91cd\u5c3a\u5ea6\u6f02\u79fb\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u6df1\u5ea6\u5355\u76ee\u89c6\u89c9SLAM\u7cfb\u7edf\u5728\u7cbe\u5ea6\u548c\u7a20\u5bc6\u91cd\u5efa\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u5728\u5927\u89c4\u6a21\u5ba4\u5185\u73af\u5883\u4e2d\u7684\u5c3a\u5ea6\u4e00\u81f4\u6027\u9c81\u68d2\u6027\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u5c40\u9650\u4e8e\u623f\u95f4\u5c3a\u5ea6\u6216\u7ed3\u6784\u7b80\u5355\u7684\u8bbe\u7f6e\uff0c\u672a\u80fd\u5145\u5206\u89e3\u51b3\u4f1a\u8bdd\u5185\u5c3a\u5ea6\u6f02\u79fb\u548c\u4f1a\u8bdd\u95f4\u5c3a\u5ea6\u6a21\u7cca\u7b49\u5173\u952e\u95ee\u9898\u3002", "method": "\u4f5c\u8005\u5f15\u5165\u4e86ScaleMaster\u6570\u636e\u96c6\uff0c\u8fd9\u662f\u9996\u4e2a\u4e13\u95e8\u8bbe\u8ba1\u7528\u4e8e\u8bc4\u4f30\u5177\u6709\u6311\u6218\u6027\u573a\u666f\u4e0b\u5c3a\u5ea6\u4e00\u81f4\u6027\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u62ec\u591a\u5c42\u7ed3\u6784\u3001\u957f\u8f68\u8ff9\u3001\u91cd\u590d\u89c6\u56fe\u548c\u4f4e\u7eb9\u7406\u533a\u57df\u3002\u4ed6\u4eec\u7cfb\u7edf\u5206\u6790\u4e86\u6700\u5148\u8fdb\u7684\u6df1\u5ea6\u5355\u76ee\u89c6\u89c9SLAM\u7cfb\u7edf\u5bf9\u5c3a\u5ea6\u4e0d\u4e00\u81f4\u6027\u7684\u8106\u5f31\u6027\uff0c\u63d0\u4f9b\u4e86\u5b9a\u91cf\u548c\u5b9a\u6027\u8bc4\u4f30\u3002\u7279\u522b\u5730\uff0c\u5206\u6790\u8d85\u8d8a\u4e86\u4f20\u7edf\u7684\u8f68\u8ff9\u6307\u6807\uff0c\u5305\u62ec\u4e86\u4f7f\u7528Chamfer\u8ddd\u79bb\u7b49\u9ad8\u4fdd\u771f3D\u5730\u9762\u5b9e\u51b5\u8fdb\u884c\u76f4\u63a5\u5730\u56fe\u5230\u5730\u56fe\u8d28\u91cf\u8bc4\u4f30\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u867d\u7136\u6700\u8fd1\u7684\u6df1\u5ea6\u5355\u76ee\u89c6\u89c9SLAM\u7cfb\u7edf\u5728\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u73b0\u5b9e\u7684\u5927\u89c4\u6a21\u5ba4\u5185\u73af\u5883\u4e2d\u5b58\u5728\u4e25\u91cd\u7684\u5c3a\u5ea6\u76f8\u5173\u6545\u969c\u3002\u901a\u8fc7\u53d1\u5e03ScaleMaster\u6570\u636e\u96c6\u548c\u57fa\u7ebf\u7ed3\u679c\uff0c\u63ed\u793a\u4e86\u73b0\u6709\u7cfb\u7edf\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u5c40\u9650\u6027\u3002", "conclusion": "ScaleMaster\u6570\u636e\u96c6\u586b\u8865\u4e86\u8bc4\u4f30\u6df1\u5ea6\u5355\u76ee\u89c6\u89c9SLAM\u7cfb\u7edf\u5c3a\u5ea6\u4e00\u81f4\u6027\u7684\u7a7a\u767d\uff0c\u4e3a\u672a\u6765\u5f00\u53d1\u5c3a\u5ea6\u4e00\u81f4\u4e14\u53ef\u9760\u7684\u89c6\u89c9SLAM\u7cfb\u7edf\u7814\u7a76\u5960\u5b9a\u4e86\u57fa\u7840\u3002\u8be5\u5de5\u4f5c\u5f3a\u8c03\u4e86\u5728\u5927\u89c4\u6a21\u5ba4\u5185\u73af\u5883\u4e2d\u89e3\u51b3\u5c3a\u5ea6\u4e00\u81f4\u6027\u95ee\u9898\u7684\u91cd\u8981\u6027\uff0c\u5e76\u63d0\u4f9b\u4e86\u76f8\u5e94\u7684\u8bc4\u4f30\u5de5\u5177\u3002"}}
{"id": "2602.18212", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.18212", "abs": "https://arxiv.org/abs/2602.18212", "authors": ["Rui Chen", "Domenico Chiaradia", "Daniele Leonardis", "Antonio Frisoli"], "title": "Design and Characterization of a Dual-DOF Soft Shoulder Exosuit with Volume-Optimized Pneumatic Actuator", "comment": null, "summary": "Portable pneumatic systems for 2 degree-of-freedom (DOF) soft shoulder exosuits remain underexplored, and face fundamental trade-offs between torque output and dynamic response that are further compounded by the need for multiple actuators to support complex shoulder movement. This work addresses these constraints through a volume-optimized spindle-shaped angled actuator (SSAA) geometry: by reducing actuator volume by 35.7% (357mL vs. 555mL), the SSAA maintains 94.2% of output torque while achieving 35.2% faster dynamic response compared to uniform cylindrical designs. Building on the SSAA, we develop a curved abduction actuator (CAA) based on the SSAA geometry and a horizontal adduction actuator (HAA) based on the pouch motor principle, integrating both into a dual-DOF textile-based shoulder exosuit (390 g). The exosuit delivers multi-modal assistance spanning shoulder abduction, flexion, and horizontal adduction, depending on the actuation.\n  User studies with 10 healthy participants reveal that the exosuit substantially reduces electromyographic (EMG) activity across both shoulder abduction and flexion tasks. For abduction with HAA only, the exosuit achieved up to 59% muscle activity reduction across seven muscles. For flexion, both the single-actuator configuration (HAA only) and the dual-actuator configuration (HAA,+,CAA) reduced EMG activity by up to 63.7% compared to no assistance. However, the incremental benefit of adding the CAA to existing HAA support was limited in healthy users during flexion, with statistically significant additional reductions observed only in pectoralis major. These experimental findings characterize actuator contributions in healthy users and provide design guidance for multi-DOF exosuit systems.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5f00\u53d1\u4e86\u4e00\u79cd\u4f53\u79ef\u4f18\u5316\u7684\u7eba\u9524\u5f62\u89d2\u5ea6\u6267\u884c\u5668(SSAA)\uff0c\u7528\u4e8e\u4fbf\u643a\u5f0f\u6c14\u52a8\u8f6f\u80a9\u5916\u9aa8\u9abc\uff0c\u5728\u51cf\u5c1135.7%\u4f53\u79ef\u7684\u540c\u65f6\u4fdd\u630194.2%\u626d\u77e9\u8f93\u51fa\u548c35.2%\u66f4\u5feb\u7684\u52a8\u6001\u54cd\u5e94\uff0c\u5e76\u96c6\u6210\u5230\u53cc\u81ea\u7531\u5ea6\u7eba\u7ec7\u57fa\u80a9\u5916\u9aa8\u9abc\u4e2d\u3002", "motivation": "\u4fbf\u643a\u5f0f\u6c14\u52a82\u81ea\u7531\u5ea6\u8f6f\u80a9\u5916\u9aa8\u9abc\u7cfb\u7edf\u7814\u7a76\u4e0d\u8db3\uff0c\u9762\u4e34\u626d\u77e9\u8f93\u51fa\u4e0e\u52a8\u6001\u54cd\u5e94\u4e4b\u95f4\u7684\u57fa\u672c\u6743\u8861\uff0c\u4e14\u9700\u8981\u591a\u4e2a\u6267\u884c\u5668\u652f\u6301\u590d\u6742\u7684\u80a9\u90e8\u8fd0\u52a8\u3002", "method": "\u91c7\u7528\u4f53\u79ef\u4f18\u5316\u7684\u7eba\u9524\u5f62\u89d2\u5ea6\u6267\u884c\u5668(SSAA)\u51e0\u4f55\u8bbe\u8ba1\uff0c\u5f00\u53d1\u57fa\u4e8eSSAA\u7684\u5f2f\u66f2\u5916\u5c55\u6267\u884c\u5668(CAA)\u548c\u57fa\u4e8e\u888b\u5f0f\u7535\u673a\u539f\u7406\u7684\u6c34\u5e73\u5185\u6536\u6267\u884c\u5668(HAA)\uff0c\u96c6\u6210\u5230\u53cc\u81ea\u7531\u5ea6\u7eba\u7ec7\u57fa\u80a9\u5916\u9aa8\u9abc\u4e2d\u3002", "result": "SSAA\u76f8\u6bd4\u5747\u5300\u5706\u67f1\u8bbe\u8ba1\u51cf\u5c1135.7%\u4f53\u79ef(357mL vs. 555mL)\uff0c\u4fdd\u630194.2%\u626d\u77e9\u8f93\u51fa\uff0c\u52a8\u6001\u54cd\u5e94\u5feb35.2%\u3002\u7528\u6237\u7814\u7a76\u663e\u793a\u5916\u9aa8\u9abc\u663e\u8457\u964d\u4f4e\u80a9\u90e8\u5916\u5c55\u548c\u5c48\u66f2\u4efb\u52a1\u7684\u808c\u7535\u6d3b\u52a8\uff0c\u5916\u5c55\u65f6\u6700\u591a\u51cf\u5c1159%\u808c\u8089\u6d3b\u52a8\uff0c\u5c48\u66f2\u65f6\u6700\u591a\u51cf\u5c1163.7%\u3002", "conclusion": "\u8be5\u7814\u7a76\u901a\u8fc7\u4f18\u5316\u7684\u6267\u884c\u5668\u8bbe\u8ba1\u548c\u591a\u81ea\u7531\u5ea6\u96c6\u6210\uff0c\u4e3a\u4fbf\u643a\u5f0f\u8f6f\u80a9\u5916\u9aa8\u9abc\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u63ed\u793a\u4e86\u4e0d\u540c\u6267\u884c\u5668\u914d\u7f6e\u5728\u5065\u5eb7\u7528\u6237\u4e2d\u7684\u8d21\u732e\u5dee\u5f02\uff0c\u4e3a\u591a\u81ea\u7531\u5ea6\u5916\u9aa8\u9abc\u7cfb\u7edf\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u6307\u5bfc\u3002"}}
{"id": "2602.18258", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2602.18258", "abs": "https://arxiv.org/abs/2602.18258", "authors": ["Gwangtak Bae", "Jaeho Shin", "Seunggu Kang", "Junho Kim", "Ayoung Kim", "Young Min Kim"], "title": "RoEL: Robust Event-based 3D Line Reconstruction", "comment": "IEEE Transactions on Robotics (T-RO)", "summary": "Event cameras in motion tend to detect object boundaries or texture edges, which produce lines of brightness changes, especially in man-made environments. While lines can constitute a robust intermediate representation that is consistently observed, the sparse nature of lines may lead to drastic deterioration with minor estimation errors. Only a few previous works, often accompanied by additional sensors, utilize lines to compensate for the severe domain discrepancies of event sensors along with unpredictable noise characteristics. We propose a method that can stably extract tracks of varying appearances of lines using a clever algorithmic process that observes multiple representations from various time slices of events, compensating for potential adversaries within the event data. We then propose geometric cost functions that can refine the 3D line maps and camera poses, eliminating projective distortions and depth ambiguities. The 3D line maps are highly compact and can be equipped with our proposed cost function, which can be adapted for any observations that can detect and extract line structures or projections of them, including 3D point cloud maps or image observations. We demonstrate that our formulation is powerful enough to exhibit a significant performance boost in event-based mapping and pose refinement across diverse datasets, and can be flexibly applied to multimodal scenarios. Our results confirm that the proposed line-based formulation is a robust and effective approach for the practical deployment of event-based perceptual modules. Project page: https://gwangtak.github.io/roel/", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u4e8b\u4ef6\u76f8\u673a\u7684\u9c81\u68d2\u7ebf\u7279\u5f81\u8ddf\u8e2a\u4e0e\u5efa\u56fe\u65b9\u6cd5\uff0c\u901a\u8fc7\u591a\u65f6\u95f4\u5207\u7247\u89c2\u6d4b\u8865\u507f\u4e8b\u4ef6\u6570\u636e\u4e2d\u7684\u566a\u58f0\uff0c\u5229\u7528\u51e0\u4f55\u4ee3\u4ef7\u51fd\u6570\u4f18\u53163D\u7ebf\u5730\u56fe\u548c\u76f8\u673a\u4f4d\u59ff\uff0c\u9002\u7528\u4e8e\u591a\u6a21\u6001\u573a\u666f\u3002", "motivation": "\u4e8b\u4ef6\u76f8\u673a\u5728\u8fd0\u52a8\u4e2d\u4e3b\u8981\u68c0\u6d4b\u7269\u4f53\u8fb9\u754c\u6216\u7eb9\u7406\u8fb9\u7f18\uff0c\u4ea7\u751f\u4eae\u5ea6\u53d8\u5316\u7ebf\u3002\u867d\u7136\u7ebf\u7279\u5f81\u53ef\u4ee5\u4f5c\u4e3a\u9c81\u68d2\u7684\u4e2d\u95f4\u8868\u793a\uff0c\u4f46\u5176\u7a00\u758f\u6027\u53ef\u80fd\u5bfc\u81f4\u5fae\u5c0f\u4f30\u8ba1\u8bef\u5dee\u4e0b\u7684\u6027\u80fd\u6025\u5267\u4e0b\u964d\u3002\u73b0\u6709\u65b9\u6cd5\u5927\u591a\u9700\u8981\u989d\u5916\u4f20\u611f\u5668\uff0c\u96be\u4ee5\u5904\u7406\u4e8b\u4ef6\u4f20\u611f\u5668\u7684\u4e25\u91cd\u57df\u5dee\u5f02\u548c\u4e0d\u53ef\u9884\u6d4b\u566a\u58f0\u7279\u6027\u3002", "method": "1) \u901a\u8fc7\u89c2\u5bdf\u4e8b\u4ef6\u6570\u636e\u591a\u4e2a\u65f6\u95f4\u5207\u7247\u7684\u591a\u91cd\u8868\u793a\uff0c\u91c7\u7528\u5de7\u5999\u7684\u7b97\u6cd5\u8fc7\u7a0b\u7a33\u5b9a\u63d0\u53d6\u4e0d\u540c\u5916\u89c2\u7684\u7ebf\u7279\u5f81\u8f68\u8ff9\uff0c\u8865\u507f\u4e8b\u4ef6\u6570\u636e\u4e2d\u7684\u6f5c\u5728\u5e72\u6270\uff1b2) \u63d0\u51fa\u51e0\u4f55\u4ee3\u4ef7\u51fd\u6570\uff0c\u6d88\u9664\u6295\u5f71\u7578\u53d8\u548c\u6df1\u5ea6\u6a21\u7cca\uff0c\u4f18\u53163D\u7ebf\u5730\u56fe\u548c\u76f8\u673a\u4f4d\u59ff\uff1b3) \u6784\u5efa\u9ad8\u5ea6\u7d27\u51d1\u76843D\u7ebf\u5730\u56fe\uff0c\u5176\u4ee3\u4ef7\u51fd\u6570\u53ef\u9002\u914d\u4efb\u4f55\u80fd\u68c0\u6d4b\u548c\u63d0\u53d6\u7ebf\u7ed3\u6784\u6216\u5176\u6295\u5f71\u7684\u89c2\u6d4b\u6570\u636e\uff0c\u5305\u62ec3D\u70b9\u4e91\u5730\u56fe\u6216\u56fe\u50cf\u89c2\u6d4b\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u663e\u8457\u63d0\u5347\u4e86\u4e8b\u4ef6\u76f8\u673a\u7684\u5efa\u56fe\u548c\u4f4d\u59ff\u4f18\u5316\u6027\u80fd\uff0c\u53ef\u7075\u6d3b\u5e94\u7528\u4e8e\u591a\u6a21\u6001\u573a\u666f\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u57fa\u4e8e\u7ebf\u7279\u5f81\u7684\u516c\u5f0f\u5316\u65b9\u6cd5\u5bf9\u4e8e\u4e8b\u4ef6\u611f\u77e5\u6a21\u5757\u7684\u5b9e\u9645\u90e8\u7f72\u662f\u9c81\u68d2\u4e14\u6709\u6548\u7684\u3002", "conclusion": "\u63d0\u51fa\u7684\u7ebf\u7279\u5f81\u65b9\u6cd5\u80fd\u591f\u7a33\u5b9a\u5904\u7406\u4e8b\u4ef6\u76f8\u673a\u6570\u636e\u4e2d\u7684\u566a\u58f0\u548c\u57df\u5dee\u5f02\uff0c\u901a\u8fc7\u51e0\u4f55\u4f18\u5316\u5b9e\u73b0\u9c81\u68d2\u76843D\u5efa\u56fe\u548c\u4f4d\u59ff\u4f30\u8ba1\uff0c\u4e3a\u4e8b\u4ef6\u611f\u77e5\u6a21\u5757\u7684\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.18260", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.18260", "abs": "https://arxiv.org/abs/2602.18260", "authors": ["Magnus Nor\u00e9n", "Marios-Nektarios Stamatopoulos", "Avijit Banerjee", "George Nikolakopoulos"], "title": "Role-Adaptive Collaborative Formation Planning for Team of Quadruped Robots in Cluttered Environments", "comment": null, "summary": "This paper presents a role-adaptive Leader-Follower-based formation planning and control framework for teams of quadruped robots operating in cluttered environments. Unlike conventional methods with fixed leaders or rigid formation roles, the proposed approach integrates dynamic role assignment and partial goal planning, enabling flexible, collision-free navigation in complex scenarios. Formation stability and inter-robot safety are ensured through a virtual spring-damper system coupled with a novel obstacle avoidance layer that adaptively adjusts each agent's velocity. A dynamic look-ahead reference generator further enhances flexibility, allowing temporary formation deformation to maneuver around obstacles while maintaining goal-directed motion. The Fast Marching Square (FM2) algorithm provides the global path for the leader and local paths for the followers as the planning backbone. The framework is validated through extensive simulations and real-world experiments with teams of quadruped robots. Results demonstrate smooth coordination, adaptive role switching, and robust formation maintenance in complex, unstructured environments. A video featuring the simulation and physical experiments along with their associated visualizations can be found at https://youtu.be/scq37Tua9W4.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u81ea\u9002\u5e94\u9886\u5bfc\u8005-\u8ddf\u968f\u8005\u7684\u56db\u8db3\u673a\u5668\u4eba\u7f16\u961f\u89c4\u5212\u63a7\u5236\u6846\u67b6\uff0c\u80fd\u591f\u5728\u590d\u6742\u73af\u5883\u4e2d\u5b9e\u73b0\u52a8\u6001\u89d2\u8272\u5206\u914d\u548c\u907f\u969c\u5bfc\u822a", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u91c7\u7528\u56fa\u5b9a\u9886\u5bfc\u8005\u6216\u521a\u6027\u7f16\u961f\u89d2\u8272\uff0c\u5728\u590d\u6742\u73af\u5883\u4e2d\u7f3a\u4e4f\u7075\u6d3b\u6027\uff0c\u96be\u4ee5\u5b9e\u73b0\u65e0\u78b0\u649e\u5bfc\u822a\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u52a8\u6001\u9002\u5e94\u73af\u5883\u53d8\u5316\u3001\u7075\u6d3b\u8c03\u6574\u7f16\u961f\u89d2\u8272\u7684\u63a7\u5236\u6846\u67b6", "method": "\u96c6\u6210\u52a8\u6001\u89d2\u8272\u5206\u914d\u548c\u90e8\u5206\u76ee\u6807\u89c4\u5212\uff0c\u91c7\u7528\u865a\u62df\u5f39\u7c27\u963b\u5c3c\u7cfb\u7edf\u786e\u4fdd\u7f16\u961f\u7a33\u5b9a\u6027\uff0c\u7ed3\u5408\u65b0\u578b\u907f\u969c\u5c42\u81ea\u9002\u5e94\u8c03\u6574\u673a\u5668\u4eba\u901f\u5ea6\u3002\u4f7f\u7528\u5feb\u901f\u884c\u8fdb\u5e73\u65b9\u7b97\u6cd5\u63d0\u4f9b\u5168\u5c40\u548c\u5c40\u90e8\u8def\u5f84\u89c4\u5212\uff0c\u52a8\u6001\u524d\u77bb\u53c2\u8003\u751f\u6210\u5668\u589e\u5f3a\u7075\u6d3b\u6027", "result": "\u901a\u8fc7\u5927\u91cf\u4eff\u771f\u548c\u771f\u5b9e\u56db\u8db3\u673a\u5668\u4eba\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u5c55\u793a\u4e86\u5e73\u6ed1\u534f\u8c03\u3001\u81ea\u9002\u5e94\u89d2\u8272\u5207\u6362\u548c\u9c81\u68d2\u7684\u7f16\u961f\u7ef4\u62a4\u80fd\u529b\uff0c\u5728\u590d\u6742\u975e\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u8868\u73b0\u826f\u597d", "conclusion": "\u8be5\u6846\u67b6\u6210\u529f\u5b9e\u73b0\u4e86\u56db\u8db3\u673a\u5668\u4eba\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u81ea\u9002\u5e94\u7f16\u961f\u63a7\u5236\uff0c\u901a\u8fc7\u52a8\u6001\u89d2\u8272\u5206\u914d\u548c\u7075\u6d3b\u7684\u907f\u969c\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7f16\u961f\u5bfc\u822a\u7684\u9002\u5e94\u6027\u548c\u9c81\u68d2\u6027"}}
{"id": "2602.18330", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.18330", "abs": "https://arxiv.org/abs/2602.18330", "authors": ["Mohsen Jafarpour", "Ayberk Y\u00fcksek", "Shahab Eshghi", "Stanislav Gorb", "Edoardo Milana"], "title": "Tendon-Driven Reciprocating and Non-Reciprocating Motion via Snapping Metabeams", "comment": "9th IEEE-RAS International Conference on Soft Robotics (RoboSoft 2026)", "summary": "Snapping beams enable rapid geometric transitions through nonlinear instability, offering an efficient means of generating motion in soft robotic systems. In this study, a tendon-driven mechanism consisting of spiral-based metabeams was developed to exploit this principle for producing both reciprocating and non-reciprocating motion. The snapping structures were fabricated using fused deposition modeling with polylactic acid (PLA) and experimentally tested under different boundary conditions to analyze their nonlinear behavior. The results show that the mechanical characteristics, including critical forces and stability, can be tuned solely by adjusting the boundary constraints. The spiral geometry allows large reversible deformation even when made from a relatively stiff material such as PLA, providing a straightforward design concept for controllable snapping behavior. The developed mechanism was further integrated into a swimming robot, where tendon-driven fins exhibited two distinct actuation modes: reciprocating and non-reciprocating motion. The latter enabled efficient propulsion, producing a forward displacement of about 32 mm per 0.4 s cycle ($\\approx$ 81 mm/s, equivalent to 0.4 body lengths per second). This study highlights the potential of geometry-driven snapping structures for efficient and programmable actuation in soft robotic systems.", "AI": {"tldr": "\u672c\u7814\u7a76\u5f00\u53d1\u4e86\u4e00\u79cd\u57fa\u4e8e\u87ba\u65cb\u5f62\u8d85\u6881\u7684\u808c\u8171\u9a71\u52a8\u673a\u6784\uff0c\u5229\u7528\u975e\u7ebf\u6027\u5931\u7a33\u5b9e\u73b0\u5feb\u901f\u51e0\u4f55\u8f6c\u6362\uff0c\u4e3a\u8f6f\u4f53\u673a\u5668\u4eba\u63d0\u4f9b\u9ad8\u6548\u8fd0\u52a8\u3002\u901a\u8fc7\u8c03\u6574\u8fb9\u754c\u6761\u4ef6\u53ef\u8c03\u8282\u673a\u68b0\u7279\u6027\uff0c\u5e76\u6210\u529f\u5e94\u7528\u4e8e\u6e38\u6cf3\u673a\u5668\u4eba\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u63a8\u8fdb\u3002", "motivation": "\u5229\u7528\u6881\u7684\u5feb\u901f\u5c48\u66f2\u5931\u7a33\u73b0\u8c61\u5b9e\u73b0\u8f6f\u4f53\u673a\u5668\u4eba\u7cfb\u7edf\u7684\u9ad8\u6548\u8fd0\u52a8\u751f\u6210\uff0c\u901a\u8fc7\u51e0\u4f55\u8bbe\u8ba1\u800c\u975e\u6750\u6599\u9009\u62e9\u6765\u63a7\u5236\u975e\u7ebf\u6027\u884c\u4e3a\u3002", "method": "\u91c7\u7528\u7194\u878d\u6c89\u79ef\u6210\u578b\u6280\u672f\u7528PLA\u6750\u6599\u5236\u9020\u87ba\u65cb\u5f62\u8d85\u6881\u7ed3\u6784\uff0c\u901a\u8fc7\u808c\u8171\u9a71\u52a8\u673a\u5236\u5b9e\u73b0\u8fd0\u52a8\uff0c\u5728\u4e0d\u540c\u8fb9\u754c\u6761\u4ef6\u4e0b\u8fdb\u884c\u5b9e\u9a8c\u6d4b\u8bd5\u5206\u6790\u975e\u7ebf\u6027\u884c\u4e3a\u3002", "result": "\u4ec5\u901a\u8fc7\u8c03\u6574\u8fb9\u754c\u7ea6\u675f\u5373\u53ef\u8c03\u8282\u4e34\u754c\u529b\u548c\u7a33\u5b9a\u6027\u7b49\u673a\u68b0\u7279\u6027\uff1b\u87ba\u65cb\u51e0\u4f55\u5141\u8bb8\u5927\u8303\u56f4\u53ef\u9006\u53d8\u5f62\uff1b\u6e38\u6cf3\u673a\u5668\u4eba\u5b9e\u73b0\u4e86\u4e24\u79cd\u9a71\u52a8\u6a21\u5f0f\uff0c\u975e\u5f80\u590d\u8fd0\u52a8\u8fbe\u5230\u7ea681mm/s\u7684\u63a8\u8fdb\u901f\u5ea6\u3002", "conclusion": "\u51e0\u4f55\u9a71\u52a8\u7684\u5c48\u66f2\u7ed3\u6784\u4e3a\u8f6f\u4f53\u673a\u5668\u4eba\u7cfb\u7edf\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u53ef\u7f16\u7a0b\u7684\u9a71\u52a8\u65b9\u6848\uff0c\u5c55\u793a\u4e86\u901a\u8fc7\u51e0\u4f55\u8bbe\u8ba1\u800c\u975e\u6750\u6599\u9009\u62e9\u5b9e\u73b0\u53ef\u63a7\u975e\u7ebf\u6027\u884c\u4e3a\u7684\u6f5c\u529b\u3002"}}
{"id": "2602.18344", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.18344", "abs": "https://arxiv.org/abs/2602.18344", "authors": ["Mengguang Li", "Heinz Koeppl"], "title": "Downwash-aware Configuration Optimization for Modular Aerial Systems", "comment": "Accepted to the IEEE International Conference on Robotics and Automation (ICRA) 2026", "summary": "This work proposes a framework that generates and optimally selects task-specific assembly configurations for a large group of homogeneous modular aerial systems, explicitly enforcing bounds on inter-module downwash. Prior work largely focuses on planar layouts and often ignores aerodynamic interference. In contrast, firstly we enumerate non-isomorphic connection topologies at scale; secondly, we solve a nonlinear program to check feasibility and select the configuration that minimizes control input subject to actuation limits and downwash constraints. We evaluate the framework in physics-based simulation and demonstrate it in real-world experiments.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u4e3a\u540c\u6784\u6a21\u5757\u5316\u7a7a\u4e2d\u7cfb\u7edf\u751f\u6210\u548c\u4f18\u5316\u9009\u62e9\u4efb\u52a1\u7279\u5b9a\u88c5\u914d\u914d\u7f6e\u7684\u6846\u67b6\uff0c\u660e\u786e\u8003\u8651\u6a21\u5757\u95f4\u4e0b\u6d17\u6d41\u7684\u7ea6\u675f", "motivation": "\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u5e73\u9762\u5e03\u5c40\u4e14\u5e38\u5ffd\u7565\u7a7a\u6c14\u52a8\u529b\u5b66\u5e72\u6270\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u5904\u7406\u4e09\u7ef4\u914d\u7f6e\u5e76\u8003\u8651\u4e0b\u6d17\u6d41\u7ea6\u675f\u7684\u6846\u67b6", "method": "\u9996\u5148\u5927\u89c4\u6a21\u679a\u4e3e\u975e\u540c\u6784\u8fde\u63a5\u62d3\u6251\uff0c\u7136\u540e\u6c42\u89e3\u975e\u7ebf\u6027\u89c4\u5212\u95ee\u9898\u68c0\u67e5\u53ef\u884c\u6027\u5e76\u9009\u62e9\u5728\u9a71\u52a8\u9650\u5236\u548c\u4e0b\u6d17\u6d41\u7ea6\u675f\u4e0b\u6700\u5c0f\u5316\u63a7\u5236\u8f93\u5165\u7684\u914d\u7f6e", "result": "\u5728\u57fa\u4e8e\u7269\u7406\u7684\u4eff\u771f\u4e2d\u8bc4\u4f30\u6846\u67b6\uff0c\u5e76\u5728\u771f\u5b9e\u4e16\u754c\u5b9e\u9a8c\u4e2d\u5c55\u793a\u5176\u6709\u6548\u6027", "conclusion": "\u8be5\u6846\u67b6\u80fd\u591f\u4e3a\u6a21\u5757\u5316\u7a7a\u4e2d\u7cfb\u7edf\u751f\u6210\u8003\u8651\u7a7a\u6c14\u52a8\u529b\u5b66\u5e72\u6270\u7684\u4f18\u5316\u914d\u7f6e\uff0c\u5728\u4eff\u771f\u548c\u5b9e\u9a8c\u4e2d\u5747\u5f97\u5230\u9a8c\u8bc1"}}
{"id": "2602.18374", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.18374", "abs": "https://arxiv.org/abs/2602.18374", "authors": ["Venkatesh Sripada", "Frank Guerin", "Amir Ghalamzan"], "title": "Zero-shot Interactive Perception", "comment": "Original manuscript submitted on April 24, 2025. Timestamped and publicly available on OpenReview: https://openreview.net/forum?id=7MhpFcr5Nx", "summary": "Interactive perception (IP) enables robots to extract hidden information in their workspace and execute manipulation plans by physically interacting with objects and altering the state of the environment -- crucial for resolving occlusions and ambiguity in complex, partially observable scenarios. We present Zero-Shot IP (ZS-IP), a novel framework that couples multi-strategy manipulation (pushing and grasping) with a memory-driven Vision Language Model (VLM) to guide robotic interactions and resolve semantic queries. ZS-IP integrates three key components: (1) an Enhanced Observation (EO) module that augments the VLM's visual perception with both conventional keypoints and our proposed pushlines -- a novel 2D visual augmentation tailored to pushing actions, (2) a memory-guided action module that reinforces semantic reasoning through context lookup, and (3) a robotic controller that executes pushing, pulling, or grasping based on VLM output. Unlike grid-based augmentations optimized for pick-and-place, pushlines capture affordances for contact-rich actions, substantially improving pushing performance. We evaluate ZS-IP on a 7-DOF Franka Panda arm across diverse scenes with varying occlusions and task complexities. Our experiments demonstrate that ZS-IP outperforms passive and viewpoint-based perception techniques such as Mark-Based Visual Prompting (MOKA), particularly in pushing tasks, while preserving the integrity of non-target elements.", "AI": {"tldr": "ZS-IP\u662f\u4e00\u4e2a\u96f6\u6837\u672c\u4ea4\u4e92\u611f\u77e5\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u591a\u7b56\u7565\u64cd\u4f5c\uff08\u63a8\u548c\u6293\uff09\u4e0e\u8bb0\u5fc6\u9a71\u52a8\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u5f15\u5bfc\u673a\u5668\u4eba\u4ea4\u4e92\u5e76\u89e3\u51b3\u8bed\u4e49\u67e5\u8be2\u95ee\u9898\u3002", "motivation": "\u4ea4\u4e92\u611f\u77e5\u5bf9\u4e8e\u673a\u5668\u4eba\u5728\u590d\u6742\u3001\u90e8\u5206\u53ef\u89c2\u5bdf\u573a\u666f\u4e2d\u63d0\u53d6\u9690\u85cf\u4fe1\u606f\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5728\u5904\u7406\u906e\u6321\u548c\u6a21\u7cca\u6027\u65b9\u9762\u5b58\u5728\u5c40\u9650\uff0c\u9700\u8981\u66f4\u6709\u6548\u7684\u4ea4\u4e92\u611f\u77e5\u6846\u67b6\u3002", "method": "ZS-IP\u5305\u542b\u4e09\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a\u589e\u5f3a\u89c2\u5bdf\u6a21\u5757\uff08\u7ed3\u5408\u4f20\u7edf\u5173\u952e\u70b9\u548c\u65b0\u578bpushlines\u89c6\u89c9\u589e\u5f3a\uff09\u3001\u8bb0\u5fc6\u5f15\u5bfc\u52a8\u4f5c\u6a21\u5757\uff08\u901a\u8fc7\u4e0a\u4e0b\u6587\u67e5\u627e\u5f3a\u5316\u8bed\u4e49\u63a8\u7406\uff09\u3001\u57fa\u4e8eVLM\u8f93\u51fa\u7684\u673a\u5668\u4eba\u63a7\u5236\u5668\uff08\u6267\u884c\u63a8\u3001\u62c9\u3001\u6293\u64cd\u4f5c\uff09\u3002", "result": "\u57287-DOF Franka Panda\u673a\u68b0\u81c2\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cZS-IP\u5728\u63a8\u52a8\u4efb\u52a1\u4e2d\u663e\u8457\u4f18\u4e8e\u88ab\u52a8\u548c\u57fa\u4e8e\u89c6\u70b9\u7684\u611f\u77e5\u6280\u672f\uff08\u5982MOKA\uff09\uff0c\u540c\u65f6\u4fdd\u6301\u975e\u76ee\u6807\u5143\u7d20\u7684\u5b8c\u6574\u6027\u3002", "conclusion": "ZS-IP\u901a\u8fc7\u521b\u65b0\u7684pushlines\u89c6\u89c9\u589e\u5f3a\u548c\u8bb0\u5fc6\u9a71\u52a8\u67b6\u6784\uff0c\u6709\u6548\u63d0\u5347\u4e86\u673a\u5668\u4eba\u5728\u590d\u6742\u573a\u666f\u4e2d\u7684\u4ea4\u4e92\u611f\u77e5\u80fd\u529b\uff0c\u7279\u522b\u662f\u5728\u63a8\u52a8\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2602.18379", "categories": ["cs.RO", "cond-mat.soft"], "pdf": "https://arxiv.org/pdf/2602.18379", "abs": "https://arxiv.org/abs/2602.18379", "authors": ["Hugo de Souza Oliveira", "Xin Li", "Mohsen Jafarpour", "Edoardo Milana"], "title": "Ori-Sense: origami capacitive sensing for soft robotic applications", "comment": "9th IEEE-RAS International Conference on Soft Robotics (RoboSoft 2026)", "summary": "This work introduces Ori-Sense, a compliant capacitive sensor inspired by the inverted Kresling origami pattern. The device translates torsional deformation into measurable capacitance changes, enabling proprioceptive feedback for soft robotic systems. Using dissolvable-core molding, we fabricated a monolithic silicone structure with embedded conductive TPU electrodes, forming an integrated soft capacitor. Mechanical characterization revealed low stiffness and minimal impedance, with torque values below 0.01 N mm for axial displacements between -15 mm and 15 mm, and up to 0.03 N mm at 30 degrees twist under compression. Finite-element simulations confirmed localized stresses along fold lines and validated the measured torque-rotation response. Electrical tests showed consistent capacitance modulation up to 30%, directly correlated with the twist angle, and maximal sensitivity of S_theta ~ 0.0067 pF/deg at 5 mm of axial deformation.", "AI": {"tldr": "Ori-Sense\u662f\u4e00\u79cd\u57fa\u4e8e\u5012\u7f6eKresling\u6298\u7eb8\u56fe\u6848\u7684\u67d4\u6027\u7535\u5bb9\u4f20\u611f\u5668\uff0c\u53ef\u5c06\u626d\u8f6c\u53d8\u5f62\u8f6c\u5316\u4e3a\u53ef\u6d4b\u91cf\u7684\u7535\u5bb9\u53d8\u5316\uff0c\u4e3a\u8f6f\u4f53\u673a\u5668\u4eba\u7cfb\u7edf\u63d0\u4f9b\u672c\u4f53\u611f\u77e5\u53cd\u9988\u3002", "motivation": "\u5f00\u53d1\u4e00\u79cd\u80fd\u591f\u5c06\u673a\u68b0\u53d8\u5f62\u76f4\u63a5\u8f6c\u6362\u4e3a\u7535\u4fe1\u53f7\u7684\u67d4\u6027\u4f20\u611f\u5668\uff0c\u4e3a\u8f6f\u4f53\u673a\u5668\u4eba\u7cfb\u7edf\u63d0\u4f9b\u53ef\u9760\u7684\u672c\u4f53\u611f\u77e5\u80fd\u529b\uff0c\u89e3\u51b3\u4f20\u7edf\u521a\u6027\u4f20\u611f\u5668\u5728\u67d4\u6027\u7cfb\u7edf\u4e2d\u96c6\u6210\u56f0\u96be\u7684\u95ee\u9898\u3002", "method": "\u91c7\u7528\u53ef\u6eb6\u89e3\u82af\u6a21\u5851\u6280\u672f\u5236\u9020\u5355\u5757\u7845\u80f6\u7ed3\u6784\uff0c\u5d4c\u5165\u5bfc\u7535TPU\u7535\u6781\u5f62\u6210\u96c6\u6210\u67d4\u6027\u7535\u5bb9\u5668\uff1b\u901a\u8fc7\u673a\u68b0\u548c\u7535\u6c14\u6d4b\u8bd5\u8868\u5f81\u6027\u80fd\uff0c\u5e76\u7ed3\u5408\u6709\u9650\u5143\u6a21\u62df\u9a8c\u8bc1\u5e94\u529b\u5206\u5e03\u548c\u626d\u77e9-\u65cb\u8f6c\u54cd\u5e94\u3002", "result": "\u4f20\u611f\u5668\u8868\u73b0\u51fa\u4f4e\u521a\u5ea6\u548c\u6700\u5c0f\u963b\u6297\uff0c\u626d\u77e9\u503c\u5728\u00b115mm\u8f74\u5411\u4f4d\u79fb\u65f6\u4f4e\u4e8e0.01N\u00b7mm\uff0c\u538b\u7f29\u72b6\u6001\u4e0b30\u5ea6\u626d\u8f6c\u65f6\u8fbe0.03N\u00b7mm\uff1b\u7535\u5bb9\u8c03\u5236\u9ad8\u8fbe30%\uff0c\u4e0e\u626d\u8f6c\u89d2\u5ea6\u76f4\u63a5\u76f8\u5173\uff0c\u57285mm\u8f74\u5411\u53d8\u5f62\u65f6\u6700\u5927\u7075\u654f\u5ea6\u4e3aS_theta ~ 0.0067 pF/\u5ea6\u3002", "conclusion": "Ori-Sense\u6210\u529f\u5b9e\u73b0\u4e86\u5c06\u626d\u8f6c\u53d8\u5f62\u8f6c\u6362\u4e3a\u7535\u5bb9\u53d8\u5316\u7684\u67d4\u6027\u4f20\u611f\u673a\u5236\uff0c\u4e3a\u8f6f\u4f53\u673a\u5668\u4eba\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u672c\u4f53\u611f\u77e5\u89e3\u51b3\u65b9\u6848\uff0c\u5176\u96c6\u6210\u5236\u9020\u65b9\u6cd5\u548c\u53ef\u9884\u6d4b\u7684\u673a\u68b0-\u7535\u6c14\u54cd\u5e94\u7279\u6027\u5177\u6709\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2602.18386", "categories": ["cs.RO", "cs.AI", "cs.LG", "eess.SY"], "pdf": "https://arxiv.org/pdf/2602.18386", "abs": "https://arxiv.org/abs/2602.18386", "authors": ["Mohamed Elgouhary", "Amr S. El-Wakeel"], "title": "Learning to Tune Pure Pursuit in Autonomous Racing: Joint Lookahead and Steering-Gain Control with PPO", "comment": null, "summary": "Pure Pursuit (PP) is widely used in autonomous racing for real-time path tracking due to its efficiency and geometric clarity, yet performance is highly sensitive to how key parameters-lookahead distance and steering gain-are chosen. Standard velocity-based schedules adjust these only approximately and often fail to transfer across tracks and speed profiles. We propose a reinforcement-learning (RL) approach that jointly chooses the lookahead Ld and a steering gain g online using Proximal Policy Optimization (PPO). The policy observes compact state features (speed and curvature taps) and outputs (Ld, g) at each control step. Trained in F1TENTH Gym and deployed in a ROS 2 stack, the policy drives PP directly (with light smoothing) and requires no per-map retuning. Across simulation and real-car tests, the proposed RL-PP controller that jointly selects (Ld, g) consistently outperforms fixed-lookahead PP, velocity-scheduled adaptive PP, and an RL lookahead-only variant, and it also exceeds a kinematic MPC raceline tracker under our evaluated settings in lap time, path-tracking accuracy, and steering smoothness, demonstrating that policy-guided parameter tuning can reliably improve classical geometry-based control.", "AI": {"tldr": "\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u5728\u7ebf\u8054\u5408\u8c03\u6574\u7eaf\u8ffd\u8e2a\u7b97\u6cd5\u7684\u524d\u77bb\u8ddd\u79bb\u548c\u8f6c\u5411\u589e\u76ca\u53c2\u6570\uff0c\u63d0\u5347\u8def\u5f84\u8ddf\u8e2a\u6027\u80fd", "motivation": "\u7eaf\u8ffd\u8e2a\u7b97\u6cd5\u5728\u81ea\u52a8\u9a7e\u9a76\u8d5b\u8f66\u4e2d\u5e7f\u6cdb\u4f7f\u7528\uff0c\u4f46\u5176\u6027\u80fd\u5bf9\u524d\u77bb\u8ddd\u79bb\u548c\u8f6c\u5411\u589e\u76ca\u53c2\u6570\u7684\u9009\u62e9\u975e\u5e38\u654f\u611f\u3002\u4f20\u7edf\u7684\u57fa\u4e8e\u901f\u5ea6\u7684\u8c03\u5ea6\u65b9\u6cd5\u8c03\u6574\u4e0d\u591f\u7cbe\u786e\uff0c\u4e14\u96be\u4ee5\u5728\u4e0d\u540c\u8d5b\u9053\u548c\u901f\u5ea6\u914d\u7f6e\u95f4\u8fc1\u79fb\u3002", "method": "\u63d0\u51fa\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u4f7f\u7528PPO\u7b97\u6cd5\u5728\u7ebf\u8054\u5408\u9009\u62e9\u524d\u77bb\u8ddd\u79bbLd\u548c\u8f6c\u5411\u589e\u76cag\u3002\u7b56\u7565\u89c2\u5bdf\u7d27\u51d1\u72b6\u6001\u7279\u5f81\uff08\u901f\u5ea6\u548c\u66f2\u7387\u4fe1\u606f\uff09\uff0c\u5728\u6bcf\u4e2a\u63a7\u5236\u6b65\u9aa4\u8f93\u51fa(Ld, g)\u3002\u5728F1TENTH Gym\u4e2d\u8bad\u7ec3\uff0c\u90e8\u7f72\u5728ROS 2\u6808\u4e2d\uff0c\u76f4\u63a5\u9a71\u52a8\u7eaf\u8ffd\u8e2a\u7b97\u6cd5\uff08\u5e26\u8f7b\u5fae\u5e73\u6ed1\uff09\u3002", "result": "\u5728\u4eff\u771f\u548c\u771f\u5b9e\u8f66\u8f86\u6d4b\u8bd5\u4e2d\uff0c\u63d0\u51fa\u7684RL-PP\u63a7\u5236\u5668\uff08\u8054\u5408\u9009\u62e9Ld\u548cg\uff09\u5728\u5708\u901f\u3001\u8def\u5f84\u8ddf\u8e2a\u7cbe\u5ea6\u548c\u8f6c\u5411\u5e73\u6ed1\u5ea6\u65b9\u9762\u4e00\u81f4\u4f18\u4e8e\u56fa\u5b9a\u524d\u77bbPP\u3001\u901f\u5ea6\u8c03\u5ea6\u81ea\u9002\u5e94PP\u548c\u4ec5\u8c03\u6574\u524d\u77bb\u7684RL\u53d8\u4f53\uff0c\u751a\u81f3\u8d85\u8fc7\u4e86\u8fd0\u52a8\u5b66MPC\u8f68\u8ff9\u8ddf\u8e2a\u5668\u3002", "conclusion": "\u7b56\u7565\u5f15\u5bfc\u7684\u53c2\u6570\u8c03\u4f18\u53ef\u4ee5\u53ef\u9760\u5730\u6539\u8fdb\u57fa\u4e8e\u51e0\u4f55\u7684\u7ecf\u5178\u63a7\u5236\u65b9\u6cd5\uff0c\u8bc1\u660e\u4e86\u5f3a\u5316\u5b66\u4e60\u5728\u4f18\u5316\u4f20\u7edf\u63a7\u5236\u7b97\u6cd5\u53c2\u6570\u65b9\u9762\u7684\u6709\u6548\u6027\u3002"}}
